<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 悟剑阁</title>
    <link>https://sword865.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Fri, 04 Nov 2016 22:47:21 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>谈谈Factorization Machine</title>
      <link>https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/</link>
      <pubDate>Fri, 04 Nov 2016 22:47:21 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/</guid>
      <description>&lt;p&gt;因子分解机(Factorization Machine, 简称FM)是一种不错的CTR预估模型，也是我们现在在使用的广告点击率预估模型，比起著名的Logistic Regression, FM能够把握一些组合的高阶特征，因此拥有更强的表现力。&lt;/p&gt;&#xA;&lt;p&gt;在做点击率预估时，我们的特征往往来自于用户(user)、广告(item)和上下文环境(context)，在线性模型中，这些特征不进行组合的话，就会发生一个很尴尬的情况，因为：&lt;/p&gt;&#xA;&lt;div&gt;$$score = f(w_{user} * x_{user} + w_{item} * x_{item} + w_{contex} * x_{contex})$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;所以，对所有用户&amp;ndash;我们将得到相同的排序。&lt;/p&gt;&#xA;&lt;p&gt;因此我们需要引入一些组合特征作为输入模型，然而仅二阶特征组合的可能性就是原始特征的平方之多，但是由于很多特征其实是相互独立的，他们的组合并没有什么价值。FM就是一种能够自动把握一些高阶特征的点击率预估模型，可以自动帮助使用者选择合适的高阶输入。&lt;/p&gt;&#xA;&lt;p&gt;我们先写出带有所有二阶组合特征的目标函数，其中矩阵W中有n^2个参数，求解非常复杂：&lt;/p&gt;&#xA;&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;我们都知道在矩阵分解的协同过滤中中，我们认为每个用户可以表示成一个K维的特征向量$u_k$,每个物品也能表示成一个高维向量$i_k$，这样用户与物品的相关性就可以用两个向量的点击表示，所有用户与所有物品的相关性就可以用两个矩阵相乘的形式表示出来了。&lt;/p&gt;&#xA;&lt;p&gt;FM的模型参考了矩阵分解的思想，认为每个特征 $x_i$都可以用一个K维的特征向量$v_i$表示，那么所有特征之前的相关性就也就可以用两个矩阵的相乘进行表示了，模型表示如下：&lt;/p&gt;&#xA;&lt;p&gt;其中矩阵W中代表了特征的特征向量的內积，既：$w_{ij}=v_i v_j$，所以，公式可以改写为:&lt;/p&gt;&#xA;&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} v_{i} v_{j} x_i x_j$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;也就是：&lt;/p&gt;&#xA;&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \sum_{h=i}^{k} v_{i,h} v_{j,h} x_i x_j$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;可以看出，W中的参数现在只有nk个了，因为一般有k&amp;laquo;n，所以FM大大降低的目标函数的复杂度。推导可得梯度公式：&lt;/p&gt;</description>
    </item>
    <item>
      <title>基于标签的推荐系统</title>
      <link>https://sword865.github.io/archives/5/</link>
      <pubDate>Mon, 17 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/5/</guid>
      <description>&lt;p&gt;一、基于图模型的推荐&lt;/p&gt;&#xA;&lt;p&gt;在不考虑标签时，基于二项图有两种随机游走的图推荐算法：&lt;/p&gt;&#xA;&lt;p&gt;1.probability spreading&lt;/p&gt;&#xA;&lt;p&gt;随机游走算法，在游走中，每个目标得到权重是基于归属者的边计算出来的。&lt;/p&gt;&#xA;&lt;p&gt;每次传播(item-&amp;gt;user-&amp;gt;item)后用户Ui的兴趣向量：&lt;/p&gt;&#xA;&lt;div&gt;$$f_j^p=\sum_{l=1}^{n}\sum_{s=1}^{m}\frac{a_{lj}a_{ls}a_{is}}{K(U_l)K(I_s)},j=1…m$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;2.heat spreading&lt;/p&gt;&#xA;&lt;p&gt;规则与ProbS相反，在游走中，每个目标得到权重是基于自己的边计算出来的。&lt;/p&gt;&#xA;&lt;p&gt;每次传播后用户Ui的兴趣向量：&lt;/p&gt;&#xA;&lt;div&gt;$$f_h^p=\frac{1}{K(I_j)}\sum_{l=1}^{n}\sum_{s=1}^{m}\frac{a_{lj}a_{ls}a_{is}}{K(U_l)},j=1…m$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;其中：&lt;/p&gt;&#xA;&lt;div&gt;$$K(I_j)=\sum_{l=1}^{m}a_{lj}$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;是节目j的邻域大小，&lt;/p&gt;&#xA;&lt;div&gt;$$K(U_l)=\sum_{l=1}^{n}a_{ls}$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;是用户l的邻域大小。&lt;/p&gt;&#xA;&lt;p&gt;$a_{ij}$是表示用户i和物品j之间是否有边存在的二元向量。&lt;/p&gt;&#xA;&lt;p&gt;相比之下，Heats算法倾向于降低热门item的权重，而Probs中与增强对热门item的推荐。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;在随机游走算法的基础上，有基于&lt;span style=&#34;color: #333333; font-family: arial; font-size: 13px; line-height: 20.0200004577637px;&#34;&gt;三分图的标签推荐算法：&lt;/span&gt;&lt;/p&gt;&#xA;&lt;img title=&#34;NewImage.png&#34; src=&#34;https://sword865.github.io/images/archives/171255254884996.png&#34; alt=&#34;NewImage&#34; width=&#34;600&#34; height=&#34;305&#34; border=&#34;0&#34; /&gt;&#xD;&#xA;&lt;p&gt;图中，用户i的每个item的权重（1 or 0）会同时像用户和标签进行传播，这样每次传播后的兴趣向量：&lt;/p&gt;&#xA;&lt;p&gt;$f_j^t=\lambda f_j^p + (1-\lambda) f_j^{pt}$，其中$f_j^p$和$f_j^{pt}$分别是从(item-&amp;gt;user-&amp;gt;item)和(item-&amp;gt;tag-&amp;gt;item)传播后得到的权重。&lt;/p&gt;&#xA;&lt;p&gt;&lt;span style=&#34;font-size: 14px;&#34;&gt;二、矩阵分解的张量模型&lt;/span&gt;&lt;/p&gt;&#xA;&lt;p&gt;对三元阵$Y_{(n\times m\times t)}$进行矩阵分解，C为核张量，U,I,T为用户特征，物品特征和标签特征矩阵。&lt;/p&gt;&#xA;&lt;p&gt;根据分解结果对Y进行填充。&lt;/p&gt;&#xA;&lt;img title=&#34;NewImage.png&#34; src=&#34;https://sword865.github.io/images/archives/171508548949009.png&#34; alt=&#34;NewImage&#34; width=&#34;600&#34; height=&#34;260&#34; border=&#34;0&#34; /&gt;&#xD;&#xA;&lt;p&gt;填充后即得到评分矩阵&lt;/p&gt;&#xA;&lt;p&gt;&lt;span style=&#34;font-size: 14px;&#34;&gt; &lt;/span&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google在KDD2013上关于CTR的一篇论文</title>
      <link>https://sword865.github.io/archives/6/</link>
      <pubDate>Sat, 03 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/6/</guid>
      <description>&lt;p&gt;最近在做CTR，刚好Google在KDD发了一篇文章，讲了他们的一些尝试，总结一下：&lt;/p&gt;&#xA;&lt;p&gt;先是一些公式的符号说明：&lt;/p&gt;&#xA;&lt;p&gt;[&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011358-949b9f116ca94563ab5a0efa047697db.png&#34; alt=&#34;image&#34; width=&#34;447&#34; height=&#34;98&#34; border=&#34;0&#34; /&gt;][1]&lt;/p&gt;&#xA;&lt;p&gt;一、优化算法&lt;/p&gt;&#xA;&lt;p&gt;CTR中经常用Logistic regression进行训练，一个常用的Loss Function为&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011358-9e64ee86e7f14dbd8a3da9f8a2bb7c13.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011358-03407ff016d04735b28103abc573d428.png&#34; alt=&#34;image&#34; width=&#34;329&#34; height=&#34;34&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Online gradient descent(OGD)是一个常用的优化方法，但是在加上L1正则化后，这种方法不能产生有效的稀疏模型。相比之下 Regularized Dual Averaging (RDA)拥有更好的稀疏性，但是精度不如OGD好。&lt;/p&gt;&#xA;&lt;p&gt;FTRL-Proximal 方法可以同时得到稀疏性与精确性，不同于OGD的迭代步骤：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011358-2e856d4483c64ef0a668ee8c8df2a548.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011359-6f55d3e1f55f4389979bec14102120b5.png&#34; alt=&#34;image&#34; width=&#34;187&#34; height=&#34;39&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;其中$\eta_t$是一个非增的学习率&lt;/p&gt;&#xA;&lt;p&gt;FTRL-Proximal通过下式迭代：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011359-900ea01f64a547bd99cefe8e375a3a9d.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011359-37b635900b57402a9efc1d0fbf12e7d2.png&#34; alt=&#34;image&#34; width=&#34;482&#34; height=&#34;60&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;参数$\sigma$是学习率，一般我们有 ${\sum_{s=1}^t\sigma_s=\frac{1}{\eta_t}}$。&lt;/p&gt;</description>
    </item>
    <item>
      <title>[转] Deep Learning（深度学习）学习笔记整理系列</title>
      <link>https://sword865.github.io/archives/7/</link>
      <pubDate>Fri, 26 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/7/</guid>
      <description>&lt;p&gt;转一套Deep Learning的文章&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/zouxy09/article/details/8775360&#34;&gt;http://blog.csdn.net/zouxy09/article/details/8775360&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;顺便附上翻译的UFLDL&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B&#34;&gt;http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>决策粗糙集</title>
      <link>https://sword865.github.io/archives/8/</link>
      <pubDate>Thu, 25 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/8/</guid>
      <description>&lt;p&gt;今天收拾资料，发现了以前刚接触粗糙集时写的一个综述，好久没写博客，发上来充数好了&lt;/p&gt;&#xA;&lt;p&gt;一、粗糙集模型&lt;a href=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgYAAAAvCAIAAAAXYZUgAAAHHUlEQVR4nO2d2aHcIAxFXRcFUceUQDVuxsWQD2MPiwQS4GVe7vlI8mYwxvKFK5YkiwcAAAC8994vTzcAAADAW4AlAAAACMASAAAABGAJAAAAArAEAAAAAVgCAACAACwBAABAAJYAAAAgAEsAAAAQgCUAAAAIwBIAAAAEYAkAAAACsAQAAAABWAIAj7HM4+lHAX8EKAkAAEAAlgAAACBwhyVws9rKnFf+4QOs1rhNe9HmjF1nt2NZFmmlm7NJozdnzvi3H2e13xWK2Y/RjyoAPHVd/TX1goJY3S+S90P0yLRjcfP8Ubj6GX8bX9vR2rlszhCioTRVjKKbM4LBV9EOlXpJS9DKv+uiPkQh9VNsoamrn1PvartC0nkZx+aMJN2Ibp/fPcpbWvV8i6r6WNHGGyXOk4j/27zNmf3P2siqmCBTida1HWMvll31uCXQfhAox6bV5u+tWoGyIcqKfs0SvJeFlP5MhcoS5BU+pF468yjdNJ4mnjGel7Z0SIW2BPUIr7iA6Y+rrVQijGQvqy1cbe8GNmvUZX3xJktQFS574DssoaqU/B0xC0VzTKEjNf5FS5CHdKhRcl29X701jebeSXvpPIVqa7ndEthgtWIgi6Qe9r5kh5911wxYgpR2V/mOTecMj6BhLOK2KGv5SUuQhHR8Ev13LEGQtBzfV/a2ZphCz3B1tyVUemmtA+f3mbZLyN00rOcVy3o/YAmk6LO114UhK09W2N+pjqmdcS6E8fvJFs0F7Xp8vgc/MmeJ2Pbiri6nKXPzPGs4niB8tD9FNs9sWoKkkg5LSBdGM8ZDOm5TTTXeod5IkCuh0XNxwq5HQI3b0qdvKyuq+WKJlm8ltDrZA0rvIrOEaj13WYI0kpobk9YS23NWQLFU8Pl85K2bPEvgdC/sJwu1CNusvE6cRaYLdavdR5t8/A+51mqjBTzZwEMsBNKFGJ2LB01yXh4q5gQnmyU0KlEOv+OJTDukPWsUCU1dXa3eWKKZEa92z2P2D06J7iPF5oyx9nxRgkDEbtIoNvbaGKXsXYtdwhfPErh6brMEaSQVdZcHGfOSaR9WPOuTllD+mH04vXCDVNyJUPM85fhytYtxa2bZok5C5NVcqaExjFsfIDKv6K7ChaN6JaqmT5hSi0I6aAqzLKGjsPeFtJKevr+M77OFJ92cWawj3mhTorJQUe+ZzVoYEemX+TQLR0w991mCXHR8upfvImdzJiJxVJxWiW0AlnBS2wYq1BO+XO1irLXF22gNTJszxq2Che0xS6gqsfKlfC+hWomm6cOOIA6pH7KFRy2BlGGcteRrBSE7NkYv0TDvbcdpSKL1rYjKQVfVXgJdz12WII6kot64u5BPR/YnWvefz+dFllAv09Gpyl+lMD3Ke887ApN+NPrbpjgxrErByMrY4a+yvKizBLaS+yxBEdLB/dBLLaGh3vxVFvNYSr/8UaFKmMIoI3qFwxNZ3qAr1q21BHrUvN4SNJEkr+S/PrY2mZlX9jGj+0+Bpo2vtwSygwkhZuRsd0u29Jj0o5I7M2tTkmZ1wewlWFeZaUv3EqqVRBedO56VlYDeB9WE9DV7CWXhpnqzISyVaNbjT91wD1wdbZNap253kbdk9xKizTviS/leAl3P9ZagjKSXLxydZblELSteu32HE5zcagllSeHUOy4md4U4vqtdjDH7mSK3ZqKPCrLdinkBZedpJbWXnDjyR4c6ai+6l8wSmpUkFx0dh09++s4iqkI65cSRpMAV6o0bv69YmsW4bXUhO4lXEkYkyk5ESC45ceRjde2/u3yqLLQEvh7SYrmsRW0J+kimFwsmZ98zkdltNPbhvVcuFsXcYQnaBL+eiykmCpEiwp7coY3omyTRaayjF3rI9JZWS+utP3XO7p3dNLQt/Sm+pGoJ0kqSi0IfbjyR/BBqV0gFS3UN6oq6WL3f6Nj1/OlclyAkWlsly94EMSImL0Oo9B6IYx25uop31raEZj2Fh9SyFoUldEYyfjbBFuNZgnv3RZ3jQ0nJ5ZawpCkSR+US1e1k9KVBswbz0e7WTBDoG0//q2pHp7hAlv2NalIRYSnI59TbmalP0KggnRVVoq7lir+qVstaBraXX8F7LeEH6V1+Hu5wM1zFd43n8y1htct+NP7hvjNlJe5t9D/UUDjmxbJr83W6JVSzlt+2hPEFU4b/0hIGNiRH+swkPzjboaptuiV8z13UjtW1kvEpuypv7716BofmzrNeU+YHSSse/pdQ35K1zGZ8sbTC/2YJySjVqf+f/P8SiPYoxuV44fS7LfM9o/XMqDwUgNeiezOAR5C1gIL/zRIAAP8BL8lafhBYAgAAgMBftoTWOraCpx8FAADuAIMdAACAACwBAABAAJYAAAAgAEsAAAAQgCUAAAAIwBIAAAAEYAkAAAACsAQAAAABWAIAAIAALAEAAEDgH3r0MGGqgOSdAAAAAElFTkSuQmCC&#34;&gt;1&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;粗糙集是Pawlak于上世纪八十年代提出的一种不确定数学模型。该模型以有限集合上的等价关系为基础，定义了上下近似两个基本操作。该模型与它的其他一般化或变种形式有着较为广泛的应用。&lt;/p&gt;&#xA;&lt;p&gt;1.1&lt;a name=&#34;OLE_LINK19&#34;&gt;&lt;/a&gt;&lt;a name=&#34;OLE_LINK18&#34;&gt;&lt;/a&gt;Pawlak粗糙集模型&lt;/p&gt;&#xA;&lt;p&gt;Pawlak粗糙集模型是以一个有限集合与集合上的一个等价关系为基础的。所谓的二元等价关系是一种满足自反性，对称性和传递性的关系的二元关系。因为这些性质，一个二元等价关系将一个集合分割成一到多个互不相较子集，形成了集合的一个分割，记为U/R，其中的元素与他们的并被称为精确集。&lt;/p&gt;&#xA;&lt;p&gt;在这一基础上，Pawlak提出了近似的概念，所谓近似，是通过一个已有集合U的一个分割中的集合的并集对一个U的任意子集X进行逼近，作为X的近似。包括上近似于下近似两种。如，设R是U上一个等价关系，则，X的上下近似分别为:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;通过2中近似，我们可以把U分为三个部分，分别称为&lt;a name=&#34;OLE_LINK4&#34;&gt;&lt;/a&gt;&lt;a name=&#34;OLE_LINK3&#34;&gt;&lt;/a&gt;正域POS(X)，边界域BND(X)，和负域NEG(X),这三个域都是U/R上一些集合的并，也就是可以用R精确表示的精确集：分别表示一定属于X，可能属于X，和一定不属于X三种定义。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;进一步的，对于任意两个等价关系R，D定义：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlwAAAAuCAIAAAByC1AEAAAGaklEQVR4nO2d24GEIAxFpy4Lso4pwWpsxmLYD1FJCOAoKrNzzt/6gIRcCA939+UAAADAOefc62kDAAAAWoGkCAAA4CEpAgAAeEiKAAAAHpIiAACAh6QIAADgISkCAAB4SIoAAAAekiIAAICHpAgAAOAhKQIAAHhIigAAAB6SIsBnvCrxtB8An1FL+Y2Lv2njAAAA7oSkCAAA4CEpAgAAeH4jKY59N0yfvjQNXT9eYU0lPneqdY/2Q0AX2nXql/U5QwvspDHlP5YUx946fk14KR+OHtpuz207DkETT0NnFCtK9LfDa/3oXz0QrRs46tQlHpmh3Kybhi4MTnR5ftjWg2UrAW3fqab0uRhUEGGGzIiz3J6GPipZvXZfNJVbQXlBfY1opkHlP7pSHHshuNlpU0l6iFVhXn/0algft1vcrl6XVS7gIc45dY1HshIjkrFd80VlmWh+3wvCtwho+061qM9U5U6KMG5apdFohFmL1Q/ZPtwYTdNZf6MVsbSp/CeT4jR0+ZFzFqA9TVuuKs3KVXV0M2dAYjne1oDjKjh1gUd6zhZFbuy7rjMUHdphjEi6IALavlMt6nOxrCDCeERyzk3DEMjPTjQyz+Ra4LZoJpdRek37II0q/8mkGA+COt2l98/WG1EhW8zLzbW1+jR0yfAUQncvVZyq7tHYy7FCT/KmYRijUCmVmyOSkAEBbd+pNvXpKy6JUMjY2hBNbToGibPUAjdFM9GbWsqJzSr/waRopLNgVZBMiXqbbZ69GU/u2XGeB+8h0+I7Cyqz7PN3w+Ad365MwYb5ulu+TUqDFFPHqdqb8TInxvNp3xGj7fJocWn0kU0VBPRKp1py55LDorIIhYz1xx+ZAUlVU7D9vmiaGWfsa6yr9qulAeW/3++PnHsuKRpHCZta87Myfc84e8rJQluxKzTiGfurkE0Tpsn+jqxx7Oegx5pZJkBj/+r7ZS5Uyylrp/I4qjmM1Z7/8kmYr0adlElrUrwyoLET+Zg2F9CEUztpzp26+vRllkSoFZA/N0ywowXukGiyASvkxM/V8rDyvyYpzi3bdWZcP8uJ29XkBzhZK4qP7YtfEmmJ6pPS6uDm2L+6YVT76LWcOulRXJgeWpTV4aHg2inENk6mF3tnCOhFTjXoTlV9LqYVRCjq1AdY5ucN1mSw3AJ3SPTCnHhALU8oP0yE35IU8zHPzA+SZ926AXepsxtGsRd7xNgC2o7w52jhv90c+1fX93r7v5ZTNQedqCwdPZH+ltnjFH9kk/pULtiSIaDZB+UnSftWFU26Uz8pFkWoqhS/1pUbdpTaCy1wj0STL6dy4qVquVn5zrn3+/2NSTHfJFOqJUTiC0+310LFtKWkznUZv2NuFw3h+/cx9Ks7xpyEVbWcqjnoaA/1AKKWhL5q2T3LB4rXBtTtj2mbAU04VaZNd6onxbIId4xIie9O1WfW6WLukmjcJdfrJ9v0iFruVv474iMX3WNJsTRNsJeKcrSdouNXUWquX4lXyx3wxAmHelX2LSWx4NlskqjgVMUzm1JOjHpivNOdG2IJaPlB07Kvdqf6mWJRhIkat2RqD0mRH0nH7pNo6uXz350eUcsTyj+WC1ceSYp7k3/4jN6vtgZfOWeLZi7bY/LynmnrUX2GRo39fIjaj/MaV1girU9N9So5dcajuCR9iiDCoA9nLGuN8cZPiqNzHQJa16k23amnz7W8vAhNa/TFeAiyBmzL9nsl6nS68iacb9EjanlI+d+UFJdR095rlMjdglho68H2gmr7aejiSZwqbY89p/QUVNCPYoIqq452JdKTqPNOVekhyv7YkeABVZuaf1tY9hHQ+k416U4VfUb1J0SY2ZSMvYxFby4erT5wn0TXIjKOHONztbSr/DT/+w+C12guHbpaHJ4LnnbqKo9ugIBmKq/rFPo8zA+2wOF9igaV/8+TYoVmrz4NCQo+GsxzNl3m0S0Q0EsKMItEn0f5uRY4qpYWlf/vk+LpA6Sr1HlizHEPr0oehoDGNdd3Cn2e47da4IRamlP+LyRF55xr6/91iX3y41X88n9rI6BB3bWdQp+V+IkWqKGWVpTv+Y2kCAAAsAOSIgAAgIekeDnJz70/5Gk/4B9SS5zoE/4NSBkAAMBDUgQAAPCQFAEAADwkRQAAAA9JEQAAwENSBAAA8JAUAQAAPCRFAAAAD0kRAADA8wc89iy1NJAALgAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;可以证明的是，在Pawlak粗糙集模型中&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHEAAAAaCAIAAAAPLyQeAAACbUlEQVRoge1Y27XEIAi0LguiHqqhGYvJ/VCjvLLGNZu9ezJ/+xBhGEAN24PVCHc78IN4OF2Ph9P1+FecEkRM7xhIGIFWeeNCcZowhhBCCNz9/etjKI8J5M8JYYaYhFFbnzMzmxgeS2eGgNl0dEpgENQvJZCsS8slC8wIgUrWEBYROm/McjxhDAGIQFqzOSWIMRpS3RcXe9JZpN2Aqdpt0x4MQGTrbZxk1d8+oWJpczhNiKSUyHoRQceYrOasUNuNjvdRLBVpwYksuSooNhJGYcritHAk6r8fEIxSOTlmC9yB1wFFd3M7+rFRx0ozZc9Flmc5+SxOCbMVVuCSUjeOCyjVRJUwCPIvWiuvYMwDb3tr7koVsc8Gp9VRtnNf3yxMuWkufOGHSMKZSraCrzHMU+qkyvyfCs8g+QWnrDvW+j+YT1XV/QrT2zkBG5ymVMsobzMz92oQp2rf6cJyf+N8ygbOfmDg88mnxtRpXTY38+3d9mCnjhKjOm2p87bRylWc9uztm4sDrhVky4Wtx/Eghta1CK1Y+Q3FK5rRmnFlsolpXSA4dVpy2948kckvc0FFQ+9DMSiXrNN2bBksA1z2vdgmrZnhM4cpNMJ2qQ7iL0Gx0SR41HuMC4C8zGqOx6aWKC6RwGpIVddhGxw+n/ZVIEPynL7pDSURJQKgsTvA6XsUPwio7nDFLaLDbe9SucvI7u3hHKuKUnUpWXrVlbhLpzlqAqCqVDUM1YpRJnpK1Sy6mtDtJk5r0Lw1Zs0eKXfk/dR9kCsb3/F+ehsSIn0m5qvxNZwmRDDOev8RX8JpbXsf6HbX40s4/Sk8nK7HH1sU7gL5/NBzAAAAAElFTkSuQmCC&#34; alt=&#34;&#34;&gt;。&lt;/p&gt;&#xA;&lt;p&gt;1.2变精度粗糙集模型(VPRT)&lt;a href=&#34;data:image/png;base64,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&#34;&gt;2&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;在原有粗糙集模型的基础上，又可以引入变精度粗糙集的概念，变精度粗糙集通过引入一个精度参数&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABcAAAAfCAIAAACDG8GaAAAAlUlEQVRIie3SwQ3EIAwEwK2LglwP1bgZinEedyQo8dpwukce+AcSw7IC9o/BVrYyTKsFfUR/UVotQKnNzMxUXAdACLVaTqKTjpIHyZFEUcGIqIyrWaUjKrzYVLklMRUGBcodoa1EygNxdlLFR0gzTPkUejE8R6B8D10/nwtm7O/GN09mCRqYV5YRR1l9DcuyPlt5v3IAOP7MKTnaHTcAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;，定义&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAAAhCAIAAABP+zCnAAADOklEQVR4nO1aybXDIAx0XRREPVTD6XdCMfkH44RFG2ADydPcEiyYkcYYsI+XQrExjtUEFAoKalDF1lCDKraGGlSxNdSgiq2hBlVsDTWoYmuoQRVbQw2q2Bpq0Inw9khg/Wo+34ABg3prXOgNDs5mwcGZd+X4btNKk3UOzvT6oGQ4huBMIczbSupDSchGoETVrZPKRIxYGjS/ydHegzPln1lkbMMZgspbvSQJCs4IUgkG0gZt0estzLP4/7EkJBcTooDGaWVCR4RmUG+z/J5ZL/IIj1dExn8gf8xUjvOlo/gZVKaXGt/bpGmtQaG2LQ1a95fXwVt8SspjiQfsXOU9FhU94kV6MxNSjUsNCjZtadB6EkgNytT6QyY4QzxaZyun7iqsc8kaVKCXTsQuBoVbdjRo5c9zYZz4k6z0aWZHFgXgAYu41nTJ4Cm5BuXtS1HhJonXO2RQJgNwEA5UFNIgKNPDDCuDFv6M27are8FYwO5UwIPoODIC6tyiHFocwjvCuEX9E+7iWb3jMyieASII4QKLkv4/n2Fp0PM4xJi0VMXZCDc1VneQhAcl4vTRoPLmh5NwBuX13vCIRzNABcFcRBMl2jCf4VE2030xBg3OGOerIz8BD3Jg7JRmuUFFeu9Yg2IZIIPAiyUrTbxlPsPCoNwMSbW/C1EfSvM8WINCjaMGHXvES/XeZVBE6bhBG7b28xnmBmWf4OhYWRUEjJrWoNbBW5zRNSgJ7qWLWO8da1AsA+zg1cX8WSfVNp/hkbVJZr6aRl17vqeGxY31n2HPX2BQ8hYO8fJdu/hGvaMGpTJQBtFJAEU1vVyaz/Cowvglf5HRIkjWFa88Pz8oThPAoMsJ0MH4PeegXXq7DSrIABiEJgES1fZ2fj7Djo9F2msN4IkT4HjrAvyYtwsiht2Ye1CPJ+G8uE3U3DJBI3Z9zXSDRR9Qfm15ampdfO/9mkk45INJiBcvN2gjw87P7dqXdAyPceXeHtYDs1Xv3fSVBsWS8Ll4tUFbGa76HrQEtLqjhv6s+qLy+KgspIf+70EX4KEkfDXDn/ii/kqbceElfJP1e9g/CV0Mf8Kgit+FGlSxNdSgiq2hBlVsjX9sY9kgdo3PogAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;,重新定义正域为&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFMAAAAkCAIAAAB695EuAAAB00lEQVRoge2Y27UDIQhFrYuCrIdqbMZicj/moSAPzc3ErKXnL6MjbFBkEl6rKsx2YJo2+Xra5Otpk6+nTb6efp48RcA8+lJGiMmZY5GnGAQpS9LJzaQyfIAk7ODJCII5Yukcrp/FdL5qh8zLeYqE47DAvOEPM8JNeE+4fx6jWgCLZGzZLW7DX8AlzwiChYZSTvH1NMXWJxe8eclyTFnQZA8heBueOsCZ+Hjl2DXQLJIxOnvdyderRs8IaoyM+Nnkgs/ADpe8MCE/j8NApfJP6ZUCNLDthUxyBk5PaHOamUE6dpYgd5OX13vOgx/OdtNessiPxAGUstmEoRe8PO1Jve6ua0ScJQbRILcjb0RcvAAGvO0gzwiAiRwpdeI4uW0/a2YJXUYUbkBvIzvkd0lTfWDeDJJ7gZeTTvPdll1h1bsLoVe+Eh6ypF8Qxs95bzh5wyLVNdYIEU/LTVy3qUpJlrsVZ3uotV0jEoqaINqz8rkpxsQmNe0fYBbqAr/Pa58uKz1+Wve5gfW8Sp6bLszp4bqk90NOA/e4qg0u9J//ZrcXmElOMq5dE+/Cu3GbSJ4ixAheMZnyff6s3kL6oGaRWx9Y39HP/xv1mDb5etrk62mTr6c/nE5I7YZQGGoAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;，边界域为&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFEAAAAcCAIAAACWK8TTAAAB30lEQVRYhe1YUZbDIAjMuTgQ5+E0XsbDdD+SGIEBTXc36Xspf7U4zAgi7fJ6ni13E7jBvpqfYV/Nn2aFSerZTVWIS+YANBdevB0oVWhd0nTa8uoMQZYzCqrQ4rkr3O3rfo3LtjUJFeS5cE9wBVUECtuVdbELVYW0yo1bnoRjb+jnY2u6IwCs2R7UmsQOozARgTh9GE8EAIWqspKoQh1IUMuxaqy5sCZmT7aKFKfJxNbENFRe42mSDXYVCsGik4OatWRf2VVYqjsJ3XACyROZHtzGgxNLIjgBQppN/wG5khVJ6TI9FlV225Npjg4LcJw5GeADNOvjgf2rfTxQt9wPwk2QDXdajmO34Pi8ZudoWSp1e32D/gWTNc7PWHMVIin2VcCOc5pt/7KZNgndcLvcx8Gm2vZAc2ta7imEvlOaR5JfWl0bRlz/8nRQjlvvaOvZfVZdenzx5+6zYbsPV2oKc2EsNNCGx5EDrG+AQbvFc8egJAZ9ux8eYdvuHAzSUe545AyucGGSCnqkfZ97ZjuOZouFn3qfr7Ejt654BnPYlJ2dw66wrpzBhfm16gTgNs0qy9Hw8q7s/MTu0lyYmCm5jLvXNb+fr7C3xPyV3aI5+zF0gX32f0P/Y1/Nz7Anav4BBdfEmcOg3+gAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;，负域为&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAcCAIAAAB56a/tAAAB60lEQVRYhe2Y2ZnEIAiAqYuCqIdqaMZi3IdEBcUjuxN3vpnwOJHj54oZiF8m8N8B7JYH+NPlAf50eUtgIeRwSSMwkiydTMCBEQAAwLrKP4+lcSZUPw5MSxCBsTZnjJ3P9G8kp95KmmyFhZzohYoloTol5nHMKTJGhJpM+uLQdiNrQ+lrKzHAQojomMlWAmObkMAs2YBb7xiF5qE0uau8aNedFl5A1sCBWZrEGdNCxik57e9HrJLSlUm0hTgwdvMyylmM0QCfAFXz6AVieOvNsty3HZkO4REYD2hXrChg4eOgaZ6at7uqXsE7acclF86WMQLqJOlRPJR025qQ6iE6+nm0Xic4s0DHI1OdGniCclBNZGrrwcJK/aA1XEeLpZ8AB0Zk4Walegf7wABQ3sNmA51qquqTiNwKJ7XFDd01n7dUmBP3gQ/YBKzRCgDYAXZclUT5lez4z82eFbqBmp08n/R+4jSw91ozmm5q6x8PCnQ6pUHI9yXLUnvwLxeT1h88BgBQl8fqYCpevZyHi6i9izbuhZCDM/PmPaztJAvWto915T28SUpVm76aRjuTizetLaK62BmkvyGvaO8GNvX1olv96nEs/+Jr6XYRQiIczGA6dfv38B65TvJy2Qk8+srZJm/5F8+d8gB/uvwAaPuG9PA2Px0AAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;1.3概率粗糙集模型&lt;a href=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlwAAAAuCAIAAAByC1AEAAAGaklEQVR4nO2d24GEIAxFpy4Lso4pwWpsxmLYD1FJCOAoKrNzzt/6gIRcCA939+UAAADAOefc62kDAAAAWoGkCAAA4CEpAgAAeEiKAAAAHpIiAACAh6QIAADgISkCAAB4SIoAAAAekiIAAICHpAgAAOAhKQIAAHhIigAAAB6SIsBnvCrxtB8An1FL+Y2Lv2njAAAA7oSkCAAA4CEpAgAAeH4jKY59N0yfvjQNXT9eYU0lPneqdY/2Q0AX2nXql/U5QwvspDHlP5YUx946fk14KR+OHtpuz207DkETT0NnFCtK9LfDa/3oXz0QrRs46tQlHpmh3Kybhi4MTnR5ftjWg2UrAW3fqab0uRhUEGGGzIiz3J6GPipZvXZfNJVbQXlBfY1opkHlP7pSHHshuNlpU0l6iFVhXn/0algft1vcrl6XVS7gIc45dY1HshIjkrFd80VlmWh+3wvCtwho+061qM9U5U6KMG5apdFohFmL1Q/ZPtwYTdNZf6MVsbSp/CeT4jR0+ZFzFqA9TVuuKs3KVXV0M2dAYjne1oDjKjh1gUd6zhZFbuy7rjMUHdphjEi6IALavlMt6nOxrCDCeERyzk3DEMjPTjQyz+Ra4LZoJpdRek37II0q/8mkGA+COt2l98/WG1EhW8zLzbW1+jR0yfAUQncvVZyq7tHYy7FCT/KmYRijUCmVmyOSkAEBbd+pNvXpKy6JUMjY2hBNbToGibPUAjdFM9GbWsqJzSr/waRopLNgVZBMiXqbbZ69GU/u2XGeB+8h0+I7Cyqz7PN3w+Ad365MwYb5ulu+TUqDFFPHqdqb8TInxvNp3xGj7fJocWn0kU0VBPRKp1py55LDorIIhYz1xx+ZAUlVU7D9vmiaGWfsa6yr9qulAeW/3++PnHsuKRpHCZta87Myfc84e8rJQluxKzTiGfurkE0Tpsn+jqxx7Oegx5pZJkBj/+r7ZS5Uyylrp/I4qjmM1Z7/8kmYr0adlElrUrwyoLET+Zg2F9CEUztpzp26+vRllkSoFZA/N0ywowXukGiyASvkxM/V8rDyvyYpzi3bdWZcP8uJ29XkBzhZK4qP7YtfEmmJ6pPS6uDm2L+6YVT76LWcOulRXJgeWpTV4aHg2inENk6mF3tnCOhFTjXoTlV9LqYVRCjq1AdY5ucN1mSw3AJ3SPTCnHhALU8oP0yE35IU8zHPzA+SZ926AXepsxtGsRd7xNgC2o7w52jhv90c+1fX93r7v5ZTNQedqCwdPZH+ltnjFH9kk/pULtiSIaDZB+UnSftWFU26Uz8pFkWoqhS/1pUbdpTaCy1wj0STL6dy4qVquVn5zrn3+/2NSTHfJFOqJUTiC0+310LFtKWkznUZv2NuFw3h+/cx9Ks7xpyEVbWcqjnoaA/1AKKWhL5q2T3LB4rXBtTtj2mbAU04VaZNd6onxbIId4xIie9O1WfW6WLukmjcJdfrJ9v0iFruVv474iMX3WNJsTRNsJeKcrSdouNXUWquX4lXyx3wxAmHelX2LSWx4NlskqjgVMUzm1JOjHpivNOdG2IJaPlB07Kvdqf6mWJRhIkat2RqD0mRH0nH7pNo6uXz350eUcsTyj+WC1ceSYp7k3/4jN6vtgZfOWeLZi7bY/LynmnrUX2GRo39fIjaj/MaV1girU9N9So5dcajuCR9iiDCoA9nLGuN8cZPiqNzHQJa16k23amnz7W8vAhNa/TFeAiyBmzL9nsl6nS68iacb9EjanlI+d+UFJdR095rlMjdglho68H2gmr7aejiSZwqbY89p/QUVNCPYoIqq452JdKTqPNOVekhyv7YkeABVZuaf1tY9hHQ+k416U4VfUb1J0SY2ZSMvYxFby4erT5wn0TXIjKOHONztbSr/DT/+w+C12guHbpaHJ4LnnbqKo9ugIBmKq/rFPo8zA+2wOF9igaV/8+TYoVmrz4NCQo+GsxzNl3m0S0Q0EsKMItEn0f5uRY4qpYWlf/vk+LpA6Sr1HlizHEPr0oehoDGNdd3Cn2e47da4IRamlP+LyRF55xr6/91iX3y41X88n9rI6BB3bWdQp+V+IkWqKGWVpTv+Y2kCAAAsAOSIgAAgIekeDnJz70/5Gk/4B9SS5zoE/4NSBkAAMBDUgQAAPCQFAEAADwkRQAAAA9JEQAAwENSBAAA8JAUAQAAPCRFAAAAD0kRAADA8wc89iy1NJAALgAAAABJRU5ErkJggg==&#34;&gt;3&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;概率粗糙集模型是变精度粗糙集模型的进一步泛化，通过引入两个参数&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAbCAIAAACImfpDAAAA30lEQVRIie3UwRGEIAwF0NRFQb8eqkkzFPM9LFmEoKOu4h7IkYHwQgDhH4S8DSAnosREWEyExVhEikEsoC8gUgwiISaSpGLNGIVIMXwFJhqN2DW0CIW1LC+pDvFyKKokijql9GemGATain81WIltRkO47RQBuIXQngMVTVrZGM+v6QaCM/iCZcNQvyFbesXkDH5ELH81qpDudfw09RSlb6hTiJuZ+wBASSqaLbd8O4b1fG9iuZjlbeY5+Yf1VZ96MXnP8l13+c9+Vt26ByN898cjjhoeRBxsxbOIEzERFgtAyO1vipj9nAAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;将正域、边界域和负域分别定义为 &lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUMAAAAqCAIAAACm3a52AAAFIUlEQVR4nO2b24G0KhCEjYtITgTGQzQkYzCcB0eFvtB4aWeWv+pt1YHmswuwdacMQdDf1/TtACAIekBwMgSNIDgZgkYQnAxBIwhOhqARBCdD0AiCkyFoBMHJEDSC4GQIGkFwMgSNIDgZgkYQnAxBIwhOhqARBCdD0AiCkyFoBA3q5DSHuJz6xRLDnJyi+eM6DzN385ymaZoGTUJRbjBViGmeBCnt1Rezi47T6yhSPD+Yfi0x0BCq+D7nymNz+vzuAuW2RIxHdEsMJRh2eL1YvhePxypJgElHdYXnfuX5aAxcDTXycju9xNmRqhPMVU2Uaa7GvzYvmoTmZtFpmos/P8z9Vj8Z1h5odaqKzPj1HdX9CBR5aOvBIrglhjrYz7123Uc0cTzAs3QyN576MxMXD4TQFPMwza7ToztMo+9myq085KluO0oQOm9jWW91x0WwShgOXqbTKaOW5hCCcN/KOITk1PA/phbM/ARP4mTtFAvLwMXzNue8xJj2BpQ5MM1+NP1htjsnvVOPynNYtX6wRhw3MIYND1pLDCpWA/l5pblOGzr/LjEmhoncSzE5fZcRe067ylNcfrudbOOqgNN0Y3tG2rSL/GDuajhZ8GCxBjSSqN4JrjPgGw915rPE6qHYINXTyknVRubrwSfV2JMMW8aFTHBclXsw3OV5ZU22cVXAaYHJeQut6BWYzZbpg9qOqD2z0XNvPNPp6V6o6z5KG9nrItUqYV39lP+q8EkCaiH5ObkDZr7HkzwM9zrZxEXLg28+Cyvyh5kbTl4X1lAUBZmte418HHWFaDuwFTa56il31JOoWO0q1o8tOLIlVEfml5hd09ktnqSy1elkE1fVG33eFGc+c669LX+YWXdyO50bGaSWEzpDvS6D1xJDiImWgOULH7ufrC1KrvLstmUUql1iQI4rTNe0eJ1n+Wx85jnZxEU6I+879eR0XaydYa7SnNzunL0TqU5sZ1gBQU3JR9QKea8iqJFX12qwTn/DQKtdNJfI4vvpulh4GvG4lq57psUbPLl7u5xs4+rIW6Vq/aXMzI8l56W+lSmsTlNeg6tbPf86at8Hhfk/Hp06yCoQe8Vtjv2cmS0j59qz4kOIcnvJHXgNZr7PU9xRT0xiUwYu5d4dM4CcuGQQd2DGhVXZXGHuUvYwfXMD/QBEqnORsq1uc0sH3jSHuAj1eKmwJ79gN2bIxmk1yZSWJnKbJitjaMBC6rEC4lswhfBu8Ty5Jpu4xFDoQeFdCn22vgcz85dFr8AULj3UHlJdKuAemhO5SHvJcuSm/t5ty7ijGX5jq1+XI9k66xmd+T65x8l1RwKB4gI2gVfrhyS21XGHmV149pBknSu4VFh14vGxkFb5d9EWTw4zM55vwBSPeqvCtQ+6tRoevyhnQOMTxwuyX+H/nH4XZjZ49jv5NVEj2zwlmJnz9Id5r+2Lqt//7QWy1rct5RZKXUfyXWKPf+D1gn4W5hMNvC22v7F4ijDpH58rXWF+y8nlN5/bLjyEmGhNY7uomPeKjZD6tusSsj+Xdqt+E2b+mzxrDDZPGSZrKG+X+MH80vammMmO+k25eagGneYwz4E+PrQqjP/U/yebMHPJ8w2YRoO/rHqwVnLKMHNj+O//f7KzOiqES0pb5Ud8sXC2xDiu+lisPAHT0G2Y3W08qy+WHJoTzcFCpHJpZhtZ1qz9oQiYPboDM3+L588VDyEIuiA4GYJGEJwMQSMIToagEQQnQ9AIgpMhaATByRA0gv4HcFPRS3/biycAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;值得一提的是，这些参数并非一定要考实验者手动选择，有时可以作为数据集的一种性质而导出，比如当我们只有两个X和~X两种结论时，我们可以通过使用成本矩阵中提供的信息对参数&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAbCAIAAACImfpDAAAA30lEQVRIie3UwRGEIAwF0NRFQb8eqkkzFPM9LFmEoKOu4h7IkYHwQgDhH4S8DSAnosREWEyExVhEikEsoC8gUgwiISaSpGLNGIVIMXwFJhqN2DW0CIW1LC+pDvFyKKokijql9GemGATain81WIltRkO47RQBuIXQngMVTVrZGM+v6QaCM/iCZcNQvyFbesXkDH5ELH81qpDudfw09RSlb6hTiJuZ+wBASSqaLbd8O4b1fG9iuZjlbeY5+Yf1VZ96MXnP8l13+c9+Vt26ByN898cjjhoeRBxsxbOIEzERFgtAyO1vipj9nAAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;进行求解。&lt;a href=&#34;data:image/png;base64,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&#34;&gt;1&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1.4其他粗糙集模型&lt;/p&gt;&#xA;&lt;p&gt;除了上面提到的3种模型外，还有其他一些模型如模糊粗糙集模型、代数粗糙集模型(如粗糙群、粗糙环)&lt;a href=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHEAAAAaCAIAAAAPLyQeAAACbUlEQVRoge1Y27XEIAi0LguiHqqhGYvJ/VCjvLLGNZu9ezJ/+xBhGEAN24PVCHc78IN4OF2Ph9P1+FecEkRM7xhIGIFWeeNCcZowhhBCCNz9/etjKI8J5M8JYYaYhFFbnzMzmxgeS2eGgNl0dEpgENQvJZCsS8slC8wIgUrWEBYROm/McjxhDAGIQFqzOSWIMRpS3RcXe9JZpN2Aqdpt0x4MQGTrbZxk1d8+oWJpczhNiKSUyHoRQceYrOasUNuNjvdRLBVpwYksuSooNhJGYcritHAk6r8fEIxSOTlmC9yB1wFFd3M7+rFRx0ozZc9Flmc5+SxOCbMVVuCSUjeOCyjVRJUwCPIvWiuvYMwDb3tr7koVsc8Gp9VRtnNf3yxMuWkufOGHSMKZSraCrzHMU+qkyvyfCs8g+QWnrDvW+j+YT1XV/QrT2zkBG5ymVMsobzMz92oQp2rf6cJyf+N8ygbOfmDg88mnxtRpXTY38+3d9mCnjhKjOm2p87bRylWc9uztm4sDrhVky4Wtx/Eghta1CK1Y+Q3FK5rRmnFlsolpXSA4dVpy2948kckvc0FFQ+9DMSiXrNN2bBksA1z2vdgmrZnhM4cpNMJ2qQ7iL0Gx0SR41HuMC4C8zGqOx6aWKC6RwGpIVddhGxw+n/ZVIEPynL7pDSURJQKgsTvA6XsUPwio7nDFLaLDbe9SucvI7u3hHKuKUnUpWXrVlbhLpzlqAqCqVDUM1YpRJnpK1Sy6mtDtJk5r0Lw1Zs0eKXfk/dR9kCsb3/F+ehsSIn0m5qvxNZwmRDDOev8RX8JpbXsf6HbX40s4/Sk8nK7HH1sU7gL5/NBzAAAAAElFTkSuQmCC&#34;&gt;4&lt;/a&gt;等，这些模型都是原有Pawlak粗糙集模型的泛化，可以根据不同的需要进行选择。&lt;/p&gt;&#xA;&lt;p&gt;二、决策粗糙集&lt;/p&gt;&#xA;&lt;p&gt;决策粗糙集是根据一张决策信息表定义的粗糙集模型，一张决策信息表是一个四元组{U，AT∪D，V，f},其中：&lt;/p&gt;&#xA;&lt;p&gt;U={u1,u2,u3….} |U|=n&amp;lt;+∞&lt;/p&gt;&#xA;&lt;p&gt;AT∪D表示属性的集合，分为信息属性AT和决策属性D两个互不相交的子集&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;通过属性的不同取值，我们可以对U进行分类，而根据AT和D两组属性，我们可以在U上定义两个等价关系，R&lt;sub&gt;AT&lt;/sub&gt;与&lt;a name=&#34;OLE_LINK8&#34;&gt;&lt;/a&gt;&lt;a name=&#34;OLE_LINK7&#34;&gt;&lt;/a&gt;R&lt;sub&gt;D&lt;/sub&gt;。通过根据这两组划分中的元素的相互关系，我们可以将他们间的元素联系起来，这样我们就得到了一系列的决策规则用于未知数据的预测。&lt;/p&gt;&#xA;&lt;p&gt;2.1完备信息系统&lt;/p&gt;&#xA;&lt;p&gt;完备信息系统是指信息系统中U中每个元素ui在AT∪D上的各个属性值确定已知，没有未知或丢失属性的情况发生，对于这种信息表，我们可以简单通过观察属性值是否相同来判断两个元素是否等价。&lt;/p&gt;&#xA;&lt;p&gt;2.1.1&lt;a name=&#34;OLE_LINK17&#34;&gt;&lt;/a&gt;&lt;a name=&#34;OLE_LINK16&#34;&gt;&lt;/a&gt;不可分辨关系&lt;/p&gt;&#xA;&lt;p&gt;当信息系统中的属性都是简单的定性分类数据时，我们就可以简单的在U上定义两个等价关系，分别为：&lt;/p&gt;&#xA;&lt;p&gt; &lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;通过这两个等价关系，我们得到了U上的两个划分，将U/R&lt;sub&gt;AT&lt;/sub&gt; 中的元素记为pi&lt;a name=&#34;OLE_LINK11&#34;&gt;&lt;/a&gt;(i=1,2..|U/R&lt;sub&gt;AT&lt;/sub&gt;|),U/R&lt;sub&gt;D&lt;/sub&gt;中的元素记为qj(j=1,2..|U/R&lt;sub&gt;D&lt;/sub&gt;|)，如果有i、j满足集合pi属于集合qj说明当一个元素与等价集合pi中的元素在AT上属性值相等时，就一定有i在D上与qj的元素属性值相等，即可得出一条规则pi-&amp;gt;qj。&lt;/p&gt;&#xA;&lt;p&gt;根据这一思想，我们可以引出更为一般的决策规则。定义信息原子表达式&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJwAAAAiCAIAAAAlGicfAAAClElEQVRoge1ayZXDIAx1XRSkeqiGZijGczCYxRJIONjPjP5pJnG0fW2QbLtiOWxvG6D4PZTUBaGkLggldUEoqQtCSV0QSuqCUFIXhJxUB8b6CZbs+757a8BNkv2PICPVW7PNjbq3ZpuXNB04iM452Cg8YV2pPhjlraleoSAgdT6jz+pB1EbCHJx/emuyIM5sU4g9dQY56BK67xJSM0dn4wVajzIIDqaKrUPr7GNdpMymwyxmVLikPhznBzMoqANAq8DBMw0XtSnXLCGASWpz1MVmb6zlhCAOjPNBpNEg+vpasqHDGj2ZQeBwDx2IxHT94ttZFqq3RpBaPFKvraB45+xa7D2i7mqXwizzlKFltLZjuNCSpP2mpbX9YtuZ28PuuwE8UqkmhISeqb1ORKik5+8ztFwF8HBWACEVL7W2wJZfbDszOVJK75FavyqaP2UmXqxOTnG0XBoar/3WB5f6+ZGB2vSLb+c5DUb27Ruk1vu1LARphiGUZqSytMhGTpKUBGNiJQO1MJz0i29nsGfsCDU+U5HtLD3TvxoKIqkHo/SmlkKauE3mH8AcFA/U/FOEX1w7j8ZvBu/XxrffPI8dbMYEZyx6LMB7G1naWfdhapH0iesZHinKTp3iTvX84top2DkR3DinltdW4T9jfZ3i6SxSJ0UzZOFxUgv6od6syodavk0XL9aTjzxxUGt4p776drZOkKFZQWOLnnKjhPYewWXMSxeFA3jwhikohNTByBBJLvS5tGKcYm35rp7XIXHqRwpN6i902su/penG+8Kp9KjzCUbfuD+MnPZWuAnfp94Y8vp9ahthGhuAeCviHBIv/eXDl0EUmJL6ZRB7mpL6YVD7h5K6IJTUBaGkLggldUEoqQviD0AoRYNKApd8AAAAAElFTkSuQmCC&#34; alt=&#34;&#34;&gt;表示满足信息属性ai等于v的所有元素集合。决策原子表达式&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIkAAAAlCAIAAAD5mF0LAAACfUlEQVRoge1Z3dXsIAi0LguiHquxGYvJ96Ax/oGQ9e5yz2EeNwkzYRA06y6DVrhfCzCgMG/0wrzRC/NGL8wbvTBv9MK80QvzRi/MG73geRPBh4Rec8455yC+FpGCZzx9gOj/wt6bFPwuHSl4h5vHATPC50TV4R4fBn0voGQ2BT/8cm29YRhzJGM8qjNEORFtnJKu7y3ISUER0SugvYnAycUc9R227hwiijAvlFy433InBT+QRZjJKW9Yi+acNdeuFg4RzYmp0b/V24bqQBJNeEN2kLtB+hCZjebus/XexcJeUHKJmo5NbxkQa4iVw1EuktFLSMGvY+HeYO/QS5NV2/BS8zLpC4pNxOu9K4pBG+0opVwio5WwamYFuDfYOyzSx280Y8XAqu2X62yiOQwB7LV2VbZTLpDRhCKckXuz6JSiGdCXzPTkE49PNLURqqdFoBoXVfUb5QIZtXNTx8ZL7M0wjuXz85koq/d7LBAQof0aoZcNG65ygYyS2Y0z0nnT+xXB+3487w/4JSp2402wIxpjMrOCDHFWiW2U82VkEZ7IVNEj26eNnTKCcxCQPpu3N6OC/Cu+BPIlERExRBbc/X3LY+cL5QIZjErINkvPN/WLA8S7E9w0w0J7Nr/jTowsmGb/ghFhj1KNfv2pZh30jXKmjGtzNMm+AXgf0pHvAg3r4kgX2AGYZ91vQaD8ECHUpuFD2n/rFLizsiaFwE62rBD+NSTKDxGW9NUS5X2H5uRssoY5BOrNeowRKT+Ee7E0m/zP/7957pHup1tdinrZj3DvSkId2/a/p1ZEMG+0IlLnG8MvEcGZN3ph3uiFeaMX5o1emDd6Yd7ohXmjF398uo8Q2Re48gAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;表示满足决策属性di等于v的所有元素集合。&lt;/p&gt;&#xA;&lt;p&gt;现在对信息表达式进行定义，如果&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAhCAIAAABWXBxEAAAAkklEQVQ4je3UzQ3AIAgFYOZ6AzkP07AMw9iD1SYFlf4cmkbOfj6ERMoPihb+EVYGWO/hvp3jgZ3ikR1gZVCrJHGsDAJrzVWGyz0saY+qPde7DCY6+eNke2+7bZZ8wVoc7nmUbGYlycTbN5cdAWVR/SV7ya35c4yXHLTKsFNzsROSszIi2LVR7JYkO75Pf0MLv4Y3qPIxo0q+Q44AAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;是信息表达式则&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIsAAAAmCAIAAAB7+f+YAAABsElEQVRoge2Zy6HCIBREqYuCqIdqaIZieItnPgYI3BtIRp1ZRj0MnpAQNYnBjnm6ANMIDaGHhtBDQ+ihIfTQEHpoCD00hB4aQs9lQ9Fb6yMCZDb2oZJXDVHQbOxFQxQ0HXvNEAXNx2oNRW/NGhfugwTXfOeQbiJavVRwpqygu6TGUPTWWB+XEyB6q/gedJDgjHNnjoZ0U9DKjipLRFRSbii4l/Nl+GW8+ZDmjIZ009EKpSp+hCVNOiy4ctaxN9o6/jpid3SQ3YTLcx/STU87lqouKxm2Zw3thnpO0GH+BUcPC0rvN53GBa4fKzRUXaEdN/A9TwzJX8qOtK8es0tuI1T3aPKSyjV08yah/K0cjj61Scg+5kLdohwrNPS6aVn7f+fa5EtOTymkey9bwa7vlV3ylDON3p6OIy2peh7KSrZaTYN0YPd8MR2gpMZQiS+e/hBIH1YPRyipMFTkS3sPgXRiF7b4wQiipOqJNecHV7iozoZ0YlNKAwXdXvLr/8GL3m1bJ9SclfwBQ3bID6hTc1by6w19fGgIPTSEHhpCDw2hh4bQQ0PooSH00BB6aAg9fx1epYKxOiUQAAAAAElFTkSuQmCC&#34; alt=&#34;&#34;&gt;也都是信息表达式，同时所有信息原子表达式是信息表达式。&lt;/p&gt;&#xA;&lt;p&gt;同理定义决策表达式，用&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAfCAIAAAByEJoXAAAAlklEQVRIie3UyRGAIAwF0F8XBaWeVJNmKEYPjGExkBw84AyclOXxI4y4Pmo40IF2hYQAIHEur5mTPpchkAQTCdXZmdOzru22FaCDbCdzanJGErUrquPGeUNCE8eN00NNIHWijFEaiTqWImSVOX5sPWZ4h71OVIPNCTuRCa0cLT4ATbd0thkhx4lDThPC5D7s/Rs50F+gG9s7LIQF46mPAAAAAElFTkSuQmCC&#34; alt=&#34;&#34;&gt;表示。&lt;/p&gt;&#xA;&lt;p&gt;用&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADUAAAAcCAIAAABOJ2DZAAAA+0lEQVRYhe3W7RGDIAwG4MyVgZgn07BMhrE/+DBRgiiejXe+P2tDnwTkCovvwL8BB/l8c3mvjwmR+EFKM6bPB8/0OeFZvgnevZ3tfEwINSFeWZMJrdIYAACqX/aSHm3rlI8JAYlLGRPOCKE5xhhWBBOW9eXHhi+G/I3SVeFeSh7Htr7NY0KjnepbMXXoFayiDsBQxBoSsvKs4QnfMG807eMUg8Gz9glE4X2bm2ZszL6OIT3v6fbzm341lm5r9dXOvD5uke9H6g2xeazP6brVeeMHLzB9/8UwceJORFwsB1E+fzzle4p35nf0/eyO9+L/pz7y+eby+ebi3fcDVcgo4aUgmNkAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;表示规则&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAhCAIAAABWXBxEAAAAkklEQVQ4je3UzQ3AIAgFYOZ6AzkP07AMw9iD1SYFlf4cmkbOfj6ERMoPihb+EVYGWO/hvp3jgZ3ikR1gZVCrJHGsDAJrzVWGyz0saY+qPde7DCY6+eNke2+7bZZ8wVoc7nmUbGYlycTbN5cdAWVR/SV7ya35c4yXHLTKsFNzsROSszIi2LVR7JYkO75Pf0MLv4Y3qPIxo0q+Q44AAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;中的所有元素属于&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAfCAIAAAByEJoXAAAAlklEQVRIie3UyRGAIAwF0F8XBaWeVJNmKEYPjGExkBw84AyclOXxI4y4Pmo40IF2hYQAIHEur5mTPpchkAQTCdXZmdOzru22FaCDbCdzanJGErUrquPGeUNCE8eN00NNIHWijFEaiTqWImSVOX5sPWZ4h71OVIPNCTuRCa0cLT4ATbd0thkhx4lDThPC5D7s/Rs50F+gG9s7LIQF46mPAAAAAElFTkSuQmCC&#34; alt=&#34;&#34;&gt;。即如果元素满足表达式&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAgCAIAAABCTrX+AAAAfElEQVQ4je3TsRHAIAgFUOb6AzkP07gMw5jCoEbBkM4itD7+IZ5UvhT9+gQtDLAEtYNt7WFTu3jSwqBWKW+0MAgsmiyM1avO6Q7TMbTZ0P2ozdzaFx3FVcfGmLN3FxxvKQwioO7PWXZ57DsnJ9HSAdx1BI+v847P/2nH6wv5pzEE+KJs/AAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;，则判断其满足表达式&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAeCAIAAACqmwlGAAAAfUlEQVQ4je2SyRXAIAgFf10WRD1WQzMUEw8YgkpicvC9HPTkMiOL4vg4sIUtLBCYACBl0aXkZHM9AnEXgUl3K19nzTbmvOTk4nrBn1y8vyWoIeTt+lZwAYzv8SAlYuNHui/a2oezieMIHs7185XwzI8CU5zKnTDj//691wgFBK4w0rbwGEAAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;。这样我们就定义了最一般的规则表达式。&lt;/p&gt;&#xA;&lt;p&gt;根据粗糙集的正域POS(X)，边界域BND(X)，和负域NEG(X)，我们可以从决策信息表的两个划分中定义以下两种规则&lt;/p&gt;&#xA;&lt;p&gt;1正规&lt;a name=&#34;OLE_LINK13&#34;&gt;&lt;/a&gt;&lt;a name=&#34;OLE_LINK12&#34;&gt;&lt;/a&gt;则：如果&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH8AAAAdCAIAAAAM4yQVAAACh0lEQVRoge2YXbrtEAyGOy4Dyngymkwmg3Eu0BKJ0rL7rJ5+V2tjo6/8sflPz2l7egP/tT76T+qj/6Q++k/qo/+kPvqZCBzy3CkZHZDZu4o+wabI2Eg5uBp0dAc4hLMZee8ZnbW9+xNbh7rS9gkKlgGi+ELZyOh2yvuA/c/QO5/SMvQn0y+kz+gEKHEcgaVu6qmVQBhO25WvqVpkugz+C+kXVru35FxlfxCjOzqqSRhhMqjFhh+lnvB2dOZBtwoASdH3T0O5yi0zdZO9oB83ttA0rbisZ67L56QuU9h+BMbo9K8d8VABv4zY5uGqfZHDGvus42NsBfKeIPSYRAZUhwIZeZqGNuTzwYBd5ijVUfTCP1pXuIAGJVWeM+GrxyziPoFpY1XMaXmiblH5KgZJtTDK1p/OX6HPzHHBsA+CGX7XSd8G2v/pmkWJjZgJN/UwolKdzo4/5k73q9cc+Of0CQDtu8EA/jZ8y/hLu6+XO5v1kgwvPZBr8LM4cDUReu8L+iHKpdycYt7ZBIpM05bbF5cqLdeKy9qCzKsUI3lTqjQEj2Qb3Y5h1jxaPdL3KFCNK5ND+wTKqerMByQGtZa6cS6i3he2k5ap7uhpe53G36z3f05MxARAdXoY1/hdNxlF77/9/V13tUJNokXIcQ3yJ9iAxvKgOvRn6cdvJwBiRLp9I2g9RUrFYJ/hN+JTGv3EG+dCpQ/PgnRwgluu0PO+nzg75Lx2sPLAM+/7fy5GpPpTibJnpbEKcXj1+GPkCfYt9BkR5D39iAxEKTvHe+z81feX2xHfewf9dBHJHZ8RAI5yaP7LdLH6tcL3HfRrMQJSFgUWwr+hN9KP5Uv2hrDqnnxXb6T/O/oH/MCpGEoCNbEAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;，则有：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX8AAAAuCAIAAABPg47WAAAFG0lEQVR4nO2c24HzKgyEU5crORVQR0qggFMHzVCM/weDzdXLRUJxMt/TbhIbjMaDkEleOwAASPCS7gAA4EeB+wAAZID7AABkgPsAAGSA+wAAZID7AABkgPsAAGSA+wAAZID7AABkgPuAfqzelCm9+tq0FegPeCZ97mP19vL8rTOjzg+/CmIFjyXUQUyoCqhlKUaRW395lqFjwH16+zN0kAD1W4q+9yvbYsDqraB0q7eo949WS2iG98GJP5l94nr7GDKjGdLDdOhJT8yXz8J9Yqzekonaq4c+BivbouUH3GffXTiuvhzRCfuWvnJMKsHQGBX866YclqmMccQYTw/3SYj04mHSzcq2SPkN98m7EtrREadyquNfNSqJL8dSJmuEHDb/+Tb3Kd8XHRQdoTCrUbCyLVLqC8dvcp88PJe1GFUJUZTQZmewWhGHlTnxcTA53LT7+MVCNCMkI75UT5PNVRyBxRLm28psoHrh5TrG4EAN5z4iaqlc+l+L3IJ1+Gyn6j3pcvpomnEiqdVlSMPdVv55v9+9p6XJfVyg6qJcOptNGUXNEQqr/mkm22qfkFzCb9RxyukEcW7ltVIt1SHuPTJYEN/pK3/P+QDPDVAeLepw703jKOY+tx4vkUu7kA8M+mPcpz2J949i6dQ4WfdZppaJMsuRw2xBaun722c+16scKVBJQfTh3qthCR1Hzn12o6qaodbTTRpNkGw+ZeVVGYjSJkBr3QHHm0ZNR2O27rNILbPmUz74ZtlVnzWYankFBTGEey+Px/v9/iD3qVylYO4z0OpTqs69M9q5F41CjbPPvBapZcJ9blYa+UaJ8A3/jtU6bvrmmse5yZ4Jw70XwvLOGDgrVd1H6WpdKjoomDXZanGVZ6FtFOOZmBlRRJva2ms3UZf9XD0u9b07KLN1nya1EEhl+FHNbZmjnPzEeU8+QuMVqBsqQ0wc7r3c+2HTOaFwn2OB6cvifrlZO8jHheoWLvVxPMpP2m3YqudwRPz9mESpMygz7tOhFgKpDN3z1ewmfD/dVFiqNSc7FRn0XlA7fbjrN5Wk+wTPAbL/qge5gWDaQVDNCI7x0zfl3LZixnEJs0Jqb+t+JZI9WC2P/jUkvuHkc51B+cN93PhMq4VIKj1P3OPA3DQbnzSv/F67goqRSQQwLqfE8lnC/bn7fXoPEvsygdVq25Qhedy8eP8Se2MtQemu96v/J9XylO+dDGCNsUYpk5eH+ul3ht6BTZNaMla7j1EvZUieAXZytknzyGeZ+ywyH46gTKpFTCpLOOKaLTyH6PSf3oHl+yrHYve5cvK0JmfOD/yd9Y5AueBb6D7LzCcKSiVh72ROLVWp7E4tDVJpyNdkvM1dlVHK+OxnalNQT7GzL9ys3yJb+Ps+/uBN26Rudw2EMT4ZddsWqDj7QnAv32z4IIe7rUpQSLx6XC3/qZpU9uvO5ZIKP17vcZHmGPPxbKjl9306w809933AbxtarZS6Vr/0X8Tb2yIDTs5qxMo1ZhuhWlikIoXV2iTDzb4gCJp2f6wNt7j7WK20CS6a51vA31m75OIqhdLUJeiI1PJt5qNU8oiQe0FwNS0UblH3cavV9DHHZwn+56g/JpYlUctXScVvSzrXPisWBFfTUuEWz30AAAn8C4LPAO4DwCfxSwsCuA8AQAa4DwBABrgPAEAGuA8AQAa4DwBABrgPAEAGuA8AQAa4DwBABrgPAEAGuA8AQAa4DwBABrgPAEAGuA8AQIZ/yYdDh3SayLIAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;2边界规则：如果&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGMAAAAmCAIAAABfzLIdAAACJ0lEQVRoge2Yy43FIAxFqYuCXI+roRmKySwgib/gaKSMJvJdEjCXgzG8V45UTOWvDfwbJamoklRUSSqqJBVVkooqSUWVpKJKUlElqaiSVFQfJdWgYn86qGOF5n59g1SDonV76lhHE1/c1Tw6m0GKxaNjLXrFbPz8TNugzaEe4rdyqgFd1rDIltNAtoxGYrxj5WzmSukoG5M7B7e1DPASKblZI2GIowa1VsM1Na2XpQJxtKaNu7Nz2hxWL5FqwDdT7m5HbIqEWAlfJg81hi0TSsToWF2mJm+L1Hl856Rj24ywtJKoY6Ai3h/12esI2BU/XpYdUCSrVlWGzQ24wHQ4gdycmhvsot/kuba3AtpwhGI0xP1lnb1rTIHmojS8RHjKPv7pG8tzYs4kiIlvkVnNaaG5jhJEQN1Ld3tIL/tuFvRSigdL1hY13+LmX04rt5UxOU+gUc3N+HewPamOtWKTN2jE8rGs6K65dTk04+gHAb2CWPJMlyTPHOtX+/ywIXV5Vo8Ns2+cVANAv0I+QLUDdXAmV8Kqaq4n5Mm5qlPM776gPahTw/1ZYORavGC2RdrvPLbsha6Cy9hGFTYend7dZ78uN+ln3H2iTs2lsAeCx1/9wDBzw69mpIPwdR9I+0eMU5Tle4o6OPtzVw/u9Y/9Qn7ydvH0t2/09/RrVl6Az5GKPdU9LTh/kNRx/Nv/p76hJBVVkooqSUWVpKJKUlElqaiSVFRJKqofoJjgr19ncNoAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;，则有：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYsAAAAqCAIAAADNteAjAAAE5ElEQVR4nO2c0YHjIAxEU5cLoo6UQAFXB81QjO/DxsGAHDASyJt5X3fZtbHRMAhB9rUCAIBWXrMfAAAASOBQAAC9wKEAAHqBQwEA9AKHAgDoBQ4FANALHAoAoBc4FABAL3AoAIBe4FCACW8X40qfvhbrJzwP+BMwOJS3yyvwXYvOHL/8KggaPJZYB2diVUAtM3GGd7Yoz0qscDlU63PeumgC9LDjf/qRbQng7VKQv7fL6emfqRYyNPsbH06a9cDpysWYeSFOI8F3V9EcGQ5VgbdLor2gSP7YjGyLl7/sUJ+nKOSD4aPkv+cro09nhFjGnuTvDYeqwxl6duR+iZFtsfLnHaoUms1bwqMFp0kfNVkMjQ+xM6ITnKRH/YpDlYdPLUVJBVExh35kW6zQi9QfcqjFWpNnQzUOJRhi2SRnXVdJC5RxqGQuKXT9eM31tEhISkRT/W1lVkG+dVyI7i6J9ORQXwQjoxbi7ekFVxYal7jRVojOs6E6h5KyKKpUxBn9ynLU+/1uvbNgDrXHgRbu6FnxvgAoSSVTKAudbdVPZvuwcWa7ZV+WSd6gYZVHC4ZfLWQvf7vocjgfW2WJdVU6lIScqM7jjn5dh+pyqGyGqbpIll1jrcF4jEN5aypfLQwmNoky1KFIwXCr5eYmeRqakKwen0Wb+adEaqpDlVoTiT7x5LErKXOo1RmyuwU0d5Gy9yazT1nlEb1QOkjp/X7B9kNnuqPBUIeiBMOslruneMpjPXrD83GjfTo0bu4qr9CaVPTzm7zfb+0ORbz63ByqteGnVMpbp8J4VdIdDIa9PEowih3qZFHpgchw/sCYiZXyi4yNN/pZiN4ZN24sWocylqyfnS6KZl/pPdGkGPuqa/dCmvFOM8sYqmlrpUZZk0V9nrj07M1BYahDUYLhVsvNnSc6hzqGOnli86tD3Zw8v0P0OHP0CR+8bUwHYg61rXBDiT+sd6mLgrq5hjnxmOQe8PXIHnnErret2hJw3B1hwCZRagxKp0NdCYZfLXcq5XloUvspf6kkzQxHyim0ly7zuKN/sZenzqHOe63UObTkor135E5VlIZPjUPVFVe2V+g11/q2rlc92ZZTufeTzfz89xqD8sWh9v7JH71CMDJqaThtcFHm/CQixLVrpLT6EOe/fF9dySwhEf2nnYdqvWjOFztiBQgVCqW4W0lpoSYozRsU5t9T1TIc75x3xrjVW9sb63YDaerkNFHmRIVDOfMyrn/Ds5V4mPfPASNMY2RbQkF5rFomsIU5q5DcolHgTZ0s+p0aDQ71yf+T93QuzaBZOyIe5/2zwECHGmZQp6AQq4FGVKilIvWb73/7GzpjnLfWHRuDdx/solSU0RB94a/8Tf/7UOHixYZvC0RdEnZIXMh093McHGQi7RzzzoyTtXRbRFBYyj/PVMt4wgtFRaOt/7tSqpq/D9US/QFzpda/semtMeaz/K4/Kw2kOMohI9ezdfyEWry1Lu964XXGqfX9H2Ojr9OhvDXWndZgf1Fyz+JTr+UpjPDxG2rx1pr0yP24zHFe9PU51L5cjkoPF0fTwSCS5ZYafkUt4fxWvMgalznOjL4+hwIAfOc3Mkc4FADP41cyx3WFQwEANAOHAgDoBQ4FANALHAoAoBc4FABAL3AoAIBe4FAAAL3AoQAAeoFDAQD08h9L/cDSZypaqwAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;根据这两种规则，我们就得到了从该信息系统中能得出的有效规则，通过这些规则，我们就可以通过任一元素在AT上的值，去估计它在D上的结论值。&lt;/p&gt;&#xA;&lt;p&gt;另外，有时我们的属性表中虽然有确定已知的属性值，但可能数据并非简单的定性分类数据，而是定量连续数据，对于这种属性，离散化处理使其变为定性分类数据是最为简单有效的方法。&lt;/p&gt;&#xA;&lt;p&gt;2.1.2定量属性—相似关系的引入&lt;a href=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABcAAAAfCAIAAACDG8GaAAAAlUlEQVRIie3SwQ3EIAwEwK2LglwP1bgZinEedyQo8dpwukce+AcSw7IC9o/BVrYyTKsFfUR/UVotQKnNzMxUXAdACLVaTqKTjpIHyZFEUcGIqIyrWaUjKrzYVLklMRUGBcodoa1EygNxdlLFR0gzTPkUejE8R6B8D10/nwtm7O/GN09mCRqYV5YRR1l9DcuyPlt5v3IAOP7MKTnaHTcAAAAASUVORK5CYII=&#34;&gt;5&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;有时，我们并不能用简单的定性分类属性表示AT中的所有属性，因此，就引入了另外两种关系：相似关系与优势关系。&lt;/p&gt;&#xA;&lt;p&gt;当AT中的属性为量化数据时，我们可以简单改变原来属性值完全相等才属于同一类别的定义，对每一个数值定义一个范围的领域，当另一元素y的该属性值属于元素x的该属性的邻域中时，我们就说y与x相似，这样我们就在U上重新定义的一个二元相似关系。&lt;/p&gt;&#xA;&lt;p&gt;由于邻域可以任意的选择(一般要包括该值自己)，所以这一新的二元关系并不一定是等价关系了，这个关系将可能不再满足传递性与对称性。我们用xRy表示x相似于y。根据R对任一个x，我们可以定义两个集合，分别表示相似于x的元素集和x相似于的元素集。&lt;/p&gt;&#xA;&lt;p&gt; &lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWwAAAAlCAIAAADp3UT+AAAENklEQVR4nO2c28GDIAyFncuBmMMRnMZlHIb/oa2CJJBwE/3P99haTMLp4SLtZAEAoIDp7gAAAM8GJgIAKAImAgAoAiYCACgCJgIAKAImAgAoAiYCACgCJgIAKAImAgAoAiYCurGv8zRNZrs7DlAXmAjoyb7OMJG3ARMBPYGJvJC+JrKZed25N4v1tZmBJsv7avhUWzBU9ix8J0e1Ef2kkKEK9Cp5lJjIZ4kbQoe6r3MqiX2dp7iS4rG0qdElzfMm3hvXuFUq0VWSp8P3RBiqf9lZHNoKBNqwQ8ojTxv2ZfIonons6+xWaTNkejKVaC4Mg2j65WHrv68zKWz9UCOrpCDOzC+agvxQCRPRdPmQ8lBrw75MHuXLmWtcH9v0I92MPPQcnbQfgVkdcrPsjPmqpJLZcdYlM9Sv8r3rNNqwQ8pDrQ37MnnUN5GvUpxItf2ulJUNXLoBd5hIWElZmO1nIjVC/ZDhCcPJ4yYTGUce9U2E9JDfBc7qzmyxtc+87sfAJVgPOmONc4vfXakB8LyVaJlZx0TiscUrKS5dj83DZKcLuWxzPFMe1Uwknr5YHnxeQnksy5K6xKOyiRD9HgS+mSMzpsr6ldtm3Hs4dzhfCW+kXWRVmYlEYktVUlq63iaStzy3lq7q8+RRdSbCp5+suSAvmTzuMZGYVxNd/p1VrdxDPe14Gnb35abU40PdhLLicoaNLVXJI45I6TrZiCRUYTNUzzxLHpWXM1z6gpoL8uLk4RrHrTMRcgOLFkp0NPn1imS+yhTFW/3RIiEfm7VdzsRjS1XSBpdxNDcSYaiKZoRvWDuePOrviZDpS2qezOtoyfv0sizDmAi5c0MWZF/NzP+OImOooWdAk9kiRxO0DwVIWWdtrNKxpSp5NBwpXe5TUCWyUHXNODxLHmpt2IQ86PRlNU/kRchjCeDi4qi+sRpkR3T51yJZPy1d9HrNrNR616pdhAmW1UliOkzFlqrk55p06fpvrObaCG0Hj5OHVhs2Kg8ufWnNY3kx8sjzjoPah82s/S3ePNP0kz/SoAuRcTKR7sbEk3SlGMl9K7aB1Jo6jE1USUHpOphIOlRFQ/5nnikPnTYsLw8+fXnN+bwi8rjLRC7rRm9Hx11EujMod1f5/HxQm1oHAchtd/992Z4ImXHs6vTGnBObqJJGULpOB2YEna5o7jIZe6g8FNqwnDyY9I1R1pzLa9hzIhK0D1O1Yyk3841NKBuTNJE2sfU6KFIPpSf8F3mUNc7l1UYevX7FK5aKfpyxbG26/1RSce9GsWXM9W9H0+X/RR6N2m4jj35/BSCJP0siR+vhaYAbh+TkxmqL2PRbjmMg1Pb/kUcJfF6t5PHC/xM5FpMDzuobxlZyXmMEev2fyMjyKCGRV0t54J/NAABFwEQAAEXARAAARcBEAABFwEQAAEXARAAARcBEAABF/AHOhzOtNNe3kgAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;这样我们就得到了两个U上的覆盖(由于这个关系将可能不再满足传递性与对称性，U上所有相似类也就不再一定会构成一个U的划分，而是构成一个覆盖。于是我们有覆盖：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAU0AAAArCAIAAABzSE1gAAAD1klEQVR4nO2b26GEIAxErcuCqMMSqIZmLMb9WB88EggoKjjn695djJBhJKI7LACA3hme7gAAoDrwOQD9A58D0D/wOQD9A58D0D/wOQD9A58D0D/wOQD9A58D0D/wOWiLWY/DMCjzdD/aAj4HzTHrET7PoyGfG1V8HTdq1DP3ZXTWzFrxR97PiRy8g0sGEFUMWlM04vNZj8eY/5VbCJ2TWY+pbM16HJjJcYP23nCOvjpfHP1r1uuHiHkKUoGgdR5N+Nxxuf2Zl5AwJQLh4w3vusazgs56DKalUQM3V99LIKJIQdcYa2va59A6QhM+JxPjJ+A/I9yUGCXPEa3+XdpTl7Lti+BjtvGbCUWUKEhC+hxax2jC596V/09wofsvCFZKxBf4I0AwU96oPZmO1xP2Oq0gybrwuwmA1nEq+Xwtt/4d3GovakXmcNtSC7o/S0ibbw2s8k+ZWJHvpzTUXhSKbh+5Cc3Wvt5yLtJu54SISQXz+vxlradpijeo4HPyAae4JiMxKlG3ExIEGTLKnr1U7oh7Ifoanw51NJSNOauWo9JxDRW02wl6nVJQzue1vt3nwcb4PvTyVYhOoLeQkAkKhByHYdSae/JCdJGr5VKhogfLTr1/QXxcxek1tNshREwpmBX8e1rb3r7b506f3EItrJQkJR8/oS1tRft0sU+tLgq1T4Xaw6WqWf7U+xeMAa72epZ2zlGFIqYUzOv6t7Sepuk5n7sDcCfHiSWBniZO6ontClKaWauRf20yS/t4qDVc1g4wOZNOPkMSU0m7HULElIJ5wT+k9RSQPOelPvfSbfTxd2RFEAZOrNdB8ggh11el2MVDfM+WDrV1QTxmJhL3COnyqr2ads4ZYs/VTjj9e1oL7b1z+Xq+DdaoYRzV8c+gDLnJI4JIDLOCeyuEO42c5YpaACR7sKJQdpek8lsbPvGjq+y3J7SzulhqRl/EtIJlvV8+ovVzPrfv1pSxblrsTBVMUC+/3r2QvQoFp/dmqDLO8em3agjtJaHIlMTu2ciRxUrO65+fp7RbZq3N2rDQitYLbSIF86JDa56b35Mp83nxLk0770hlUfkBepLoT0X4gwpFlMeH1ixN+PzcHp5Mfabha7V/8H04o0q0qH9xgtY8Tfj89B5N6kh+hrxQ+5z7wAps1XvJkbXf14XWHI34fFlOFX7lv0l+GbWL3xTbrrK9HZ9F/QFAa4pbfX5quxY8jb3L1IxbwLIsjfxeDQBwCvgcgP6BzwHoH/gcgP6BzwHoH/gcgP6BzwHoH/gcgP6BzwHoH/gcgP6BzwHoH/gcgP6BzwHonx9yQYDNalfpcAAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;有了关系R以后，我们认为一个元素x绝对属于X当且仅当x相似于的元素都属于X，同时，所有相似于X中某一元素x的元素都有可能属于X，于是可以定义U的任意子集X在关系R下上下近似为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;有了这个两个近似后&lt;a name=&#34;OLE_LINK25&#34;&gt;&lt;/a&gt;&lt;a name=&#34;OLE_LINK24&#34;&gt;&lt;/a&gt;，我们就可以像之前一样计算POS(X),BND(X),NRG(X)并简单的定义信息系统上的两种规则了。&lt;/p&gt;&#xA;&lt;p&gt;2.1.3优势关系与排序问题&lt;/p&gt;&#xA;&lt;p&gt;但是当我们决策属不再是简单的分类关系而是一中偏序关系时，我们就不能再像以前一样进行规则的推导了，这是，就要通过引入优势关系下的近似来进行处理。&lt;/p&gt;&#xA;&lt;p&gt;在优势关系中，在属性集AT上我们定义xDy表示x比y更好，一般来说xDy定义为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQkAAAAiCAIAAADDFrtyAAAD3UlEQVR4nO1b3ZnjIAykLhekOlwC1dCMi9l7MI75kYTANnHum3m53QvIY6GRBGTdHwAAHNy3CQDASwFtAAAPaAMAeEAbAMAD2gAAHtAGAPCANgCAB7QBADygDQDgAW38t9j84pxzbvGbPCiQ+nG0Q+FeaqX9d/Kcp41AjkHzjR/B5hflwYHco8HAPPB4Hu+kAU9tfnGL3z7/SmNML6oZuYg385xaNza/5GscQ2FuJEYmykpMFWy6oIE+P25+SRxjSJspAjWdag44bXinkQqzeI5hqjYC1elvr6jz1SEVh6c7iPpxZ00460eZRoLvkEYuKx6JCjusckb3/Dbis7k8+zFTG4IvAg32VmIJNaQjpX2ZKI1Ajogly6URu9HW1LH40eJ092Z3cZvOswd3a+OIuPhGaVqU0sRROY6p53u1mlB9z2Co1pUBpnc5t4peWMxjRK+6AjkK/HsEGhJppnd5uuA7Ze20aYVhi0Iu8WzGiWXbsa5rg+NDdSNmhCL4pDyRdVX5IL2OG6C7qFRC1U+lnY1Q3kaT1OEc1iuWbkNCOx2r1vm1s5o+7TfHXeOpxomB59e0wVZYkXAWdIU/kg48wpqaatP857lLs6fV/q9WavM0VL0/oSdYtQSXaLk1VQ8dpTtqSjZWUIuqL/JU40Tgmerhe9pgWhqxS8jjN3utzftQD+7Mp+qUtFIUVaRcGfkgoXvHUpa9cvyFzYZxfzvWjirGu1LWDTzVOOGMr+v6Im04g5TrY6rTIXxOvrVu5Am8kkbKV+p9+mM4K0+c2cHNRiRkk2ZDG+IZHi/kbilf56nFSWV9rWAh+dB+g3y9PZK2VVXPHy+DlG7lpv1GtBXvnpR+Sjxp7hZHMYGLkWc3G60H1GunWB8+L73OU4sTzrpdEh88oI299zu4B6LAaUC69tsdQs27ruvnVMlQovK2I02egdyyLM5RqJu8rv7H1Gk2qoZ6ndCxWeaHMWtnmdaLyzzVOJGmfVMb+U5M6MWbKb2jo5DOiDp3JawvE/oUjt+kgGruN1JnpOde2X+WHuNeQf3ykVmndb5n164gULtbWWONxwWeuRH+w7feb1xHfTQFVGDvyfl8GdtBygPmm/fNDE+epMpTipPfvBc3YPI3Nn4UTGv3J1TLI7bYdq5HHXclY42K+gYZ5Di5j+dLtLG/0Oxv+f0kqn4kBgMbV6dHua+P2fcPdwScyFMnmfFsxcmNwvh7iTbUbh7QEfctwjcOFr+pBz7z/i5C4mkgefDU4+T2nuMV2gAewhGPROMX7Y/jtSShDQDgAW0AAA9oAwB4QBsAwAPaAAAe0AYA8PgHUlH3Du58GJsAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;这样，我们就可以对每个x定义两个集合分别表示比x更优的元素和x比其更优的元素。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUYAAAAqCAIAAABA9GUyAAADyUlEQVR4nO2b26GEIAxELefWYEHUYQlUQzMW4/1Y96EmISCIxDm/KwQyDC/dYQEAGGJo3QAAQElgaQBMAUsDYApYGgBTwNIAmAKWBsAUsDQApoClATAFLA2AKWBpAEwBSwNgirtYOng/t24DAAboxNLBjeLvsx9dKN2mJbhhGBLqnb27cGJKbd1NaCDls3Ssa+ngBoKNpLMf2d++T8R7PPuRKpzN7MfkROcMBUWKYqUvsXWylFQXGkj5PB2rr9KzH7c9W/u+az+3SusGQfqziqrS68qc3ZUpoglOP3BOcqqdbaR8oo7VLU019TWdb3rKWDq4pH4WM3XerJk5FJQp4mLmDNosTrWzjZRP1LG2pZmWBqfZlWTImjhwpMjpFeUNhdMpumiV7lHKJ+pY2dLc3KOavKgz1fvIshZ91fPzUKlz2G56f4f9Vk0FoodCpGw0RWKXr1uk+5TyuEwrpGQsLTc4lh85rl7HaZrkBypbmjshaMYB38u11tmPx7qpiPTVhXh/Edwx9rZqYt0RZne+rC5FbJephlahTymZ9MhSSqu0KEQ0P/Iw6MHS7JlfsxsRLgxewpI/Flm26D3frurg9mGEocCW1aaI7/I1pu5SSnbvLkspbryZBuvyI8UVdPy1cVtLs5cTqvOFdAfId//0OFBWPXt/eEhr6Z+y+hSJzq1u6/6kFFMiSymfpcmKlfmJDSGq7mmabmPpU6ev+Diga6CCJm+8o9s1WnTFhm33VEKK2C4XfHfH0qmUvK1FKeOWPh65lPkR4pI6TgfYdr2paGnmEk93A7hIk3RwznN3J4Xe75GD4X2fwUkuDQWyrD5FbJcv2Xb3KyWXHlFKeWomGqzPDxuX11Hp5A/1LE2Oy2HQTOorzJXn6wDy/vFwDKpy4/2tfHCO/aIxZul9WXWK+C5fcuPdsZTcHkCUkteRbnBCfri4so7tLc3ujhIV2m1F1g8ON+8QiI1ZxffSkS8X4tcqn7LKFEW7XPu9dOdS8umRpCT34nSDk/PDxO3nvfRZbvb12PGSexdetHSVxfTCr8fOcbOvxyQ5av5tg4vbz9djp0kZCYWW6IVO8Bz9ixA/FOJlcyl10riABlIyRonIUc3SQtxyOt7e0urOlvPzJ+p6YGK/hTiWOd55aMvmUegy8CoaSPm7o9XKUdrS8bgldezA0suyLA3/Lz3+ZRwdP63KLhsncrC/K+3+L11XDp5I3NI6dmJpAIAOWBoAU8DSAJgClgbAFLA0AKaApQEwBSwNgClgaQBMAUsDYApYGgBTwNIAmAKWBsAU/+jynOrCHTlSAAAAAElFTkSuQmCC&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;而处理决策集时，我们不再简单对每一类决策类求近似，而是求他们并集的近似：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;有了这四种近似后，我们可以将(1)(2)分为一组，(3)(4)分为一组，分别按照原有方法导出两种规则求解一定属于&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAArCAIAAABjIZbQAAABFklEQVRYhe3W3RWDIAwF4MzFQMyTaViGYeyDlNOQG0UB29OT+6gRPwL+0PYboW8DStwh4w4Zd8i4Q2bIkXPurk0x8EH1mIMDkR4+c4ipLduPmJrxdWkwmcPHrbQ0RSCftj9ShBPOHEi0phxUx5AjRQIxVzdFonoz0PcU24sr+tixgXYWW3O5IFiOOl4tRA3BDj2HMpaeiLrwpMIogg7JFzrUpc+Fkw0BBf37w2L0dUQ+L93nkQOtSrdjMyb8Hrr//WEx9LpMjHboh+ABhnYMbY55DuOTsbQXwAHuh19h6xz4dY7bkJkns65/5/aNMrtBd763J780TzlWMG44ljCuO/btMZ3yF//rE+MOGXfIuEPGHTLukHkBU2B8TaHjdcAAAAAASUVORK5CYII=&#34; alt=&#34;&#34;&gt;的元素规则和可能属于&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACsAAAApCAIAAAAj90ecAAABBklEQVRYhe3W0RHDIAgGYOZyIOZxGpZhmPQhOS/Cb6sX0lx7+GiJfgI1oe3pQU8DUpCCFKTgLwSqOh0rXOoo+oKgFiK/sNbCYsP2Gey4WAXD0FpOm3ijsDdH9IEwPKTWQl06jkkzZwXCBMawisJEbRuQZWH7cOOOBBtI3qEy9m7zkaCt1wJdEoDAu49VfE7dgx8iUJAX9OTOhTJzLlOfBBAw0wcjwFwW+v/C3O9OgGowLdjQIU9Lz9wHI4CvQtQwAt/gdwOM4FITRAgGV/1957cCsBO+jO4Q4MsYH11rjQQtvpn2hghNyvK78d3HxlcE4YBVQTxgUbC3QSzil7+VU5CCFPyb4AUdP3ozW1qttQAAAABJRU5ErkJggg==&#34; alt=&#34;&#34;&gt;的元素规则。&lt;/p&gt;&#xA;&lt;p&gt;2.1.4成对比较表法PCT&lt;a href=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOAAAAAhCAIAAABP+zCnAAADOklEQVR4nO1aybXDIAx0XRREPVTD6XdCMfkH44RFG2ADydPcEiyYkcYYsI+XQrExjtUEFAoKalDF1lCDKraGGlSxNdSgiq2hBlVsDTWoYmuoQRVbQw2q2Bpq0Inw9khg/Wo+34ABg3prXOgNDs5mwcGZd+X4btNKk3UOzvT6oGQ4huBMIczbSupDSchGoETVrZPKRIxYGjS/ydHegzPln1lkbMMZgspbvSQJCs4IUgkG0gZt0estzLP4/7EkJBcTooDGaWVCR4RmUG+z/J5ZL/IIj1dExn8gf8xUjvOlo/gZVKaXGt/bpGmtQaG2LQ1a95fXwVt8SspjiQfsXOU9FhU94kV6MxNSjUsNCjZtadB6EkgNytT6QyY4QzxaZyun7iqsc8kaVKCXTsQuBoVbdjRo5c9zYZz4k6z0aWZHFgXgAYu41nTJ4Cm5BuXtS1HhJonXO2RQJgNwEA5UFNIgKNPDDCuDFv6M27are8FYwO5UwIPoODIC6tyiHFocwjvCuEX9E+7iWb3jMyieASII4QKLkv4/n2Fp0PM4xJi0VMXZCDc1VneQhAcl4vTRoPLmh5NwBuX13vCIRzNABcFcRBMl2jCf4VE2030xBg3OGOerIz8BD3Jg7JRmuUFFeu9Yg2IZIIPAiyUrTbxlPsPCoNwMSbW/C1EfSvM8WINCjaMGHXvES/XeZVBE6bhBG7b28xnmBmWf4OhYWRUEjJrWoNbBW5zRNSgJ7qWLWO8da1AsA+zg1cX8WSfVNp/hkbVJZr6aRl17vqeGxY31n2HPX2BQ8hYO8fJdu/hGvaMGpTJQBtFJAEU1vVyaz/Cowvglf5HRIkjWFa88Pz8oThPAoMsJ0MH4PeegXXq7DSrIABiEJgES1fZ2fj7Djo9F2msN4IkT4HjrAvyYtwsiht2Ye1CPJ+G8uE3U3DJBI3Z9zXSDRR9Qfm15ampdfO/9mkk45INJiBcvN2gjw87P7dqXdAyPceXeHtYDs1Xv3fSVBsWS8Ll4tUFbGa76HrQEtLqjhv6s+qLy+KgspIf+70EX4KEkfDXDn/ii/kqbceElfJP1e9g/CV0Mf8Kgit+FGlSxNdSgiq2hBlVsjX9sY9kgdo3PogAAAABJRU5ErkJggg==&#34;&gt;6&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;当我们拥有一张已经排序的信息表时，除了以上提到的方法以外，还有一种成对比较表法可以处理排序问题，导出规则对未知元素进行排序。&lt;/p&gt;&#xA;&lt;p&gt;该方法的核心是将原来定义在U上的信息表扩展为一个UXU上的信息表。&lt;/p&gt;&#xA;&lt;p&gt;新的信息表表示为{UXU，AT∪D，V，f}}&lt;/p&gt;&#xA;&lt;p&gt;UXU: |UXU|=n*n&lt;/p&gt;&#xA;&lt;p&gt;AT∪D中的属性与原有信息表中的属性一一对应，但是不再表示具体的属性值，对于AT中的属性a和元素(x,y)，新的信息表中a的值表示在原信息表中元素x比y在属性a上的优异程度。而对于D中的属性值，新的信息表中的属性d仅表示x优于y或y优于x。&lt;/p&gt;</description>
    </item>
    <item>
      <title>算法总结9—优化</title>
      <link>https://sword865.github.io/archives/13/</link>
      <pubDate>Wed, 30 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/13/</guid>
      <description>&lt;p&gt;不同于之前的分类和聚类算法，优化的目的是尝试找到一个使成本函数输出最小化的值。这里主要包括两个算法：模拟退火算法和遗传算法。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;成本函数&lt;/strong&gt;:&lt;br&gt;&#xA;接受一个经推测的题解，并返回一个数值结果，该值越大代表成本越高（题解表现越差），该值越小就表示题解越好。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;模拟退火算法：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;优化算法的目标可以看为寻找x使函数f(x)最小。&lt;/p&gt;&#xA;&lt;p&gt;但是严格的最小值往往是很难达到的，我们不得不把眼光投入到寻找一个尽可能好的次优解去。&lt;/p&gt;&#xA;&lt;p&gt;最简单的方法被称为随机法，即成千上万次的对x进行猜测，然后把这些x中使f(x)最小的一个作为答案。虽然这样很简单，但是效果很差。于是出现了爬山法。&lt;/p&gt;&#xA;&lt;p&gt;爬山法从一个随机解出发，然后不断向该解附近的使f(x)的值更小的x移动，直到当前x附近的解都比x差为止。为了使效果更好，我们可以从多个随机解出发重复着一个过程，将最好的一个作为答案。很容易就能认识到，这样找到的解是一个极值点，是一个局部最小值。&lt;/p&gt;&#xA;&lt;p&gt;爬山法虽然好，但是在其寻找最优解的过程中，前进的方向是固定的（使f(x)更小的方向），但是有时向其他方法前进也是必要的，因为f(x)可能先增大在变小成为最优的。&lt;/p&gt;&#xA;&lt;p&gt;于是就有了模拟退火法。&lt;/p&gt;&#xA;&lt;p&gt;该算法这源于固体的退火过程，即先将温度加到很高(大量原子被激发)，再缓慢降温(即退火)，则能使达到能量最低点。如果急速降温(即为淬火)则不能达到最低点。&lt;/p&gt;&#xA;&lt;p&gt;模拟退火法同样是从一个随机解出发。但是它在寻找最优解时并不一定是向更好的x移动，也有一定的概率向更差的x移动，这个概率开始较大，但是会随时间而渐渐变小，直到稳定。一般该概率可以定义为：p=e ^ (-  (highcost – lowcost ) / temperature )，其中temperature是随时间增大而变小的温度，开始温度很高时p -&amp;gt; 1，后来会渐渐变小使p-&amp;gt;0。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;遗传算法：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;遗传算法的思想来自生物的遗传和变异，算法以种群为单位（一个种群为一组既多个解），其算法的运行过程如下：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;随机生成一组初始解（初始种群）。&lt;/li&gt;&#xA;&lt;li&gt;计算种群中各个解的成本，然后进行排序。&lt;/li&gt;&#xA;&lt;li&gt;我们将种群中靠前（成本低）的解保留下来，删除其他解，这一过程称为精英选拔。&lt;/li&gt;&#xA;&lt;li&gt;对已有解进行微小的改变，将改变后的结果作为新的元素加入种群，这一过程称为变异。&lt;/li&gt;&#xA;&lt;li&gt;选择一些优秀的解两两组合，然后将他们按某种方式进行结合（如求平均），将得到的结果作为新的元素加入种群，这一过程称为交叉（配对）。&lt;/li&gt;&#xA;&lt;li&gt;不断重复2—5步，直到达到指定迭代次数或成本函数连续数代都没有更好的改善。&lt;/li&gt;&#xA;&lt;li&gt;得到一组解，该组解为算法输出。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;该系列结束，恩，也许以后学了更多，有了更好的了解后会回来改一改，谁知道呢？&lt;/p&gt;</description>
    </item>
    <item>
      <title>算法总结8—非负矩阵因式分解</title>
      <link>https://sword865.github.io/archives/14/</link>
      <pubDate>Fri, 25 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/14/</guid>
      <description>&lt;p&gt;&lt;strong&gt;数学基础:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;线性代数的矩阵乘法运算。&lt;/p&gt;&#xA;&lt;p&gt;   非负矩阵分解是一种特征提取的算法，它尝试从数据集中寻找新的数据行，将这些新找到的数据行加以组合，就可以重新构造出数据集。&lt;/p&gt;&#xA;&lt;p&gt;   算法要求输入多个样本数据，每个样本数据都是一个m维数值向量，首先把我们的数据集用矩阵的形式写出来，每一列是一个数据，而每一行是这些数据对应维度的数值。于是我们就有了一个大小为m*n的输入矩阵。而算法的目标就是将这个矩阵分解为另外两个非负矩阵的积。&lt;/p&gt;&#xA;&lt;div&gt;$$M_{m,n}=A_{m,r}B_{r,n}$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;   我们将分解矩阵后新得出的一个维度称为特征，那么在前一个m*r的矩阵中，第i行第j列的值就代表属性i对第j种特征的贡献值，而后一个矩阵的第i行第j列则代表第i种特征对第j个样本的贡献值。这样我们就找出了输入样本的r种特征。&lt;/p&gt;&#xA;&lt;p&gt;   r的大小应该依照需要进行选择，比如如果是希望找到某些共性特征，则就要选择较小的r。当我们确定了一个较为合适的r值后，就要想办法确定后面两个矩阵具体的值了。&lt;/p&gt;&#xA;&lt;p&gt;   书中给出的算法大致如下：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;定义一个函数计算用来两个矩阵的差异程度（每个对应元素相减后平方的和）&lt;/li&gt;&#xA;&lt;li&gt;随机生成2个矩阵(m&lt;em&gt;r维和r&lt;/em&gt;n维)记为A（权重矩阵）,B（特征矩阵）&lt;/li&gt;&#xA;&lt;li&gt;计算A&lt;em&gt;B与输入的m&lt;/em&gt;n的数据矩阵的差异，足够小则停止，否则继续&lt;/li&gt;&#xA;&lt;li&gt;按一定规则调整A，B的值后转3.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;对于调整的方法，可以用模拟退火（下一篇文章中会提到）等多种算法，书里使用的是乘法更新法则，该法则我没有认真去看….感兴趣的可以去看论文….英文的…&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://hebb.mit.edu/people/seung/papers/nmfconverge.pdf&#34;&gt;http://hebb.mit.edu/people/seung/papers/nmfconverge.pdf&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;算法如下：&lt;/p&gt;&#xA;&lt;p&gt;hn 转置后的权重矩阵和数据矩阵相乘的结果&lt;/p&gt;&#xA;&lt;p&gt;hd 转置后的权重矩阵和原权重矩阵相乘再乘特征矩阵的结果&lt;/p&gt;&#xA;&lt;p&gt;wn数据矩阵与转置后的特征矩阵相乘的结果&lt;/p&gt;&#xA;&lt;p&gt;wd权重矩阵与特征矩阵相乘，再与转置后的特诊矩阵相乘得到的矩阵&lt;/p&gt;&#xA;&lt;p&gt;为了更新特征矩阵和权重矩阵，我们先把上面所有矩阵变为数组．然后把特征矩阵中每一个值与hn中对应值相乘，并除以hd中对应的值．类似的，我们再将权重矩阵中每一个值与wn中的对应值相乘，并除以wd中对应的值．&lt;/p&gt;&#xA;&lt;p&gt;最近的算法都很好理解的样子…不过写起来还是挺麻烦的….还有最后一篇优化了，内容挺多，包括模拟退火和遗传算法….恩&lt;/p&gt;&#xA;&lt;div&gt;&#xD;&#xA;  &lt;embed id=&#34;lingoes_plugin_object&#34; width=&#34;0&#34; height=&#34;0&#34; type=&#34;application/lingoes-npruntime-capture-word-plugin&#34; hidden=&#34;true&#34; /&gt;&#xD;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>算法总结7—多维缩放</title>
      <link>https://sword865.github.io/archives/15/</link>
      <pubDate>Sun, 20 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/15/</guid>
      <description>&lt;p&gt;一直没有时间写…..唉&lt;/p&gt;&#xA;&lt;p&gt;这个东西好像是属于数据可视化？反正就是把多维的数据降到低维空间但是仍然尽可能的保持原来数据之间的距离关系(就是在原来维度下离的远的点仍然离得远，接近的点仍然接近) 。最常见的应该就是降到2维以方便打印和屏幕输出。&lt;/p&gt;&#xA;&lt;p&gt;算法的输入是所有数据在高维情况下两两之间的距离（记i与j的距离为Dij）。现在以降到2维为例说明这个算法。&lt;/p&gt;&#xA;&lt;p&gt;首先我们把所有数据点随机绘制在一张二维图像上，然后计算它们两两之间的距离dij，然后我们计算出它与高维距离Dij的误差，根据这些误差，我们将每对数据点按比例移近或移远，然后重新计算所有dij，不断重复到我们没法减少误差为止。&lt;/p&gt;&#xA;&lt;p&gt;还是来具体说明一下吧，假设有n个点&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;输入每一对点之间的距离Dij。&lt;/li&gt;&#xA;&lt;li&gt;随机在2维平面生成n个点，点i坐标记为x[i]、y[i]，计算它们两之间的距离，记为dij.&lt;/li&gt;&#xA;&lt;li&gt;对所有i 和j计算：eij=(dij-Dij) / Dij，每个点用一个二维的值grad[k]来表示它要移动的距离的比例因子(初始为0,0)。在计算出每个eij后，计算 ((x[i]-x[j]) / dij)* eij，然后把它加到grad[i][x]上，同样把((y[i]-y[j]) / dij)* eij加到grad[i][y]上。&lt;/li&gt;&#xA;&lt;li&gt;把所有eij的绝对值相加，为总误差，与前一次的总误差比较(初始化为无穷大)，大于前一次的话就停止。否则把它作为上一次总误差，继续。&lt;/li&gt;&#xA;&lt;li&gt;对每个点，新的坐标为x[i] -= rate * grad[i][x]  y[i] -= rate*grad[i][y]，其中rate是开始时自己定义的一个常数参数，该参数影响了点的移动速度。重新计算各个dij，回到3。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;伪码：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;for m = 1 to 1000{&#xD;&#xA;  for i=1 to n&#xD;&#xA;    for  j = 1 to n&#xD;&#xA;      dij=sqrt((x[i] – x[j])^2+(y[i]-y[j])^2)&#xD;&#xA;  for i=1 to n&#xD;&#xA;    gradi=0&#xD;&#xA;  totale=0&#xD;&#xA;  for i= 1 to n&#xD;&#xA;    for j= 1 to n{&#xD;&#xA;       if(j==i) continue&#xD;&#xA;         eij=(dij-Dij) / Dij&#xD;&#xA;         grad[i][0]+= ((x[i] - x[j]) / dij)* eij&#xD;&#xA;         grad[i][1]+=((y[i] - y[j]) / dij)* eij&#xD;&#xA;         totale+=abs(eij)&#xD;&#xA;       }&#xD;&#xA;  if (laste &amp;amp;lt; totale) break;&#xD;&#xA;  laste=totale&#xD;&#xA;  for i=1 to n{&#xD;&#xA;    x[i] -= rate * grad[i][x]&#xD;&#xA;    y[i] - = rate* grad[i][y]&#xD;&#xA;  }&#xD;&#xA;}&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;</description>
    </item>
    <item>
      <title>算法总结5&amp;6—-k-最近邻与聚类</title>
      <link>https://sword865.github.io/archives/16/</link>
      <pubDate>Mon, 14 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/16/</guid>
      <description>&lt;p&gt;因为这两个算法比较简单，又有些相似，所以这里放在一起。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;K-最近邻：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;k-最近邻也是一种用来进行预测的算法。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;工作原理：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;接受一个用以进行数值预测的新数据项，然后将它与一组已经赋过值的数据项进行比较。算法会从中找出与待预测数据最为接近的k项，并这k项其求均值以得到最终的结果。&lt;/p&gt;&#xA;&lt;p&gt;总计来说这是一个很简单的算法，只要我们做好距离的定义并选择一个适合的k值，我们就可以很容易的实现它。&lt;/p&gt;&#xA;&lt;p&gt;由于我们计算2组数据的距离的通常方法是将他们中对应的每一项目的差值的绝对值(或平方)相加，所以就会出现不同数据范围不同导致的误差。比如每组数据有2个分量，一个取值为0—10,另一个是0—-999999，那么第二的值就会几乎完全决定我们最后的结果。所以我们要对每一组数据进行缩放。&lt;/p&gt;&#xA;&lt;p&gt;对数据的缩放取决于具体的应用，我们可以通过交叉验证尝试多组缩放因子然后比较它们的优劣。交叉验证的做法是先数据的一部分去除，然后用剩余数据去推测这组数据，我们就可以根据预测的结果对缩放因子进行评估。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;能利用复杂函数进行数值预测，又简单易懂，并且我们可以很容易的在算法中实现查看用哪些近邻进行预测。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;每次进行预测，它都会使用所有样本，这会导致效率的低下。&lt;/p&gt;&#xA;&lt;p&gt;寻找缩放因子是一项很乏味的工作.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;聚类：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;聚类算法可以用于任何具有一个或多个数值属性的数据集合，通过这些数值属性，我们将其所有数据映射到一个n维空间中，并定义该空间中的距离，然后我们可以通过各个数据间的距离对其实现聚类。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;分级聚类:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;分级聚类的算法是不断找出所有数据中距离最小的两个数据A、B，然后将它们合并成一个新的节点，该节点在n维空间中的坐标是原来两数据点的均值，通过不断进行这一操作，我们最终可以得到一个树形的层级结构。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;K-均值聚类:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;不同于分级聚类，K-均值聚类的目的是将数据拆成K个不同的群组，其具体算法如下：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;在n维空间中随机生成K个中心点&lt;/li&gt;&#xA;&lt;li&gt;将每个数据项分配给与其距离最近的中心点。&lt;/li&gt;&#xA;&lt;li&gt;将中心点位置移动到所有分配给它的数据项的中心。如果中心点位置没有改变，则结束算法，否则回到第二步。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;具体选择哪种聚类算法取决于要处理的问题，当要将数据拆分到不同的群组时，k均值聚类往往是很有价值的，而如果我们更想了解哪些群组间更为接近，分级聚类更好。当然，我们也可以同时使用２种算法得到更加详细的信息。&lt;/p&gt;&#xA;&lt;div&gt;&#xD;&#xA;  &lt;embed id=&#34;lingoes_plugin_object&#34; width=&#34;0&#34; height=&#34;0&#34; type=&#34;application/lingoes-npruntime-capture-word-plugin&#34; hidden=&#34;true&#34; /&gt;&#xD;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>算法总结4—支持向量机</title>
      <link>https://sword865.github.io/archives/18/</link>
      <pubDate>Tue, 08 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/18/</guid>
      <description>&lt;p&gt;支持向量机……复杂的东西，书里讲得也不怎么详细，起码具体算法没有说……所以又去查了些资料……&lt;/p&gt;&#xA;&lt;p&gt;支持向量机是用来对数据进行分类的。&lt;/p&gt;&#xA;&lt;p&gt;首先从最简单的情况开始吧：&lt;/p&gt;&#xA;&lt;p&gt;如果有一条直线，我们把它看成一条数轴，上面有一些样本点，其中坐标大于某个值的点都属于一类，坐标小于某个值的点都属于一类，那么我们就可以用这个值来做分分界点，它点把直线上的点分为了两类。因为样本点是有限可数的。所以这个分类点的取法不唯一。选好后，随便给我们一个点，我们就可以根据这个随机给出的点是在分界点的左侧还是右侧来判断这个点的类别。&lt;/p&gt;&#xA;&lt;p&gt;同样，一个平面上有很多样本点，这些点也分为2类，如果我们在平面上可以找到这样一条直线满足这两类样本点分别分布在直线的两侧，那么我们就可以用这个平面作分界面，来对之后随机给出的点进行分类。&lt;/p&gt;&#xA;&lt;p&gt;仍然用同样的方法，我们可以用一个平面给分布在一个3维立体空间中的点分类。&lt;/p&gt;&#xA;&lt;p&gt;总结起来就是说：在n维空间中有很多样本点，如果我们能找到一个n-1维的超平面，这个平面恰好把空间中的样本点分在它的两侧，那么我们就可以用这个n-1维的超平面来对之后随机给出点分类。&lt;/p&gt;&#xA;&lt;p&gt;这种方法有两个问题：&lt;/p&gt;&#xA;&lt;p&gt;1）  因为那个n-1维的超平面选法往往是不唯一的，我们要选哪一个?&lt;/p&gt;&#xA;&lt;p&gt;2）  更多情况下，我们找不到这样一个n-1维超平面，你可以想象更多情况下，我们要分类的数据是“混合”在一起的，很难简单的用一个点，一条线，或者一个更高纬度的线性分类器把它分开。&lt;/p&gt;&#xA;&lt;p&gt;接下来我们就要来解决这个问题。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;最优超平面的确定:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;如果选择合适的分界超平面呢？直观的来说，我们因该选择一个距离两组数据“最远”的超平面。首先每个点都和这个超平面有一个距离（该距离可以通过把n维空间放入一个n维坐标系后用代数的方法计算出来，具体计算过程此处就不说了。不过1维２维３维的情况你应该能自己算出来吧～～～，理解就好）我们选择的超平面要让所有这些距离中最小的一个值最大。&lt;/p&gt;&#xA;&lt;p&gt;我们在n维空间空建立一个n维坐标系&lt;/p&gt;&#xA;&lt;p&gt;在这个n维坐标系中，每个n-1维超平面都可一个用一个方程表示出来，这里设为。&lt;/p&gt;&#xA;&lt;div&gt;$$H(x)=a_{0}+\sum_{i=1}^{n}(a_{i}*x_{i})$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;我们用一个变量Y表示一个点相对超平面的关系，在一侧为1.另一侧为-1.&lt;/p&gt;&#xA;&lt;p&gt;可以证明:（证明过程略）&lt;/p&gt;&#xA;&lt;p&gt;该平面在满足下面的约束时：&lt;/p&gt;&#xA;&lt;div&gt;$Y_{i}H_(x_{i})\geq 1$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;极小化函数&lt;/p&gt;&#xA;&lt;div&gt;$\frac{1}{2}\sum_{j=1}^{n}(a_{j}^{2})$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;这是一个二次规划问题，我们对它求解就可以得到最优平面。&lt;/p&gt;&#xA;&lt;p&gt;有时我们找不到这样一个超平面，这时，我们可以把超平面的约束条件放的宽松一点，也就是在超平面附近允许两种分类的点的重叠，可以同过把改为（e&amp;gt;0）来实现这一目的。&lt;/p&gt;&#xA;&lt;p&gt;（具体证明与求解参考《统计学完全教程》 科学出版社 P290的支持向量机一节）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;第二个问题的解决—核方法&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;很多时候，我们是找不到一个简单的超平面对样本进行划分的，这个时候，我们可以通过坐标变换，把样本点映射到一个可以线形划分的空间中。&lt;/p&gt;&#xA;&lt;p&gt;这个映射可以是同维度的，即映射前后样本空间的纬度相同，比如：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://pic002.cnblogs.com/images/2012/52809/2012063021394557.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;就可以通过一个简单的&lt;strong&gt;求平方&lt;/strong&gt;运算，把数据从线性不可划分变为线性可分—我们可以很容易的找到一条直线把后者的样本点分成两个部分。&lt;/p&gt;&#xA;&lt;p&gt;但是很多时候，问题没有这么简单，我们就需要用另外一种映射，即把样本点映射到更高纬度的空间去。&lt;/p&gt;&#xA;&lt;p&gt;比如上面的左图还可以做这么一种变换：&lt;/p&gt;&#xA;&lt;p&gt;$$z_1=x_1x_1, z_2=\sqrt{2}x_1x_2, z_3=x_2x_2$$&lt;/p&gt;&#xA;&lt;p&gt;这样我们就可以在新的样本空间中很简单的找到一个平面把这些点分开了．仔细分析，你可以发现，这个平面其实是左图中的一个椭圆的经过上述变换后得到的．&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;较高维空间的线性分类器对应于原空间的一个非线性分类器．&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;这就是&lt;strong&gt;核方法&lt;/strong&gt;的核心。&lt;/p&gt;&#xA;&lt;p&gt;通过找到一个合适的映射，我们就可以前面的问题(2)了&lt;/p&gt;&#xA;&lt;p&gt;这种映射称为核函数，核函数的选择是很有技巧的，它也有一些常见的模型，很多时候我们只要选择合适的模型并计算适当的参数就可以了。具体方法这里不说了，有兴趣的可以参见《&lt;a href=&#34;http://download.csdn.net/source/1353188&#34;&gt;RBF核函数的支持向量机参数选择&lt;/a&gt;》一文。&lt;/p&gt;&#xA;&lt;p&gt;找到核函数后，我们就完全解决上述问题了。&lt;/p&gt;&#xA;&lt;p&gt;（其实这里还有一些简化计算的技巧，这些技巧与其它更具体的东西还是可以去看《统计学完全教程》 科学出版社，真是一本非常强大的书。）&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;优点:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;可以很快的判断一个样本的种类。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;缺点:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;由于对每个数据集的最佳核变换及相应参数都不一样，所以对每个数据集都要重新学习确定函数与参数。&lt;/p&gt;&#xA;&lt;p&gt;一般而言，支持向量机更适合包含大量数据的问题，而其他方法如决策树，更适合小规模的数据集。&lt;/p&gt;&#xA;&lt;p&gt;支持向量机也是一种黑盒技术，由于存在像高维空间的判断，我们很难解释分类的具体标准与原因。&lt;/p&gt;&#xA;&lt;div&gt;&#xD;&#xA;  &lt;embed id=&#34;lingoes_plugin_object&#34; width=&#34;0&#34; height=&#34;0&#34; type=&#34;application/lingoes-npruntime-capture-word-plugin&#34; hidden=&#34;true&#34; /&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;p style=&#34;margin:0;padding:0;height:1px;overflow:hidden;&#34;&gt;&#xD;&#xA;  &lt;a href=&#34;http://www.wumii.com/widget/relatedItems&#34; style=&#34;border:0;&#34;&gt;&lt;img src=&#34;http://static.wumii.cn/images/pixel.png&#34; alt=&#34;无觅相关文章插件，快速提升流量&#34; style=&#34;border:0;padding:0;margin:0;&#34; /&gt;&lt;/a&gt;&#xD;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>算法总结3—神经网络</title>
      <link>https://sword865.github.io/archives/19/</link>
      <pubDate>Mon, 07 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/19/</guid>
      <description>&lt;p&gt;&lt;strong&gt;生物神经网络：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;    &lt;/strong&gt; 在生物的神经网络中的基本单位是神经元，神经元与神经元之间是由突触的相互联系来传递信息的，在静止息状态时，神经元的膜的内外电压保持一种稳定状态（膜内电压低于膜外电压），当神经元受到刺激后，在被刺激的部分周围，这种平衡状态会被打破，电压改变，与没有受到刺激的部分形成电流传递信息，电流的强弱取决于受刺激部位电压的改变量。&lt;/p&gt;&#xA;&lt;p&gt;     前一个神经元的轴突末梢作用于下一个神经元的胞体、树突或轴突等处组成突触。不同的轴突末梢可以释放不同的化学物质对下一个神经元产生不同的影响。也就是说会使下一个神经元的受刺激部分产生不同的电压，也就导致了不同程度的电流，最终也就传递了完全不同的信息。&lt;/p&gt;&#xA;&lt;p&gt;     一个神经元可以通过轴突作用于成千上万的神经元，也可以通过树突从成千上万的神经元接受信息。当多个神经元同时对一个神经元产生作用时，结果这些神经元的作用强度共同决定。&lt;/p&gt;&#xA;&lt;p&gt;     神经系统按功能可大致分为传入神经（感觉神经）、中间神经（脑：延脑、脑桥、小脑、中脑、间脑、大脑脊髓）与传出神经（运动神经）三类。&lt;/p&gt;&#xA;&lt;p&gt;     感受神经的作用是接受外界信息（输入），中间神经则起到了信息传递与计算分析的作用，最用，传出神经会负责对外界信息作出相应的反应（输出）。&lt;/p&gt;&#xA;&lt;p&gt;     模仿这一过程，我们就可以建立人工神经网络。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;人工神经网络：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;     人工神经网络的基本单位是人工神经元（以下简称神经元）。一个神经元可以有多个输入，每个输入有一个相应权值。&lt;/p&gt;&#xA;&lt;p&gt;图示如下：&lt;/p&gt;&#xA;&lt;img class=&#34;alignnone  wp-image-125&#34; src=&#34;http://upload.wikimedia.org/wikipedia/commons/9/97/Ncell.png&#34; alt=&#34;nn&#34; /&gt;&#xD;&#xA;&lt;pre&gt;&lt;code&gt;a1~an为神经元的输入值&#xD;&#xA;w1~wn为神经元各个的输入所拥有的权值&#xD;&#xA;b为偏移量&#xD;&#xA;sum对各个输入与其权值的积求和(含偏移量)。&#xD;&#xA;f为传递函数，接受sum的输出，通过一个函数变换，输出t&#xD;&#xA;t为神经元输出&#xD;&#xA;数学表示 t=f(WA&#39;+b)&#xD;&#xA;W为权向量&#xD;&#xA;A为输入向量，A&#39;为A向量的转置&#xD;&#xA;b为偏移量&#xD;&#xA;f为传递函数&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;在人工神经网络中，神经元之间相互连接，在连接点将前者的输出作为后者的输出，形成错综复杂的网状结构，进行信息的传递与计算。&lt;/p&gt;&#xA;&lt;p&gt;我们这里要介绍的是其中比较简单的一种模型，称为“多层感知机（MLP）”网络。&lt;/p&gt;&#xA;&lt;p&gt;为了简化模型，我们假设偏移量b=0.&lt;/p&gt;&#xA;&lt;p&gt;多层感知机网络由3部分组成：&lt;/p&gt;&#xA;&lt;p&gt;输入层：功能类似感受神经，每个节点接受外界的直接输入。这里的模型中，每个节点接受单一输入，权值为1。&lt;/p&gt;&#xA;&lt;p&gt;输出层：功能类似运动神经，该层输出就是神经网络的输出。&lt;/p&gt;&#xA;&lt;p&gt;隐藏层：是输入层和输出层之间的多层神经网络，可以有1或多层。&lt;/p&gt;&#xA;&lt;p&gt;因此，MLP网络中至少有3个层次。&lt;/p&gt;&#xA;&lt;img class=&#34;alignnone  wp-image-125&#34; src=&#34;https://sword865.github.io/images/archives/2012063021381464.jpg&#34; alt=&#34;mlp&#34; /&gt;&#xD;&#xA;&lt;p&gt;这些层次中，每层的每个神经元的输出都会作为下一层的每个神经元的输入，因此当我们对输入层进行输入后，该信息会一层层传递下去，最终从输出层输出。&lt;/p&gt;&#xA;&lt;p&gt;神经网络建立后，我们需要设法确定每个神经元的各个输入的权重w，并选择合适的函数f对输入进行变换，只有完成以上工作后，我们才能使用神经网络完成相应的工作。&lt;/p&gt;&#xA;&lt;p&gt;我们一般会选择过关于源点对称的S形函数作为函数f，该种函数特点是:输入接近0时，函数对输入的变化有敏感的反应，这一敏感度将随输入绝对值的增大而下降，最终趋于0。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;权重的获取：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;选择合适的函数后，我们就要去确定各权重w了，权重的选择取决于我们想要神经网络完成的任务，我们首先会给每个输入一个初始化的默认值，该值可任意选取。&lt;/p&gt;&#xA;&lt;p&gt;完成初始化后，我们就要开始训练神经网络了，即给神经网络大量的已知的正确的输入及其对应的输出，神经网络会将自己得到到的输出与正确输出向比较，然后根据某一算法调整自身的权重，使自身输出更接近正确答案。&lt;/p&gt;&#xA;&lt;p&gt;我们这里要介绍的调整算法称为&lt;strong&gt;反向传播法&lt;/strong&gt;，因为该算法是沿网络反向调整权值的。&lt;/p&gt;&#xA;&lt;p&gt;这一算法中，我们会分析输出与正确答案，并将将输出向正确答案推进，为了了解如何推进，我们需要一个函数来计算函数f的斜率，设该函数为g。根据该函数，我们可以计算sum因改变的值。&lt;/p&gt;&#xA;&lt;p&gt;整个算法如下：&lt;/p&gt;&#xA;&lt;p&gt;从后向前对输出层和所有隐含层：&lt;/p&gt;&#xA;&lt;p&gt;1）  计算节点当前输出与期望结果的差值d。(期望结果t – 实际输出 y)&lt;/p&gt;&#xA;&lt;p&gt;对输出层: t在输入训练数据时一同输入。&lt;/p&gt;&#xA;&lt;p&gt;对隐含层: t = sum ( 前一层的每个节点的差值di * 这两个节点间连线的权值 )&lt;/p&gt;&#xA;&lt;p&gt;2）  利用函数g确定函数f在节点输出值y处的改变速率v。v=g(y)&lt;/p&gt;&#xA;&lt;p&gt;3）  改变每个输入链接的权值，其改变量与链接的当前输入强度与学习速率rate（自己定义的属于(0,1)的常量）成正比。&lt;/p&gt;&#xA;&lt;p&gt;（每个wi的改变量为（v&lt;em&gt;d&lt;/em&gt;rate*输入ai））&lt;/p&gt;&#xA;&lt;p&gt;这样一层层的从后向前反推，最终完成对一个训练样本的学习。&lt;/p&gt;&#xA;&lt;p&gt;当对所有样本完成训练后，我们就可以使用这个神经网络了。&lt;/p&gt;&#xA;&lt;p&gt;比如，我们想用神经网络模拟一个数学函数，我们先向网络提供大量的正确的输入输出进行训练，然后就可以用神经网络作模拟这个函数进行计算了。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>算法总结2—决策树分类器</title>
      <link>https://sword865.github.io/archives/20/</link>
      <pubDate>Sun, 06 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/20/</guid>
      <description>&lt;p&gt;&lt;strong&gt;数学基础：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;树：树是一种数据结构，它是由n（n&amp;gt;=1）个有限结点组成一个具有层次关系的集合。把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。它具有以下的特点：&lt;/p&gt;&#xA;&lt;p&gt;每个结点有零个或多个子结点；&lt;/p&gt;&#xA;&lt;p&gt;每一个子结点只有一个父结点；&lt;/p&gt;&#xA;&lt;p&gt;没有前驱的结点为根结点；&lt;/p&gt;&#xA;&lt;p&gt;除了根结点外，每个子结点可以分为m个不相交的子树；&lt;/p&gt;&#xA;&lt;p&gt;没有子节点的节点称为叶节点。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;决策树分类器原理：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;决策树是一颗树，要分类的样本从树根进入，在树的每个节点通过对样本的某种属性的判断选择不同的路径逐步下降到底,得出其所属类别。&lt;/p&gt;&#xA;&lt;p&gt;例图:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://pic002.cnblogs.com/images/2012/52809/2012063021362216.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;为了建立一棵决策树,我们首先应向程序输入大量训练数据(包含所属类别的数据)，程序将根据训练数据按某一算法自动生成决策树。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;决策树生成算法:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;   为了构造决策树，算法首先创建一个根节点，然后通过分析训练数据，逐步选出适合的变量对数据进行拆分(即逐步构造上图中的非叶子节点。)&lt;/p&gt;&#xA;&lt;p&gt;   为了选择适合的变量对数据进行拆分，我们需要一个方法来评估一种拆分方案的好坏，&lt;strong&gt;其评估方法包括：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&lt;strong&gt;基尼不纯度：&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;定义：基尼不存度是指来自集合的某种结果随机应用于集合中某一数据的预期误差。（如果集合中所有结果属于同一类，则误差为0）&lt;/p&gt;&#xA;&lt;p&gt;使用：利用这一思想，我们可以将集合中每种类别的数据出现的次数除以数据总数计算相应概率，再将这些概率的乘积相加（所有概率两两相乘后在相加），这样就会得到某一数据被随机分配到错误结果的总概率。&lt;/p&gt;&#xA;&lt;p&gt;伪代码：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;imp=0&#xD;&#xA;for k1 in kinds&#xD;&#xA;    p1=count(k1) / total&#xD;&#xA;    for k2 in counts&#xD;&#xA;        if (k1==k2)continue&#xD;&#xA;        p2=count(k2) / total&#xD;&#xA;        imp+=p1*p2&#xD;&#xA;ans=imp&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p align=&#34;left&#34;&gt;&#xD;&#xA;  &amp;nbsp;&amp;nbsp; (p1*p2是一个p1类别的数据被当作p2的概率)&#xD;&#xA;&lt;/p&gt;&#xD;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;**熵：**在信息论中，熵代表的是集合的无序程度—–基本上就相当于我们在此处所说的集合的混杂程度。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;熵的值是遍历所有结果后得到的pi*log2(pi)的和的绝对值&lt;/p&gt;&#xA;&lt;p&gt;伪代码：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;ent=0.0&#xD;&#xA;for k in kinds&#xD;&#xA;    p=count(k) / total&#xD;&#xA;    ent=ent – p*log2(p)     // 因为0&amp;lt;p&amp;lt;=1，所以必有log2(p)&amp;lt;=0&#xD;&#xA;ans=ent&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;有了上述评估方法后，我们就可以不断尝试各种拆分方法，然后选出最好的拆分方法构造树中的节点了。我们将计算拆分前的熵（基尼不存度）值，与拆分后的熵（基尼不存度）的值的加权平均，将其差值作为&lt;strong&gt;信息增益&lt;/strong&gt;。最终对能得到最大信息增益的属性进行拆分。然后再分别对拆分后得集合选择属性进行拆分，直到最大信息增益为非正时停止拆分，这时决策树就构建完毕了。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;优化：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;为了防止决策树变的过度拟合（过度针对训练数据），我们可以在信息增益小于某个值后就停止拆分。但是我们可能遇到这样的数据―――某次拆分信息增益很小，但下一次就会很大。为了防止这一状况，我们可以在用先前的方法构造整棵树后，在尝试消除多余的节点。这个过程就是&lt;strong&gt;剪枝&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;剪枝的过程就是对具有相同父节点的节点进行检查，判断将其合并后，信息增益是否会小于某个指定发值。若是，则合并这些节点。合并后节点包括所有可能的结果值。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;在处理数值型数据时，熵和基尼不存度并不是一个好的选择，因为有些数值相差很近，有些相差很远，不能简单用是否为同一类别进行判断。所以我们可以用方差代替它们。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;决策树对缺失数据的处理：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;当我们要判断类别的样本缺少某些决策树作判断时必须的数据时，我们可以选择同时走两个分支，不过我们不是平均统计各分支的结果值，而是进行加权统计。为了达到这一目标，决策树中每个节点都有一个值为１的权重，即观测数据对于数据向是否属于某个特定分类的概率具有１００％的影响，而如果走多个分支，我们将给每个分支一个权重，其值等于所有位于该分支的其他数据所占的比重。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;决策树最大的优势是它可以轻易对一个受训模型给予解释。（解释分类原理）&lt;/p&gt;&#xA;&lt;p&gt;决策树可以同时接受分类型和数值型数据。&lt;/p&gt;&#xA;&lt;p&gt;比起贝叶斯分类器（参考**[&amp;lt;集体智慧编程&amp;gt;算法总结1—贝叶斯分类器][2]**）决策树可以更好的处理变量间的相互影响。&lt;/p&gt;</description>
    </item>
    <item>
      <title>算法总结1—贝叶斯分类器</title>
      <link>https://sword865.github.io/archives/21/</link>
      <pubDate>Sat, 05 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/21/</guid>
      <description>&lt;p&gt; 这几天以很快的速度翻完了&amp;lt;集体智慧编程&amp;gt;,因为只是对里面的算法感兴趣,对那些web2.0的应用没什么感觉,所以很多地方都是一扫而过,现在按最后一章的顺序来对所有相关的算法作一个详细的复习….&lt;/p&gt;&#xA;&lt;p&gt;这个是第一篇……贝叶斯分类器&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;数学基础：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;条件概率&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;定义：设A, B是两个事件，且P(A)&amp;gt;0 称P(B∣A)=P(AB)/P(A)为在条件 A下发生的条件事件B发生的条件概率。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;乘法公式&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;设P(A)&amp;gt;0，则有P(AB)=P(B∣A)P(A)&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;全概率公式和贝叶斯公式&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;定义: 设S为试验E的样本空间，B1, B2, …Bn为E的一组事件，若BiBj=Ф, i≠j, i, j=1, 2, …,n; B1∪B2∪…∪Bn=S则称B1, B2, …, Bn为样本空间的一个划分。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;设试验E的样本空间为，A为E的事件，B1, B2, …,Bn为的一个划分，且P(Bi)&amp;gt;0(i=1, 2, …n)，则P(A)=P(A∣B1)P(B1)+P(A∣B2)+ …+P(A∣Bn)P(Bn)称为全概率公式。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;设试验E的样本空间为S，A为E的事件，B1, B2, …,Bn为的一个划分，则P(Bi∣A)=P(A∣Bi)P(Bi)/∑P(B｜Aj)P(Aj)=P(B｜Ai)P(Ai)/P(B)称为贝叶斯公式。&lt;/p&gt;&#xA;&lt;p&gt;说明：i，j均为下标，求和均是1到n&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;贝叶斯分类器原理：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;通过某些特征对不同的内容进行分类。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;特征的定义&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;任何可以用来判断内容中具备或缺失的东西。如要对文档进行分类时，所谓的内容就是文档，特征就是文档中的单词(当然你也可以选择其他合理的东西)&lt;/p&gt;&#xA;&lt;p&gt;当向贝叶斯分类器输入一个要进行分类的样本后，分类器会先对该样本进行分析，确定其特征，然后将根据这些特征时，计算样本属于各分类的概率。&lt;/p&gt;&#xA;&lt;p&gt;朴素贝叶斯分类器的具体工作步骤：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;学习：向分类器输入一系列的训练数据，注意这些数据是包括其所属类别的，分类器将对训练数据进行分析，计算出&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1.各个特征在各个分类中出现的概率(=某分类中具有该特征的数据数目/该分类数目)如先计算出各个单词在各种分类的文档出现的概率。&lt;/p&gt;&#xA;&lt;p&gt;将该概率作为某分类下某特征出现的条件概率P(feature|category)&lt;/p&gt;&#xA;&lt;p&gt;2.任选一个样本属于某分类的概率(=某分类文章数 / 文章总数)&lt;/p&gt;&#xA;&lt;p&gt;记该概率为p(category)&lt;/p&gt;&#xA;&lt;p&gt;在朴素的贝叶斯分类器中，我们假设将要组合的各个概率相互独立(当然，很多时候并非如此。我们有时会发现，当样本拥有某一特征时，则它就更可能拥有另一项特征。)&lt;/p&gt;&#xA;&lt;p&gt;2)分类计算：在向分类器提供大量学习数据后，我们就可以用它对新的样本进行分类了。&lt;/p&gt;&#xA;&lt;p&gt;首先对样本进行分析，找出其具有的各种特征，利用这些特征，我们来计算各个分类中出现该样本的概率p(sample | category)。为了完成这一计算，我们只要简单将该分类下在该文档中出现过的特征出现的条件概率相乘即可。即∏P(feature | category) 这里的feature是该样本拥有的所有特征。&lt;/p&gt;&#xA;&lt;p&gt;但是，我们实际要计算的是P (category | sample),即给定样本属于某分类的条件概率。&lt;/p&gt;&#xA;&lt;p&gt;这里，就用到了贝叶斯定理：P(A | B)=P(B | A)P(A) / P(B)&lt;/p&gt;&#xA;&lt;p&gt;这里就是：P(category | sample)= P(sample | category)P(category) / P(sample)&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
