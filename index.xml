<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>悟剑阁</title>
    <link>https://sword865.github.io/</link>
    <description>Recent content on 悟剑阁</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Thu, 29 Jan 2026 21:55:26 +0800</lastBuildDate>
    
        <atom:link href="https://sword865.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>模型分片，KV Cache和推理加速的一些思考：计算与数据</title>
        <link>https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</link>
        <pubDate>Thu, 29 Jan 2026 21:55:26 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/ -&lt;p&gt;这段时间主要在做推理加速相关的工作，也做了一些实验，准备写点文章做一些记录。这篇文章就从“移动计算”和“移动存储”的视角开个头，聊聊&lt;strong&gt;并行策略的动态切换&lt;/strong&gt;和&lt;strong&gt;KV Cache 流动管理&lt;/strong&gt;的一些实践，梳理一下在 Agentic Workflow 日益普及的当下，我们能否通过对&lt;strong&gt;算力、存储与带宽&lt;/strong&gt;的调度，在大规模集群上更好地提升推理效率。&lt;/p&gt;
&lt;p&gt;在传统的大数据时代，“计算中心”还是“数据中心”一直是个有趣的问题。随着技术的发展，大家也逐渐总结出了 &lt;strong&gt;&amp;ldquo;Move Compute to Data&amp;rdquo;&lt;/strong&gt; 的实践经验：因为 SSD 很贵、IO 很贵、带宽也很贵。相比之下，不如把代码调度过去，使用本地的 CPU 进行计算。因此，调度器的核心任务是保证 Data Locality，尽量把计算分发到数据所在的硬盘旁边。&lt;/p&gt;
&lt;p&gt;但在 LLM 的时代，我们面对的是超大模型在 GPU 上的推理，移动计算已经变成了移动模型（权重）这个巨无霸。相比之下，似乎还是移动数据（KV Cache）更现实一些——但什么才是最合适的解法呢？&lt;/p&gt;
&lt;h2 id=&#34;1-需求的演变从-chat-到-agent&#34;&gt;1. 需求的演变：从 Chat 到 Agent&lt;/h2&gt;
&lt;p&gt;系统架构的演进，必然是随着业务不断演变的。&lt;/p&gt;
&lt;h3 id=&#34;第一阶段单轮指令&#34;&gt;第一阶段：单轮指令&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：用户发送一句指令任务（如翻译、总结），模型执行并回复。请求之间几乎独立。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：纯粹的算力（Prefill）或显存带宽（Decoding）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;：最简单的加权轮询。此时 &lt;strong&gt;KV Cache&lt;/strong&gt; 的存在感很低，除了每台机器都有的 System Prompt，几乎没有状态复用的需求，我们可以随意把请求调度到任一台机器上执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;第二阶段多轮对话&#34;&gt;第二阶段：多轮对话&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：多轮对话可以通过 Prefix Caching 复用前面上文。Context 越来越长，每次对话对应一次交互。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：显存容量 &amp;ndash;&amp;gt; Prefill 时间&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;：开始引入 &lt;strong&gt;Affinity（亲和性）&lt;/strong&gt; 调度——为了命中 Cache，我们尽量把请求发给存储了该用户历史数据的节点。也就是 &lt;strong&gt;&amp;ldquo;Move Compute to Data&amp;rdquo;&lt;/strong&gt;，因为此时 Prefill（重算数据）太贵，而搬运 KV Cache 也还没在大规模集群中普及。但是这也会导致热点问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;第三阶段agentic-workflow&#34;&gt;第三阶段：Agentic Workflow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：系统提示词、工具定义、思维链、上下文可以在并行的分支任务中共享，多轮对话可以并行执行，但对应一次交互（用户从感知多次交互变成感知任务完成）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：极其复杂的依赖关系，以及直接复用 KV Cache 带来的负载不均衡。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度（面临的问题）&lt;/strong&gt;：如果 &amp;ldquo;Move Compute to Data&amp;rdquo; 会导致严重的热点问题——存有热门 Context 的节点会被打爆，而其他空闲节点却因为没有数据而帮不上忙。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到，随着需求的变化，我们不再只关注单次请求的 TTFT/TPOT，而是开始关注整个 Agent 任务的 &lt;strong&gt;任务完成时间&lt;/strong&gt; 以及系统的 &lt;strong&gt;总吞吐&lt;/strong&gt;量。&lt;/p&gt;
&lt;h2 id=&#34;2-核心命题移动计算-vs-移动存储&#34;&gt;2. 核心命题：移动计算 vs 移动存储&lt;/h2&gt;
&lt;p&gt;为了解决 Agent 时代的冲突，我们必须重新审视 &lt;strong&gt;调度策略的核心权衡&lt;/strong&gt;。在大数据时代，移动数据（IO）是昂贵的；但在 LLM 时代，事情变得愈加复杂：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;移动计算（权重）也变得更加复杂（移动权重、计算图编译等）。&lt;/li&gt;
&lt;li&gt;计算资源逐渐替代存储成为主要成本（某存储团队原话：我们终于不是最贵的了）。&lt;/li&gt;
&lt;li&gt;NVLink/RDMA 让移动数据（KV Cache）变得越来越便宜。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，对于 LLM 来说，不同的并行策略会带来不同的性能表现，并适配不同的请求类型（比如 batch 大小、提示词长度、预填充、解码等），因此这里我们可以定义两种“移动计算”方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把计算移动到更合适、已经部署了模型的节点（如 PD 分离），无需移动权重。&lt;/li&gt;
&lt;li&gt;把计算移动到新的、空白的节点（其实就是传统的 Auto Scaling）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在目前的实践中，我们发现，在前面第 1 种约束下，后面这一种移动计算的成本巨大。再加上我们希望最大化系统资源利用率，通常只有在任务无法被当前系统处理时，才会把它作为最终手段。所以这里先不展开，重点放在第一种移动计算上，用它来约束移动计算的成本。&lt;/p&gt;
&lt;p&gt;这样一来，调度器就面临一个二选一的困境：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;策略&lt;/th&gt;
          &lt;th&gt;A. 优先调度&lt;strong&gt;算力效率高&lt;/strong&gt;的节点 (Compute-First)&lt;/th&gt;
          &lt;th&gt;B. 优先调度&lt;strong&gt;存储效率高&lt;/strong&gt;的节点 (Data-First)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;代价&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;需要&lt;strong&gt;移动数据&lt;/strong&gt;（KV Cache 迁移，消耗带宽）&lt;/td&gt;
          &lt;td&gt;可能需要&lt;strong&gt;等待计算&lt;/strong&gt;（排队）或进行&lt;strong&gt;低效计算&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;带宽充足、延迟敏感、算力稀缺&lt;/td&gt;
          &lt;td&gt;带宽紧张、吞吐优先、算力充足&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;经典案例&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;PD 分离、过热点保护&lt;/td&gt;
          &lt;td&gt;多轮对话（KV Cache 复用）&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这个权衡并非一成不变，它取决于以下几个关键变量：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算力 vs 带宽的相对稀缺性&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果 GPU 算力极度紧张（如高峰期），让算力空转等待 Cache 命中是巨大浪费，此时应倾向于 &lt;strong&gt;A&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;如果跨节点带宽是瓶颈（如跨机房），频繁移动 KV Cache 会引入不可接受的延迟，此时应倾向于 &lt;strong&gt;B&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Batching 策略&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;大 Batch / 高吞吐场景&lt;/strong&gt;：可以容忍一定的调度延迟来凑齐 Batch，倾向于 &lt;strong&gt;B&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小 Batch / 低延迟场景&lt;/strong&gt;：每一毫秒都很宝贵，必须立即找到可用算力，倾向于 &lt;strong&gt;A&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;延迟 vs 吞吐的优化目标&lt;/strong&gt;：&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-shift-parallelism节点内的移动计算&#34;&gt;3. Shift Parallelism：节点内的移动计算&lt;/h2&gt;
&lt;p&gt;Snowflake 提出的 &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2509.16495&#34;&gt;Shift Parallelism&lt;/a&gt;&lt;/strong&gt; 是一个非常巧妙的设计，通过对计算进行预先编排（移动计算），把运行时移动数据的成本降到了 0。&lt;/p&gt;
&lt;p&gt;传统的推理部署要么是 &lt;strong&gt;TP&lt;/strong&gt;（Tensor Parallelism，低延时），要么是 &lt;strong&gt;SP/DP&lt;/strong&gt;（Sequence/Data Parallelism，高吞吐）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;低负载时&lt;/strong&gt;：我们想要 TP 来降低 Latency。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高负载时&lt;/strong&gt;：我们想要 SP 来提升 Throughput。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shift Parallelism 的核心观点是：&lt;strong&gt;与其移动数据（KV Cache），不如改变计算方式（并行策略）。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;存储共享kv-cache-invariance&#34;&gt;存储共享：KV Cache Invariance&lt;/h3&gt;
&lt;p&gt;Shift Parallelism 通过对不同并行策略的研究，发现 Ulysses SP 策略下 KV Cache 的内存布局与经典 TP 模式下完全一致。这意味着在运行时从 TP 切换到 SP 时，&lt;strong&gt;KV Cache 无需移动或重排&lt;/strong&gt;，可以通过巧妙的计算编排，避免对存储进行移动。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2026/20260129/SP_TP_compare.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;可以看出来，Shift Parallelism 选择了“存储共享”：通过预先对计算进行编排，把计算从一个进程移动到另一个进程来提升整体算力，降低计算能力的稀缺性。&lt;/p&gt;
&lt;p&gt;不过，为了支持 TP 和 SP 的无缝切换，GPU 必须同时存储 TP 和 SP 所需要的不同权重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一些实验观察&lt;/strong&gt;：
我也基于相关代码 &lt;a href=&#34;https://github.com/snowflakedb/ArcticInference&#34;&gt;ArcticInference&lt;/a&gt; 做了一些实验和分析。在目前的实现中，TP 和 SP 实际上是存了两份权重，其中在每块 GPU 上，SP 所需要的权重又是 TP 的超集。&lt;/p&gt;
&lt;p&gt;虽然理论上可以只保留 SP 一份权重，但对性能会有极大的损耗，这里主要问题在于 GEMM 计算时对&lt;strong&gt;权重矩阵连续性&lt;/strong&gt;的要求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算 Kernel（如 cuBLAS）通常要求输入 Tensor 在内存中是连续的，直接切分 SP 的权重矩阵无法满足这一要求。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;虽然理论上可以通过编写定制的 &lt;strong&gt;CUDA Kernel&lt;/strong&gt; 来处理这种复杂的非连续内存映射，但工程复杂度较高，而且分散的内存读取极易破坏合并访问，仍然会导致实际带宽利用率下降。&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，Shift Parallelism 使用 &lt;strong&gt;显存空间（冗余权重）&lt;/strong&gt; 换取了 &lt;strong&gt;计算移动的灵活性（动态并行策略）&lt;/strong&gt;，从而避免了高昂的运行时数据搬运。&lt;/p&gt;
&lt;h2 id=&#34;4-llm-d集群间的数据流&#34;&gt;4. llm-d：集群间的“数据流”&lt;/h2&gt;
&lt;p&gt;如果说 Shift Parallelism 延续的是“让计算适应数据”的思路，那么 &lt;strong&gt;llm-d&lt;/strong&gt; 则提出了“让数据移向计算”：作为 &lt;a href=&#34;https://docs.d.run/en/blogs/2026/llm-d#the-vision-a-context-aware-inference-ecosystem&#34;&gt;Agentic Runtime&lt;/a&gt;，主张 &lt;strong&gt;KV Cache 在集群间流动&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;打破-data-locality-执念&#34;&gt;打破 Data Locality 执念&lt;/h3&gt;
&lt;p&gt;前文提到，Agent 工作流的负载较为复杂，而且极不均衡。llm-d 作为集群的入口，不再只关心单次请求的流量，而是希望整体优化任务完成时间。其社区最新的提案中，提倡利用 P2P NVLink/RDMA 实现 KV Cache 的快速流转。这意味着调度器可以更自由地选择&lt;strong&gt;最空闲&lt;/strong&gt;或&lt;strong&gt;逻辑最近&lt;/strong&gt;的计算节点，而不是被历史遗留的 Cache 死死绑在某个过载的节点上。&lt;/p&gt;
&lt;p&gt;这让人想起云原生数据库从“Shared-Nothing”走向“存算分离（Shared-Storage/Memory）”的演进路线。未来的推理集群，很可能是一个巨大的、通过高速总线互联的共享显存池。&lt;/p&gt;
&lt;h3 id=&#34;语义化-kv-cache&#34;&gt;语义化 KV Cache&lt;/h3&gt;
&lt;p&gt;为了让这种“流动”更高效，社区认为 llm-d 不能傻傻地移动所有数据，而是应该让集群调度器理解 Agent 工作流，理解 KV Cache：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;System Prompt / Tool 定义&lt;/strong&gt;：高频复用的“静态”数据 -&amp;gt; **主动复制（推送）**到多个节点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning Branch&lt;/strong&gt;：用完即弃的“瞬态”数据 -&amp;gt; &lt;strong&gt;低优先级驱逐&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里又回到了权衡：对于高频数据，我们通过复制来减少传输；对于低频数据，我们通过传输来平衡算力。&lt;/p&gt;
&lt;p&gt;这实际上对 llm-d 的调度提出了更高的要求：KV Cache 的复制不再是请求到来时的被动转移，系统需要具备类似于指令预取的调度预测能力，通过提前对 KV Cache 进行编排来保证缓存的可用性。&lt;/p&gt;
&lt;h2 id=&#34;5-思考与未来重构推理系统的时空观&#34;&gt;5. 思考与未来：重构推理系统的时空观&lt;/h2&gt;
&lt;p&gt;将两者的思路结合来看，未来的推理系统架构可能会展现出以下特征：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;层次化的灵活性&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;在节点内部&lt;/strong&gt;: 利用类似 Shift Parallelism 的技术，&lt;strong&gt;保持 KV Cache 不动&lt;/strong&gt;，动态调整计算图（TP/SP），平衡延迟与吞吐。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在集群层面&lt;/strong&gt;: 利用语义化 KV Cache 和 P2P 网络，&lt;strong&gt;主动移动 KV Cache&lt;/strong&gt;，平衡节点间的负载。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KV Cache 从“静态缓存”变为“流动的数据”&lt;/strong&gt;：
它不再是单纯被动被命中的 Cache，而是被系统调度的核心资源。它可以作为静态数据驻留，也可以按照实际需求动态流转。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;智能调度与流量塑性&lt;/strong&gt;：
为了更智能地进行调度，未来的调度策略不会只停留在被动转发流量，而是需要主动参与数据聚合和 KV Cache 的流动，让不同类型的负载在时间维度上更好地重叠与错峰，从而摊薄一些难以避免的开销。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算语境的代价与以空间换时间&lt;/strong&gt;：
无论是 Shift Parallelism 中冗余的权重，还是 llm-d 中主动复制的上下文，其本质都在说明：&lt;strong&gt;权重和上下文本质上都是计算语境的一部分&lt;/strong&gt;。
&lt;ul&gt;
&lt;li&gt;运行时重构语境（重新切分权重、重算上下文）极其昂贵（占用 SM、需要重新编译 CUDA 图）。&lt;/li&gt;
&lt;li&gt;因此，&lt;strong&gt;&amp;ldquo;以空间换时间&amp;rdquo;&lt;/strong&gt; 将成为常态。即使显存昂贵，但为了更好更便利的计算流动性，我们依然愿意支付这份“存储税”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后简单提一下百度最新的研究（如 &lt;a href=&#34;https://arxiv.org/pdf/2512.16134&#34;&gt;SPS&lt;/a&gt;）。文章中提出的策略已经不再是简单的实时流量转发，而是通过感知负载的方式对流量进行重组：调度器可以把请求缓存在本地，等满足一定条件后再统一下发，主动将负载塑造成适合当前系统状态的形式。这相当于把原来在服务端做的动态攒批能力，上移到了网关调度层。&lt;/p&gt;
&lt;p&gt;延续这些研究的思路，未来推理系统与调度服务之间的边界会变得越来越模糊，“网关 + 推理引擎”这两层调度器之间也会越来越紧密。调度的思路也会从被动处理请求，转向主动管理计算和数据：在上游通过攒批塑造流量形态，在中游通过并行策略切换匹配算力形态，在下游通过 KV Cache 的流动与复制打通存储形态。&lt;/p&gt;
&lt;p&gt;看上去推理加速正在从单纯的“算子优化”走向“系统级编排”，这个演变的核心，仍然围绕着计算和存储的移动。在叠加多模态、稀疏注意力、线性注意力、Engram 等架构优化之后，我们以后看到的也许是一个巨大的推理集群，而不再是一个“网关 -&amp;gt; 推理引擎”的两层服务。&lt;/p&gt;
&lt;p&gt;总之我们在 26 年还有很多可以继续探索的空间。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Linear Attention基础-工程篇</title>
        <link>https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/</link>
        <pubDate>Thu, 01 Jan 2026 15:23:41 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/ -&lt;p&gt;本文重点参考了文章&lt;a href=&#34;https://srush.github.io/annotated-mamba/hard.html&#34;&gt;Mamba: The Hard Way&lt;/a&gt;和开源项目&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention&#34;&gt;flash-linear-attention&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&#34;prefill与decoding&#34;&gt;Prefill与Decoding&lt;/h1&gt;
&lt;p&gt;我们都知道，在Attention的计算中，Prefill和Decoding是两个不同的场景，具体特性如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;特性&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Prefill&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Decoding&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;输入&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;长序列（长度 $L$）&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;1 个新 token + 历史状态&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;常见瓶颈&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Compute bound（Tensor Core 利用）&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Memory/Latency bound（状态读写 + 小矩阵计算）&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在回忆一下理论篇的介绍，特别是关于Mamba章节中的推导，常见的Linear Attention有两种表示格式：&lt;/p&gt;
&lt;p&gt;矩阵格式（Attention视角）：&lt;/p&gt;
&lt;p&gt;$$ y_i = \sum_{j=0}^i (CausalMask(Q_i K_j^T)) V_j $$&lt;/p&gt;
&lt;p&gt;递推格式（SSM视角）：&lt;/p&gt;
&lt;p&gt;$$ h_t = A_t h_{t-1} + B_t x_t $$
$$ y_t = C h_t $$&lt;/p&gt;
&lt;p&gt;其中Decoding的算子可以比较直接的使用递归格式进行计算，因此我们本文重点还是看Prefill的实现。&lt;/p&gt;
&lt;h1 id=&#34;linear-attention常见算法&#34;&gt;Linear Attention常见算法&lt;/h1&gt;
&lt;p&gt;在Linear Attention的计算中，有一些常见的思路，本章结合&lt;code&gt;flash-linear-attention&lt;/code&gt;的实现，对这些思路进行讲解。&lt;/p&gt;
&lt;h2 id=&#34;prefix-scan--cumsum-前缀和&#34;&gt;Prefix Scan / Cumsum 前缀和&lt;/h2&gt;
&lt;h3 id=&#34;算法简介&#34;&gt;算法简介&lt;/h3&gt;
&lt;p&gt;先讲一下什么是Prefix Scan算法，简单来说Prefix是针对有结合性的算子提出的一种优化方式&lt;/p&gt;
&lt;p&gt;假设有 $y_t = x_1 ⊗ x_2 ⊗ x_3 &amp;hellip; ⊗ x_t$, 如果⊗支持结合律，那么y_t就可以用一个简单的Reduce进行求解&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;=== Parallel Prefix Scan (Up Sweep) ===
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;序列: [x1][x2][x3][x4][x5][x6][x7][x8]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Step 1: 相邻元素两两合并 (4 次并行操作)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        [x_1] [x1 ⊗ x2] [x_3] [x3 ⊗ x4] [x_5] [x5 ⊗ x6] [x_7] [x7 ⊗ x8]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Step 2: 继续合并 (2 次并行操作)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        [x_1] [x_1 ⊗ x_2] [x_3] [x1 ⊗ x2 ⊗ x3 ⊗ x4] [x_5] [x5 ⊗ x6] [x_7] [x5 ⊗ x6 ⊗ x7 ⊗ x8]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Step 3: 最终合并 + Down-sweep (恢复所有前缀和)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        [完整的前缀和序列(y_t)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;总计: O(log L) 轮，每轮 O(L) 次操作
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们把这个步骤称为Up Sweep，这个过程中我们不仅计算了最终的$y_t$，也产生了很多中间结果，我们可以把这个数组看成一个二叉树。
回忆一下，我们要算的可能不只有最终的 $y_L$，我们想计算所有的 $y_1$ 到 $y_L$。如果只用 Up Sweep，我们只能得到最后一个位置的完整前缀和。为了高效地得到所有位置的前缀和，我们需要引入 &lt;strong&gt;Down Sweep&lt;/strong&gt; 阶段。&lt;/p&gt;
&lt;p&gt;Down Sweep 的核心思想是：&lt;strong&gt;利用 Up Sweep 阶段在树节点中留下的中间结果，从根节点开始向下分发“左侧累加值”信息。&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;=== Parallel Prefix Scan (Down Sweep) ===
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Up Sweep 后的状态: [x1] [x1⊗x2] [x3] [x1⊗x2⊗x3⊗x4] [x5] [x5⊗x6] [x7] [x1⊗...⊗x8]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Step 1: 置零 (Set to Identity)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        将最后一个元素（根节点）设为单位元（如 0）。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        这是因为第一个元素的前缀和应该是 0。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        [x1] [x1⊗x2] [x3] [x1⊗x2⊗x3⊗x4] [x5] [x5⊗x6] [x7] [0]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Step 2: 交换与分发 (Swap and Sum)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        从上往下，每一层进行如下操作：
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        1. 暂存当前节点的左孩子值。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        2. 将当前节点的值赋给左孩子。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        3. 将“暂存的左孩子值 ⊗ 当前节点值”赋给右孩子。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (第一轮分发：将根节点的 0 传给左半区，将左半区的总和传给右半区)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        [x1] [x1⊗x2] [x3] [0] [x5] [x5⊗x6] [x7] [x1⊗x2⊗x3⊗x4]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Step 3: 递归向下 (Recursive Down)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        继续向下重复此过程，直到叶子节点。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        最终数组会变成 Exclusive Scan 序列（即每个位置存储它之前所有元素的和）。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Step 4: 恢复 Inclusive Scan
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        将 Exclusive Scan 的结果与原始序列对应位置进行 ⊗ 操作，即可得到 y_1 到 y_L。
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;总计: O(log L) 轮，每轮 O(L) 次操作，总复杂度依然是 O(L)。
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这两个过程可以从两个个树型的计算来表示：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2026/20260101/prefix-scan-tree.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;顺带的，给出一个例子：
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2026/20260101/prefix-scan-tree-example.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;linear-attention的prefix-scan&#34;&gt;Linear Attention的Prefix Scan&lt;/h3&gt;
&lt;p&gt;了解了 Prefix Scan 算法，我们来看一下 Linear Attention 的递推公式：&lt;/p&gt;
&lt;p&gt;$$ h_t = A_t h_{t-1} + B_t x_t $$
$$ y_t = C h_t $$&lt;/p&gt;
&lt;p&gt;这个公式描述了一个一阶线性递推过程。在传统的 RNN 中，我们需要串行地计算 $h_1, h_2, \dots, h_L$，这在训练长序列时会成为严重的瓶颈。为了将其转化为 Prefix Scan，我们需要找到一个&lt;strong&gt;结合律算子&lt;/strong&gt; $\otimes$。&lt;/p&gt;
&lt;h4 id=&#34;数学推导线性递推的结合律&#34;&gt;数学推导：线性递推的结合律&lt;/h4&gt;
&lt;p&gt;为什么线性递推可以转化为 Prefix Scan？关键在于将状态转移看作是&lt;strong&gt;线性函数的复合&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对于任意时刻 $t$，状态更新公式为：
$$ h_t = A_t h_{t-1} + b_t \quad (\text{其中 } b_t = B_t x_t) $$&lt;/p&gt;
&lt;p&gt;我们可以将这个过程看作是一个线性变换 $f_t(h) = A_t h + b_t$。那么 $h_t$ 实际上是这些变换的连续嵌套：
$$ h_t = f_t(f_{t-1}(\dots f_1(h_0) \dots)) $$&lt;/p&gt;
&lt;p&gt;如果我们定义一个算子 $\otimes$ 来表示两个线性变换的复合 $f_j \circ f_i$（假设 $j &amp;gt; i$，即 $j$ 是较晚的时刻）：
$$ (f_j \circ f_i)(h) = A_j (A_i h + b_i) + b_j = (A_j A_i) h + (A_j b_i + b_j) $$&lt;/p&gt;
&lt;p&gt;因此，我们可以定义元组 $(A, b)$ 上的算子 $\otimes$：
$$ (A_j, b_j) \otimes (A_i, b_i) = (A_j A_i, A_j b_i + b_j) $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个算子的物理意义是&lt;/strong&gt;：它计算了从状态 $h_{i-1}$ 到 $h_j$ 的“总转移矩阵”和“总偏置项”。
当我们对序列 $[(A_1, b_1), (A_2, b_2), \dots, (A_L, b_L)]$ 执行 Prefix Scan 时，第 $t$ 个位置的结果 $(A_{1:t}, b_{1:t})$ 就代表了从初始状态 $h_0$ 到 $h_t$ 的完整变换参数：
$$ h_t = A_{1:t} h_0 + b_{1:t} $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;证明结合律&lt;/strong&gt;：
假设有三个连续的状态转移 $(A_3, b_3), (A_2, b_2), (A_1, b_1)$：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;左结合&lt;/strong&gt;：$((A_3, b_3) \otimes (A_2, b_2)) \otimes (A_1, b_1) = (A_3 A_2, A_3 b_2 + b_3) \otimes (A_1, b_1) = (A_3 A_2 A_1, (A_3 A_2) b_1 + (A_3 b_2 + b_3))$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;右结合&lt;/strong&gt;：$(A_3, b_3) \otimes ((A_2, b_2) \otimes (A_1, b_1)) = (A_3, b_3) \otimes (A_2 A_1, A_2 b_1 + b_2) = (A_3 A_2 A_1, A_3 (A_2 b_1 + b_2) + b_3)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;展开后可以发现两者完全一致。这意味着我们可以使用 Parallel Prefix Scan 在 $O(\log L)$ 的时间内并行计算出所有的 $h_t$。&lt;/p&gt;
&lt;h4 id=&#34;代码实现参考&#34;&gt;代码实现参考&lt;/h4&gt;
&lt;p&gt;在 &lt;code&gt;flash-linear-attention&lt;/code&gt; 或 &lt;code&gt;Mamba&lt;/code&gt; 的实现中，通常会利用 &lt;code&gt;jax.lax.associative_scan&lt;/code&gt; 或在 Triton 中手动实现类似的逻辑。以下是一个简化的逻辑示例：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;associative_scan_op&lt;/span&gt;(q_earlier, q_later):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    q = (A, b)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    计算两个线性变换的复合: f_later(f_earlier(h))
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    a_i, b_i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q_earlier
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    a_j, b_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q_later
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 新的 A 是两个矩阵的乘积 (注意顺序: 晚的在左)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 新的 b 是 晚的A * 早的b + 晚的b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a_j &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; a_i, a_j &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b_i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_j
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 1. 准备输入元组序列&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# A_bars: [L, d_state]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# b_bars: [L, d_state], 即 B_t * x_t&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;inputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (A_bars, b_bars)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 2. 执行并行前缀扫描&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 返回每个位置 t 从 h_0 到 h_t 的总变换参数 (A_1:t, b_1:t)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;combined_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; associative_scan(associative_scan_op, inputs)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 3. 计算隐藏状态 h_t (假设 h_0 = 0)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# h_t = A_1:t * h_0 + b_1:t = b_1:t&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;h_states &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; combined_params[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 4. 计算输出 y = C * h&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;outputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; C &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; h_states
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种表示法的精妙之处在于，它将原本必须串行执行的 &lt;code&gt;for&lt;/code&gt; 循环（$h_t$ 依赖 $h_{t-1}$）转化为了树状的并行规约过程。在 GPU 上，这意味着我们可以利用数千个核心同时处理序列的不同部分，极大地提升了 Prefill 阶段的吞吐量。&lt;/p&gt;
&lt;h2 id=&#34;chunk-wise-parallel分块并行&#34;&gt;Chunk-wise Parallel：分块并行&lt;/h2&gt;
&lt;p&gt;Chunk wise Parallel 是 &lt;code&gt;flash-linear-attention&lt;/code&gt; 中最核心的 Prefill实现方式包括&lt;code&gt;Mamba the hard way&lt;/code&gt;也是在讲解这一思想，虽然这一思想在具体实现时根据Kernel的划分可以有多种不同的实现方式，但是其核心思想始终都是 &lt;strong&gt;Two-pass&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;具体来说，我们把整个要计算的序列（长度L）拆分成多个不同的Chunk（长度C），然后按照进行计算：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pass 1：计算每个 Chunk 的最后一个token的隐藏状态（顺序计算，一共$L/C$ 步，产生 $L/C$ 个状态）&lt;/li&gt;
&lt;li&gt;Pass 2：在Chunk 内做局部计算 + 叠加来自前序 Chunk 的状态贡献（并行计算、MatMul 密集）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;该计算过程可以用下图表示：
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2026/20260101/chunk-wise.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;第一遍Pass顺序对每个Chunk进行计算，得到所有深绿色方块。&lt;/li&gt;
&lt;li&gt;第二遍Pass同时在Chunk内先基于深绿色方块来计算所有浅绿色方块，然后计算最终要输出的蓝色方块。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;triton-伪代码&#34;&gt;Triton 伪代码&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 1. Inter-chunk: 状态传递 (Pass 1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 类似于 RNN，但只在 Chunk 边界进行&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_chunks):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 加载上一个 Chunk 的状态 (i=0 时为初始状态 S_0)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b_s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(s_ptr &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; stride) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 更新状态：S = S * Decay + K^T * V (当前 Chunk 的贡献)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    b_s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b_s &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; chunk_decay &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;trans(b_k_block), b_v_block)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 保存状态供下一个 Chunk 使用&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;store(s_ptr &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; stride, b_s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 2. Intra-chunk: 块内并行计算 (Pass 2)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 类似于标准 Attention，但只看 Chunk 内部&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 加载当前 Chunk 的 Q, K, V&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(q_ptr)  &lt;span style=&#34;color:#75715e&#34;&gt;# [BLOCK_SIZE, D_K]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(k_ptr)  &lt;span style=&#34;color:#75715e&#34;&gt;# [BLOCK_SIZE, D_K]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(v_ptr)  &lt;span style=&#34;color:#75715e&#34;&gt;# [BLOCK_SIZE, D_V]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 计算局部 Attention Score: Q @ K^T&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(b_q, tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;trans(b_k)) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; decay_mask  &lt;span style=&#34;color:#75715e&#34;&gt;# [BLOCK_SIZE, BLOCK_SIZE]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 计算输出&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_o &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(b_s, b_v)  &lt;span style=&#34;color:#75715e&#34;&gt;# [BLOCK_SIZE, D_V]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 加上来自前一个 Chunk 状态的贡献&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b_o &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(b_q, prev_s) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; decay
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;fft和卷积型线性注意力&#34;&gt;FFT和卷积型线性注意力&lt;/h2&gt;
&lt;h3 id=&#34;核心思想卷积加速&#34;&gt;核心思想：卷积加速&lt;/h3&gt;
&lt;p&gt;对于某些Linear Attention变体（如Hyena），其特殊之处在于：&lt;strong&gt;衰减权重只依赖于相对位置&lt;/strong&gt;，而不是绝对位置。这意味着我们可以把计算转化为卷积操作，从而利用FFT加速。&lt;/p&gt;
&lt;h4 id=&#34;从递推到卷积&#34;&gt;从递推到卷积&lt;/h4&gt;
&lt;p&gt;考虑一个简化的输出公式：
$$o_t = \sum_{j=1}^t u_j \cdot \alpha_{t-j}$$&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u_j$ 是时刻 $j$ 的输入表示&lt;/li&gt;
&lt;li&gt;$\alpha_{t-j}$ 是&lt;strong&gt;只依赖于距离 $(t-j)$ 的衰减系数&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到这里$\alpha$ 的下标是 $t-j$（相对距离），和&lt;strong&gt;卷积&lt;/strong&gt;的定义是可以对上的。&lt;/p&gt;
&lt;p&gt;直接计算卷积的复杂度是 $O(L^2)$（对每个位置 $t$，都要累加前面所有的 $j$）。但根据&lt;strong&gt;卷积定理&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;$$\text{时域卷积} = \text{频域点乘}$$
$$f * g = \mathcal{F}^{-1}(\mathcal{F}(f) \cdot \mathcal{F}(g))$$&lt;/p&gt;
&lt;p&gt;FFT的复杂度是 $O(L \log L)$，频域点乘是 $O(L)$，总复杂度 $O(L \log L)$ 远小于 $O(L^2)$。&lt;/p&gt;
&lt;p&gt;举一个具体的例子：&lt;/p&gt;
&lt;p&gt;假设序列长度 $L=4$，输入 $u = [1, 2, 3, 4]$，衰减核 $\alpha = [1.0, 0.5, 0.25, 0.125]$（距离越远衰减越强）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;直接计算&lt;/strong&gt;（$O(L^2)$）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;o_1 = u_1 * α_0 = 1 * 1.0 = 1.0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;o_2 = u_2 * α_0 + u_1 * α_1 = 2*1.0 + 1*0.5 = 2.5
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;o_3 = u_3 * α_0 + u_2 * α_1 + u_1 * α_2 = 3*1.0 + 2*0.5 + 1*0.25 = 4.25
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;o_4 = u_4 * α_0 + u_3 * α_1 + u_2 * α_2 + u_1 * α_3 = 4*1.0 + 3*0.5 + 2*0.25 + 1*0.125 = 6.125
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;FFT加速&lt;/strong&gt;（$O(L \log L)$）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1. FFT(u) 和 FFT(α)  → 转到频域（O(L*logL)）
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2. 频域点乘           → 逐元素相乘（O(L)）
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3. IFFT               → 转回时域（O(L*logL)）
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;代码实现&#34;&gt;代码实现&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fft_conv&lt;/span&gt;(u, k, seqlen):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    使用 FFT 加速因果卷积计算
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    参数:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        u: 输入序列 [batch, seqlen, dim]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        k: 衰减核 [seqlen] (只依赖相对位置)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        seqlen: 序列长度
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    返回:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        输出序列 [batch, seqlen, dim]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 1. 补零到 2*L (避免循环卷积的边界问题)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    fft_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; seqlen
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 2. FFT 变换到频域 (O(L log L))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    k_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rfft(k, n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;fft_size)  &lt;span style=&#34;color:#75715e&#34;&gt;# 衰减核&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    u_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rfft(u, n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;fft_size)  &lt;span style=&#34;color:#75715e&#34;&gt;# 输入信号&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 3. 频域点乘 (O(L)) -&amp;gt; 这一步替代了原本 O(L^2) 的时域卷积。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; u_f &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; k_f
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 4. IFFT 逆变换回时域 (O(L log L))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fft&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;irfft(y_f, n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;fft_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# 5. 截取有效部分（去掉补零部分）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y[&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;, :seqlen]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;虽然看上去FFT是一个很好的算法，但是因为大部分的Linear Attention变体的衰减核不只和位置有关(会受到输入x影响)，因此，FFT加速现在并不常见，只在特定架构中有效。&lt;/p&gt;
&lt;h1 id=&#34;进一步的计算效率与性能优化&#34;&gt;进一步的计算效率与性能优化&lt;/h1&gt;
&lt;h3 id=&#34;delta-netwy表示与ut变换&#34;&gt;Delta Net：WY表示与UT变换&lt;/h3&gt;
&lt;p&gt;我们在前文中讲解了基础的Chunk Wise算法框架，但是在实践中这个框架并不能直接out-of-box的拿出来使用，这里以Delta Net为例讲解一下WY表示和UT变换（KDA中也使用了类似的方法）&lt;/p&gt;
&lt;p&gt;先回顾一下 Chunk-wise 算法的核心思想（Two-pass）：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pass 1 (Inter-chunk)&lt;/strong&gt;：顺序计算每个 Chunk 的边界状态&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于传统 Linear Attention：$S[i+1] = S[i] + V[i]^T K[i]$（矩阵乘法，高效）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Pass 2 (Intra-chunk)&lt;/strong&gt;：并行计算 Chunk 内部的输出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要把 Chunk 内的局部累积效果表示成矩阵乘法形式&lt;/li&gt;
&lt;li&gt;例如：$O[i] = Q[i] S[i]^T + (\text{Chunk内部的局部贡献})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了提高计算效率，这两个 Pass 都会希望计算必须能表示成&lt;strong&gt;高效的矩阵乘法&lt;/strong&gt;（可以利用 GPU Tensor Core），而不是串行循环或者稠密的 $d \times d$ 矩阵加减法操作。&lt;/p&gt;
&lt;h4 id=&#34;deltanet-原始形式的问题&#34;&gt;DeltaNet 原始形式的问题&lt;/h4&gt;
&lt;p&gt;在传统的 Linear Attention 中，状态更新是简单的加法：$S_t = S_{t-1} + v_t k_t^T$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chunk 内累积：$S[i+1] = S[i] + \sum_{t=1}^{C} v_t k_t^T = S[i] + V^T K$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是回忆一下理论篇的推导，在 &lt;strong&gt;DeltaNet&lt;/strong&gt; 中， &lt;strong&gt;Delta Rule&lt;/strong&gt;的更新规则会更加复杂：
$$ S_t = S_{t-1}(I - \beta_t k_t k_t^{\top}) + \beta_t v_t k_t^{\top} $$&lt;/p&gt;
&lt;p&gt;现在让我们尝试在 Chunk 内做累积，就会发现很多计算效率上的问题：&lt;/p&gt;
&lt;p&gt;经过论文&lt;a href=&#34;https://arxiv.org/pdf/2406.06484&#34;&gt;Parallelizing Linear Transformers with the Delta Rule over Sequence Length&lt;/a&gt;中Section 2的公式推导，可以发现在一个 Chunk 内，从 $S[i]$ 到 $S[i+1]$ 的更新公式为：&lt;/p&gt;
&lt;p&gt;$$ S[i+1] = S[i] \prod_{t=1}^C \bigl(I - \beta_t k_t k_t^{\top}\bigr) + \sum_{t=1}^C \left( \beta_t v_t k_t^{\top} \prod_{j=t+1}^C \bigl(I - \beta_j k_j k_j^{\top}\bigr) \right). $$&lt;/p&gt;
&lt;p&gt;其中符号含义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$C$：Chunk size（每个 Chunk 的长度）&lt;/li&gt;
&lt;li&gt;$S[i]$：第 $i$ 个 Chunk 的输入状态&lt;/li&gt;
&lt;li&gt;$S[i+1]$：第 $i$ 个 Chunk 的输出状态（传递给下一个 Chunk）&lt;/li&gt;
&lt;li&gt;$t$：当前 Chunk 内的位置索引（$t = 1, 2, \dots, C$）&lt;/li&gt;
&lt;li&gt;$k_t, v_t$：第 $i$ 个 Chunk 内第 $t$ 个位置的 key 和 value&lt;/li&gt;
&lt;li&gt;$\beta_t$：第 $i$ 个 Chunk 内第 $t$ 个位置的学习率/遗忘门&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还有公式种两个项的含义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;第一项&lt;/strong&gt;：前一个 Chunk 的状态 $S[i]$ 经过当前 Chunk 内所有 Delta 更新的累积衰减&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第二项&lt;/strong&gt;：当前 Chunk 内每个位置 $t$ 的贡献 $\beta_t v_t k_t^T$，并经过后续位置 $j &amp;gt; t$ 的衰减&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于这个公式，如果我们直接计算 $P[i] = \prod_{t=1}^C (I - \beta_t k_t k_t^T)$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;展开两个矩阵的乘积：$(I - \beta_2 k_2 k_2^T)(I - \beta_1 k_1 k_1^T) = I - \beta_2 k_2 k_2^T - \beta_1 k_1 k_1^T + \beta_1\beta_2 k_2 k_2^T k_1 k_1^T$&lt;/li&gt;
&lt;li&gt;继续乘下去，交叉项会越来越多，经过 $C$ 步，结果变成一个&lt;strong&gt;秩为 $O(C)$ 的稠密矩阵&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于这种方式计算 $P[i]$ 是非常低效的&amp;ndash;&amp;gt;因为我们&lt;strong&gt;无法高效计算和存储&lt;/strong&gt;这个计算过程：需要 $O(d^2)$ 的空间，计算复杂度是 $O(Cd^3)$&lt;/p&gt;
&lt;h4 id=&#34;解决方案wy-表示--ut-变换&#34;&gt;解决方案：WY 表示 + UT 变换&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;WY 表示：让 Chunk 累积可以用矩阵乘法表示&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以看到，高秩矩阵值的状态对于chunk-wise的计算和存储都是一个很大的挑战，因此我们需要引入一些变换来解决这个问题。&lt;/p&gt;
&lt;p&gt;针对DeltaNet，一个关键的发现是：虽然直接计算 $\prod (I - \beta_t k_t k_t^T)$ 会秩爆炸，但利用 Householder 变换的数学性质，这个连乘&lt;strong&gt;依然可以紧凑地表示&lt;/strong&gt;为：
$$ \prod_{i=1}^C \bigl(I - \beta_i k_i k_i^{\top}\bigr) = I - W_C K_C^{\top}, $$
其中 $W_C \in \mathbb{R}^{C \times d}$，$K_C \in \mathbb{R}^{C \times d}$。&lt;/p&gt;
&lt;p&gt;这样，Inter-chunk 更新就变成了：
$$ S[i+1] = S[i] \bigl(I - W[i] K[i]^{\top}\bigr) + U[i]^{\top} K[i] = S[i] - (S[i]W[i])K[i]^{\top} + U[i]^{\top} K[i] $$&lt;/p&gt;
&lt;p&gt;这是一个标准的&lt;strong&gt;矩阵乘法&lt;/strong&gt;，我们可以更好地利用Tensor Core, 计算复杂度 $O(Cd^3)$ 降到了 $O(Cd^2)$，同时状态空间也从 $O(d^2)$ 变成了 $O(dC)$。大大降低了计算和存储IO的负担。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UT 变换：让 W 的计算也能并行化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但是，$W$ 矩阵中的每一行 $w_t$ 的计算仍然还是递归的：
$$ w_t = \beta_t \Bigl(k_t - \sum_{i=1}^{t-1} w_i (k_i^{\top} k_t)\Bigr) $$&lt;/p&gt;
&lt;p&gt;这又是&lt;strong&gt;串行计算&lt;/strong&gt;，无法利用 Tensor Core。&lt;/p&gt;
&lt;p&gt;UT 变换通过定义下三角矩阵 $A$（包含 $k_i^T k_j$ 的信息），将递归转化为：
$$ W = T \mathrm{diag}(\beta) K, \qquad \text{其中 } T = (I - A)^{-1} $$&lt;/p&gt;
&lt;p&gt;由于 $A$ 是严格下三角，$T$ 可以通过前向替换高效求解，或者在 Chunk 较小时直接用矩阵乘法，于是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;计算变成了矩阵运算&lt;/strong&gt;，可以调用GPU Tensor Core加速计算&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;在 Chunk 内并行处理&lt;/strong&gt;，不在需要一步步串行计算&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;WY 表示 + UT 变换&lt;/strong&gt;将 DeltaNet 的 Chunk 内计算转化为了 GPU 友好的矩阵乘法，这让DeltaNet的 Chunk-wise 并行算法真正可行。&lt;/p&gt;
&lt;h1 id=&#34;性能测试与分析&#34;&gt;性能测试与分析&lt;/h1&gt;
&lt;p&gt;最后，我在半张H200(H200 MIG3-70G)的GPU上对上面的算法做了一些性能测试：&lt;/p&gt;
&lt;h2 id=&#34;simple-gla性能测试&#34;&gt;SIMPLE GLA性能测试&lt;/h2&gt;
&lt;h3 id=&#34;算法特性&#34;&gt;算法特性&lt;/h3&gt;
&lt;p&gt;Simple GLA 相比完整的 GLA，采用了 &lt;strong&gt;head-wise gating&lt;/strong&gt; 而非 elementwise gating。这一简化减少了参数量，也降低了数值不稳定的概率。&lt;/p&gt;
&lt;p&gt;其状态更新公式为：
$$S_{t+1} = g_{t+1} \odot S_{t} + K_{t+1} V_{t+1}^{\top}$$
其中 $g$ 是一个标量，这使得衰减操作可以高效地融合到矩阵计算中。&lt;/p&gt;
&lt;h3 id=&#34;性能测试对比&#34;&gt;性能测试对比&lt;/h3&gt;
&lt;p&gt;测试配置：Batch=4, Heads=8, d_head=128, 对不同的序列长度L进行测试：&lt;/p&gt;
&lt;p&gt;测试一共使用了3种算法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Chunk-wise算法： 使用2个Kernel的Chunk-wise算法&lt;/li&gt;
&lt;li&gt;Fused Chunk-wise算法： 使用1个Kernel的Chunk-wise算法&lt;/li&gt;
&lt;li&gt;Parallel Scan：基于Sweep Up/Down的算法&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;L&lt;/th&gt;
          &lt;th&gt;Chunk-wise&lt;/th&gt;
          &lt;th&gt;Fused Chunk-wise&lt;/th&gt;
          &lt;th&gt;Parallel Scan&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;32&lt;/td&gt;
          &lt;td&gt;0.099184&lt;/td&gt;
          &lt;td&gt;0.045680&lt;/td&gt;
          &lt;td&gt;0.013440&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;64&lt;/td&gt;
          &lt;td&gt;0.093040&lt;/td&gt;
          &lt;td&gt;0.044832&lt;/td&gt;
          &lt;td&gt;0.018016&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;128&lt;/td&gt;
          &lt;td&gt;0.113344&lt;/td&gt;
          &lt;td&gt;0.070464&lt;/td&gt;
          &lt;td&gt;0.020704&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;256&lt;/td&gt;
          &lt;td&gt;0.111024&lt;/td&gt;
          &lt;td&gt;0.112288&lt;/td&gt;
          &lt;td&gt;0.034528&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;512&lt;/td&gt;
          &lt;td&gt;0.172096&lt;/td&gt;
          &lt;td&gt;0.281088&lt;/td&gt;
          &lt;td&gt;0.064032&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1024&lt;/td&gt;
          &lt;td&gt;0.498624&lt;/td&gt;
          &lt;td&gt;0.546480&lt;/td&gt;
          &lt;td&gt;0.150272&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2048&lt;/td&gt;
          &lt;td&gt;0.985968&lt;/td&gt;
          &lt;td&gt;1.078208&lt;/td&gt;
          &lt;td&gt;0.422528&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4096&lt;/td&gt;
          &lt;td&gt;1.955248&lt;/td&gt;
          &lt;td&gt;2.132128&lt;/td&gt;
          &lt;td&gt;1.350048&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;8192&lt;/td&gt;
          &lt;td&gt;3.882000&lt;/td&gt;
          &lt;td&gt;4.229376&lt;/td&gt;
          &lt;td&gt;4.766928&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;16384&lt;/td&gt;
          &lt;td&gt;7.750400&lt;/td&gt;
          &lt;td&gt;8.427360&lt;/td&gt;
          &lt;td&gt;17.839392&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;可以看到，在序列较短时（例如 $L\le 4096$），Parallel Scan 的优势更明显；这主要来自两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;并行 scan 的关键路径是 $O(\log L)$ 轮的 up-sweep/down-sweep，短序列时轮数少；&lt;/li&gt;
&lt;li&gt;这类实现往往可以把“状态合并”写成很轻量的向量/小矩阵算子，kernel 本身更偏 latency-bound，短 $L$ 时更容易占优。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但是随着序列变长（从表中可见在 $L\approx 8192$ 附近出现拐点，$L=16384$ 时差距被明显拉开），Parallel Scan 会逐渐变慢，原因通常是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要的 sweep 轮数随 $\log L$ 增长，并且每一轮都伴随全局同步/跨 block 的数据交换，整体更偏 memory-bound；&lt;/li&gt;
&lt;li&gt;scan 的算术强度相对较低，序列越长越容易被 HBM 带宽与同步开销限制。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相比之下，Chunk-wise 的 Two-pass 结构在长序列时更“吃得满” Tensor Core：Pass 1 只在 chunk 边界更新状态、Pass 2 在 chunk 内做更大粒度的矩阵运算（更高的 arithmetic intensity），因此随 $L$ 增长时吞吐更稳定。&lt;/p&gt;
&lt;p&gt;另外，Fused Chunk-wise 虽然只需要启动一次 Kernel，但它往往需要在一个 kernel 内同时承担 Pass 1 + Pass 2 的职责：为了避免中间状态落到显存，必须把更多的中间量/状态（例如每个 head 的累计状态 $S$ 或 chunk 边界状态）尽可能保存在寄存器中。
这会带来寄存器压力上升与 occupancy 下降，极端情况下还会触发 register spill（溢出到 local memory），从而在长序列时反而不如两 Kernel 的 Chunk-wise 实现。&lt;/p&gt;
&lt;h2 id=&#34;delta-net性能测试&#34;&gt;Delta Net性能测试&lt;/h2&gt;
&lt;p&gt;这节我们以 Delta Net 为例，先给出核心的性能对比结果，然后解释为什么某些并行化策略（特别是基于Parallel scan 的方法）在实践中会失败。&lt;/p&gt;
&lt;p&gt;对于不同的序列长度L，运行的结果表现如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;L&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;Chunk-wise Parallel&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;Parallel Scan&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;128&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.178384&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.159504&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;256&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.172256&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.159840&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;512&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.185056&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.210512&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1024&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.198560&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.478272&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;2048&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.350336&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1.239392&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;4096&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;0.674592&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;3.765408&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;8192&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1.318208&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;12.916416&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;16384&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;2.600304&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;47.368912&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;性能测试对比分析&#34;&gt;性能测试对比分析&lt;/h3&gt;
&lt;p&gt;可以看到在Delta Net种，&lt;code&gt;Parallel Scan&lt;/code&gt;算法的优势明显变小，在L=512时候就被Chunk-wise的算法反超了。&lt;/p&gt;
&lt;p&gt;为了看清 Parallel Scan 需要合并的是什么，我们把 DeltaNet 的单步更新再写一遍：&lt;/p&gt;
&lt;p&gt;$$ S_t = S_{t-1}(I - \beta_t k_t k_t^{\top}) + \beta_t v_t k_t^{\top}. $$&lt;/p&gt;
&lt;p&gt;然后，记&lt;/p&gt;
&lt;p&gt;$$M_t = I - \beta_t k_t k_t^{\top} \in\mathbb{R}^{d\times d}$$&lt;/p&gt;
&lt;p&gt;$$B_t = \beta_t v_t k_t^{\top} \in\mathbb{R}^{d\times d}$$&lt;/p&gt;
&lt;p&gt;于是每一步都是对矩阵S的仿射变换：&lt;/p&gt;
&lt;p&gt;$$S_t = S_{t-1} M_t + B_t$$&lt;/p&gt;
&lt;p&gt;要把这个过程用Parallel Scan加速，我们可以继续使用前文种定义的算子 $\otimes$ ：&lt;/p&gt;
&lt;p&gt;$$(A_j,b_j)\otimes (A_i,b_i) = (A_j A_i,; A_j b_i + b_j),$$&lt;/p&gt;
&lt;p&gt;它表示先做时刻 $i$ 的变换再做时刻 $j$ 的变换。将该算子应用到序列 $(M_t,B_t)$ 后：&lt;/p&gt;
&lt;p&gt;$$S_1 = S_0 M_1 + B_1$$
$$S_2 = S_1 M_2 + B_2 = S_0 (M_1 M_2) + (B_1 M_2 + B_2)$$&lt;/p&gt;
&lt;p&gt;可以看到，在合并一段区间时需要把对应的 $M$ 矩阵相乘（产生 $M_1M_2$），同时把先前的 $B$ 按新的 $M$ 变换后累加（产生 $B_1M_2+B_2$），这是两个比较重的矩阵操作，也是Delta Net的Parallel Scan算法效率不高的根本原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;合并里必须显式算/存 $M$&lt;/strong&gt;：
对普通线性注意力（加法累积）而言，scan 合并的是固定形状的小矩阵/向量，算子很轻。
但 DeltaNet 合并的是矩阵 $M_t$ 是 $d\times d$，而且要在合并时做 $M_1M_2$等运算，复杂度要高得多。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;难以通过低秩假设优化计算&lt;/strong&gt;：
单步 $M_t = I - \beta_t k_t k_t^T$ 是 rank-1 更新（把一个已有矩阵加上或减去一个rank为1的矩阵）；但两步相乘：
$$ (I - \beta_2 k_2 k_2^T)(I - \beta_1 k_1 k_1^T)
= I - \beta_1 k_1 k_1^T - \beta_2 k_2 k_2^T + \beta_1\beta_2 k_2 (k_2^T k_1) k_1^T $$
最后的交叉项会不断出现。把一段长度为 $m$ 的连乘写成“单位阵减低秩”的形式时，其有效秩一般会随 $m$ 增长（直观上接近 $O(m)$）。
这意味着：如果 scan 想只携带低秩因子（比如 $I - W K^T$），那么在 up-sweep 的更高层节点里，$W,K$ 的行数会越来越大——&lt;strong&gt;中间态不再是常数大小&lt;/strong&gt;，从而无法像普通 scan 那样用固定 shape 的张量高效实现。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Parallel Scan 虽然只有 $O(\log L)$ 轮，但&lt;strong&gt;每一轮&lt;/strong&gt;都要对整段序列做一次“合并 + 全局读写/同步”，当合并算子本身已经是大矩阵操作时，就会表现出你表里那种随长度急剧变慢的趋势。&lt;/p&gt;
&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;
&lt;p&gt;可以看到，在Linear Attention中，虽然有着多重不同的工程实现方式，但是具体到特定的算子，还是需要根据算子的特性和计算的参数(如序列长度)来选择最合适的算法。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Linear Attention基础-理论篇</title>
        <link>https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/</link>
        <pubDate>Tue, 16 Dec 2025 22:09:11 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/ -&lt;p&gt;准备写一些关于线性注意力的文章，对相关理论和工程(Kernel)做一些梳理，这一篇是关于基础理论的。&lt;/p&gt;
&lt;h1 id=&#34;softmax-attention到线性注意力&#34;&gt;Softmax Attention到线性注意力&lt;/h1&gt;
&lt;h2 id=&#34;softmax-attention与on2复杂度&#34;&gt;Softmax Attention与O(N^2)复杂度&lt;/h2&gt;
&lt;p&gt;标准的Transformer使用的是Softmax Attention。给定查询（Query）、键（Key）、值（Value）矩阵 $Q, K, V \in \mathbb{R}^{N \times d}$，其中 $N$ 是序列长度，$d$ 是特征维度（通常 $d \ll N$）。Attention的计算公式为：&lt;/p&gt;
&lt;p&gt;$$ Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V $$&lt;/p&gt;
&lt;p&gt;让我们仔细分析一下这个计算过程的维度变化：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;计算相似度矩阵&lt;/strong&gt;：$QK^T$。
&lt;ul&gt;
&lt;li&gt;$Q$ 是 $N \times d$，$K^T$ 是 $d \times N$。&lt;/li&gt;
&lt;li&gt;相乘得到 $N \times N$ 的矩阵。这一步的计算复杂度是 $O(N^2 d)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用Softmax&lt;/strong&gt;：对每一行进行归一化，维度不变，仍为 $N \times N$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加权求和&lt;/strong&gt;：乘以 $V$。
&lt;ul&gt;
&lt;li&gt;$N \times N$ 的矩阵乘以 $N \times d$ 的矩阵 $V$。&lt;/li&gt;
&lt;li&gt;结果是 $N \times d$。这一步的计算复杂度也是 $O(N^2 d)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;瓶颈所在&lt;/strong&gt;：
由于Softmax是非线性的，我们必须先完整地计算出 $N \times N$ 的Attention Matrix。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;计算复杂度&lt;/strong&gt;：$O(N^2 d)$。随着序列长度 $N$ 的增加，计算量呈平方级增长。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;显存占用&lt;/strong&gt;：$O(N^2)$（如果不使用FlashAttention等优化）。需要存储 $N \times N$ 的矩阵。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KV Cache&lt;/strong&gt;：在推理时，为了避免重复计算，我们需要缓存所有的 $K$ 和 $V$，显存占用为 $O(Nd)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;之前大部分情况下这个问题不大， 但是随着Agent能力的发展，Context越来越长，这个 $O(N^2)$ 的复杂度就变得有些过高了。&lt;/p&gt;
&lt;h2 id=&#34;线性注意力-linear-attention&#34;&gt;线性注意力 (Linear Attention)&lt;/h2&gt;
&lt;p&gt;线性注意力的核心思想是：&lt;strong&gt;如果我们能去掉Softmax，或者将其替换为某种核函数 $\phi(\cdot)$，我们就可以利用矩阵乘法的结合律优化计算。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;假设我们将相似度函数定义为 $\text{sim}(q, k) = \phi(q)^T \phi(k)$，其中 $\phi(\cdot)$ 是一个特征映射函数（Feature Map），将输入映射到某个特征空间。那么Attention公式变为：&lt;/p&gt;
&lt;p&gt;$$ Attention(Q, K, V) = \left( \phi(Q) \phi(K)^T \right) V $$&lt;/p&gt;
&lt;p&gt;这里 $\phi(Q)$ 和 $\phi(K)$ 的维度仍然是 $N \times d$（假设特征映射不改变维度，或者映射到 $D$ 维）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基于结合律的化简&lt;/strong&gt;：
矩阵乘法满足结合律 $(AB)C = A(BC)$。我们可以改变计算顺序：&lt;/p&gt;
&lt;p&gt;$$ Attention(Q, K, V) = \phi(Q) \left( \phi(K)^T V \right) $$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(注：标准的Linear Attention公式通常包含一个分母归一化项 $\phi(Q) (\phi(K)^T \mathbf{1})$ 以保持数值尺度，此处为了简化推导过程省略了该项。)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;让我们看看现在的计算过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;先计算 $\phi(K)^T V$&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;$\phi(K)^T$ 是 $d \times N$，$V$ 是 $N \times d$。&lt;/li&gt;
&lt;li&gt;相乘得到一个 $d \times d$ 的矩阵。我们称之为 &lt;strong&gt;状态矩阵 (State Matrix)&lt;/strong&gt; 或 &lt;strong&gt;KV Memory&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;计算复杂度：$O(N d^2)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;再左乘 $\phi(Q)$&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;$\phi(Q)$ 是 $N \times d$，状态矩阵是 $d \times d$。&lt;/li&gt;
&lt;li&gt;相乘得到 $N \times d$ 的结果。&lt;/li&gt;
&lt;li&gt;计算复杂度：$O(N d^2)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;优势&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;总复杂度&lt;/strong&gt;：$O(N d^2)$。由于 $d$ 通常是一个较小的常数（如64, 128），这相对于序列长度 $N$ 是&lt;strong&gt;线性&lt;/strong&gt;的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;显存占用&lt;/strong&gt;：我们只需要维护一个 $d \times d$ 的状态矩阵，与 $N$ 无关。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;循环神经网络-rnn-view&#34;&gt;循环神经网络 (RNN View)&lt;/h2&gt;
&lt;p&gt;线性注意力不仅计算高效，而且可以写成RNN的形式，这对于自回归生成（Inference）非常友好。&lt;/p&gt;
&lt;p&gt;我们可以将上述矩阵运算写成累加的形式。对于第 $t$ 个时刻的输出 $o_t$：&lt;/p&gt;
&lt;p&gt;$$ o_t = \sum_{i=1}^t \text{sim}(q_t, k_i) v_i = \sum_{i=1}^t (\phi(q_t)^T \phi(k_i)) v_i $$&lt;/p&gt;
&lt;p&gt;提取出与 $i$ 无关的项 $\phi(q_t)$：&lt;/p&gt;
&lt;p&gt;$$ o_t = \phi(q_t)^T \sum_{i=1}^t \phi(k_i) v_i^T $$&lt;/p&gt;
&lt;p&gt;我们可以定义状态 $S_t = \sum_{i=1}^t \phi(k_i) v_i^T$。这个状态 $S_t$ 是一个 $d \times d$ 的矩阵。
显然，它可以递归地计算：&lt;/p&gt;
&lt;p&gt;$$ S_t = S_{t-1} + \phi(k_t) \phi(v_t)^T $$
$$ o_t = \phi(q_t)^T S_t $$&lt;/p&gt;
&lt;p&gt;这就是线性注意力的RNN形式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;更新 (Update)&lt;/strong&gt;：用当前的 $k_t$ 和 $v_t$ 更新状态 $S$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查询 (Query)&lt;/strong&gt;：用当前的 $q_t$ 从状态 $S$ 中提取信息。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这解释了为什么线性注意力在推理时极其高效：它不需要像Softmax Attention那样重新访问所有的历史KV Cache，只需要维护一个固定大小的状态 $S$。&lt;/p&gt;
&lt;h1 id=&#34;线性注意力的设计思路与框架&#34;&gt;线性注意力的设计思路与框架&lt;/h1&gt;
&lt;p&gt;经典的线性注意力虽然看上去可行，但是效果并不好，需要找到一些入手点，在整个公式的这个基础上进行进一步的优化。&lt;/p&gt;
&lt;p&gt;常见的线性注意力有多种不同的叙事方式，这里用比较有名的Mamba(SSM)和DeltaNet举例说明：&lt;/p&gt;
&lt;h2 id=&#34;状态空间模型-ssm&#34;&gt;状态空间模型 (SSM)&lt;/h2&gt;
&lt;p&gt;Mamba系列的架构基于状态空间模型（State Space Models, SSM），这个模型起源于控制理论，用来描述连续空间上的状态变化。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20251216/mamba.png&#34; class=&#34;center&#34; /&gt;
&lt;h3 id=&#34;mamba-ssm-architecture&#34;&gt;Mamba SSM Architecture&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;连续时序空间&lt;/strong&gt;：SSM通常定义在连续时间上，描述了一个系统如何根据输入 $x(t)$ 更新其内部状态 $h(t)$ 并产生输出 $y(t)$：
$$ h&amp;rsquo;(t) = A h(t) + B x(t) $$
$$ y(t) = C h(t) $$
其中 $A$ 是状态转移矩阵，$B$ 是输入投影，$C$ 是输出投影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;到离散状态空间&lt;/strong&gt;：为了在计算机上处理离散的序列数据（如文本），我们需要将连续系统离散化。常用的方法是&lt;strong&gt;零阶保持 (ZOH)&lt;/strong&gt;。
基于该方法离散化后，我们去掉了导数，公式变为递归形式：
$$ h_t = \bar{A}&lt;em&gt;t h&lt;/em&gt;{t-1} + \bar{B}_t x_t $$
$$ y_t = C h_t $$&lt;/p&gt;
&lt;p&gt;其中 $\bar{A}_t$ 和 $\bar{B}_t$ 是 $A, B$ 和采样步长 $\Delta_t$ 的函数。例如，在ZOH下，$\bar{A}_t = \exp(\Delta_t A)$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;与Linear Attention的联系&lt;/strong&gt;：
仔细观察离散化后的SSM公式，它与Linear Attention的RNN形式惊人地相似：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Attention: $S_t = I \cdot S_{t-1} + \phi(k_t) \phi(v_t)^T$&lt;/li&gt;
&lt;li&gt;SSM: $h_t = \bar{A} h_{t-1} + \bar{B} x_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果我们将Linear Attention中的状态衰减看作 $\bar{A}$，将键值对的更新看作输入项，两者在数学形式上是高度统一的。
&lt;strong&gt;Mamba&lt;/strong&gt; 的创新之处在于引入了&lt;strong&gt;选择性机制（Selection Mechanism）&lt;/strong&gt;，让参数 $B, C, \Delta$ 成为输入 $x_t$ 的函数（即 $B(x_t), C(x_t), \Delta(x_t)$）。这使得模型能够根据当前的内容动态地决定“记住什么”和“忽略什么”，从而解决了传统LTI（线性时不变）系统无法进行上下文推理的问题。&lt;/p&gt;
&lt;h3 id=&#34;半可分离矩阵&#34;&gt;半可分离矩阵&lt;/h3&gt;
&lt;p&gt;接下来我们对Mamba架构进行进一步推导：&lt;/p&gt;
&lt;p&gt;先不考虑随着时间变化的$\Delta(x_t)$，对于整个序列 $x = [x_1, \dots, x_L]$ 和输出 $y = [y_1, \dots, y_L]$，把Mamba的公式展开，我们会发现这个变换可以写成 $y = M x$，其中 $M$ 是一个下三角矩阵。&lt;/p&gt;
&lt;p&gt;可以得到一个&lt;strong&gt;半可分离矩阵 (Semiseparable Matrix)&lt;/strong&gt; 的形式。（推导过程略）&lt;/p&gt;
&lt;p&gt;$$
M = \begin{pmatrix}
C_0 B_0 &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 \\
C_1 A_1 B_0 &amp;amp; C_1 B_1 &amp;amp; \dots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
C_L A_L \dots A_1 B_0 &amp;amp; \dots &amp;amp; \dots &amp;amp; C_L B_L
\end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;矩阵 $M$ 的第 $(i, j)$ 个元素（$i \ge j$）表示第 $j$ 个输入对第 $i$ 个输出的影响：
$$ M_{ij} = C_i (A_i A_{i-1} \dots A_{j+1}) B_j $$&lt;/p&gt;
&lt;p&gt;在Mamba中，为了计算效率，矩阵 $A$ 通常被设计为&lt;strong&gt;对角矩阵&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当 $A_t$ 是对角矩阵时，矩阵乘法满足交换律。我们可以把累乘项 $A_{i} \dots A_{j+1}$ 分解。
定义 $L_t = \prod_{k=1}^t A_k$ 为从时刻 1 到 $t$ 的累积衰减。那么对于 $i &amp;gt; j$，有：
$$ A_{i} \dots A_{j+1} = L_i L_j^{-1} $$&lt;/p&gt;
&lt;p&gt;代入 $M_{ij}$ 的公式：
$$ M_{ij} = C_i (L_i L_j^{-1}) B_j = (C_i L_i) (L_j^{-1} B_j) $$&lt;/p&gt;
&lt;p&gt;如果我们定义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Q_i = C_i L_i$&lt;/li&gt;
&lt;li&gt;$K_j = (L_j^{-1} B_j)^T$&lt;/li&gt;
&lt;li&gt;$V_j = x_j$ (这里输入 $x$ 扮演 Value 的角色)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么输出 $y_i = \sum_{j=0}^i M_{ij} x_j$ 就可以写成：
$$ y_i = \sum_{j=0}^i (Q_i K_j^T) V_j $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：该推导对于随着时间变换的 $\Delta(x_t)$ 同样有效： $\Delta(x_t)$ 作为采样步长是一个标量，而对角矩阵的exp仍然是对角矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这正是前文线性注意力的形式！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过这种变换，Mamba2揭示了 &lt;strong&gt;SSM (State Space Models)&lt;/strong&gt; 和 &lt;strong&gt;Attention&lt;/strong&gt; 之间的对偶性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SSM视角&lt;/strong&gt;：通过递归公式 $h_t = A_t h_{t-1} + B_t x_t$ 进行 $O(N)$ 的推理。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attention视角&lt;/strong&gt;：通过矩阵乘法 $Y = \text{CausalMask}(QK^T)V$ 进行并行训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mamba2-ssd-architecture&#34;&gt;Mamba2 SSD Architecture&lt;/h3&gt;
&lt;p&gt;Mamba2在SSM的基础上提出了新的SSD（State Space Duality）架构：
从公式外观上看，Mamba2延续了Mamba的基本形式，但它通过引入更严格的结构约束，对底层架构进行了重构。Mamba2的核心设计目标是最大化硬件效率，特别是针对GPU Tensor Cores的利用率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;与Mamba1的关键区别&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;Mamba2的架构被称为SSD，可以认为是SSM的一种特例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;矩阵 $A$ 的约束&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mamba1&lt;/strong&gt;: 通常每个 $A_t$ 都是&lt;strong&gt;对角矩阵&lt;/strong&gt;，每个特征维度（Channel）拥有独立的衰减参数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mamba2&lt;/strong&gt;: 将每个 $A_t$ 进一步限制为&lt;strong&gt;标量&lt;/strong&gt;（Scalar），即在每个Head内部，所有特征维度共享同一个衰减参数 ($A_t = a_t I$)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;计算范式&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mamba1&lt;/strong&gt;: 主要依赖&lt;strong&gt;并行扫描&lt;/strong&gt; 算法。这是一种内存密集型操作，难以利用GPU上专为矩阵乘法设计的Tensor Cores。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mamba2&lt;/strong&gt;: 得益于 $A$ 的标量化和SSD对偶性，Mamba2可以将计算重写为&lt;strong&gt;分块矩阵乘法&lt;/strong&gt;。这使得计算可以运行在Tensor Cores上，极大地提高了训练吞吐量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，Mamba2是为了&lt;strong&gt;算法-硬件协同设计&lt;/strong&gt;优化硬件利用效率，对Mamba1表达能力的“简化”（从对角矩阵简化为标量）。虽然Mamba2牺牲了一些自由度，但是换取的工程效率提升可以用来扩展状态的维度d，因此在实践中性能反而会有所提高，也使得模型能够扩展到更大的参数规模和更长的序列长度。&lt;/p&gt;
&lt;h3 id=&#34;mamba3进一步优化&#34;&gt;Mamba3进一步优化&lt;/h3&gt;
&lt;p&gt;Mamba3是对SSM的进一步优化，旨在解决Mamba2为了硬件效率（标量化A）而牺牲的部分表达能力。它主要引入了以下几个关键改进（本文只阐述思路，细节可以参考论文）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;复数状态空间&lt;/strong&gt;：
Mamba2中的状态转移矩阵 $A$ 是实数标量，只能表达&lt;strong&gt;指数衰减&lt;/strong&gt;。Mamba3将 $A$ 扩展为复数，引入了&lt;strong&gt;旋转&lt;/strong&gt;机制。这使得状态不仅能衰减，还能像RoPE一样在复平面上旋转，从而更好地捕捉周期性模式和长距离依赖。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;梯形法则离散化&lt;/strong&gt;：
之前的SSM的离散化使用的是最简单的&lt;strong&gt;零阶保持 (ZOH)&lt;/strong&gt; 假设（假设信号在采样间隔内不变）。而Mamba3改用了精度更高的&lt;strong&gt;梯形法则&lt;/strong&gt;（假设信号线性变化），能够更准确地追踪快速变化的信号。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-Input Multi-Output&lt;/strong&gt;：
Mamba2在SSM层面上是&lt;strong&gt;SISO&lt;/strong&gt;的，即各个通道间独立演化。Mamba3通过把状态更新中的外积换成矩阵乘来允许不同通道的信息交互实现&lt;strong&gt;MIMO&lt;/strong&gt;，提高对Tensor Core的利用效率。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;key的正交假设与delta-rule&#34;&gt;Key的正交假设与Delta Rule&lt;/h2&gt;
&lt;p&gt;Delta Rule从另一个角度对线性注意力进行了推演。&lt;/p&gt;
&lt;p&gt;在标准的线性Attention更新规则 $S_t = S_{t-1} + k_t v_t^T$ 中，我们简单地将新的键值对叠加到状态矩阵中。
这种更新方式类似于神经科学中的 &lt;strong&gt;Hebbian Learning&lt;/strong&gt;（赫布学习规则）：&amp;ldquo;Cells that fire together, wire together&amp;rdquo;。我们简单地增加 $k_t$ 和 $v_t$ 之间的关联强度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;潜在问题&lt;/strong&gt;：
这隐含了一个假设：Key之间是正交的。
如果 $k_t$ 与之前的 $k_{t-1}$ 高度相似（不正交），那么 $S_{t-1} k_t$（即模型对当前Key的旧预测）将不为零。简单的叠加会导致信息的冗余和干扰，使得检索时出现噪声。这里有两种常见的信息冲突：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;单个Key的多义性&lt;/strong&gt;：
在长序列中，同一个Key（或者语义上非常相似的Key）可能会出现多次，但对应不同的Value。例如，单词 &amp;ldquo;Apple&amp;rdquo; 可能在一段话中指代水果，在另一段话中指代公司。
在简单的线性叠加 $S_t = \sum k_i v_i^T$ 中，这些不同的Value会被直接相加。当我们用 &amp;ldquo;Apple&amp;rdquo; 去查询时，得到的是所有历史Value的混合（平均），而不是特定上下文下的那个Value。这导致了语义的模糊。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;不同Key的相关性&lt;/strong&gt;：
理想情况下，我们希望Key之间是正交的，这样更新一个Key不会影响其他Key。但实际上，Key向量通常存在复杂的协方差结构（Covariance）。
如果 $k_t$ 与之前的 $k_{j}$ 高度相关（非正交），那么向状态 $S$ 中添加 $k_t v_t^T$ 不仅更新了 $k_t$ 的方向，也会改变 $k_{j}$ 方向上的投影。这意味着，新信息的写入会干扰旧信息的读取，产生“串扰” (Crosstalk)。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;transformer的处理方式&#34;&gt;Transformer的处理方式&lt;/h3&gt;
&lt;p&gt;标准的Softmax Attention通过&lt;strong&gt;加权平均&lt;/strong&gt;来处理这种冲突。它显式地计算 Query 与所有历史 Key 的相似度，然后对 Value 进行加权。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于多义性，Softmax可以通过上下文（Query）和位置编码来区分同一个Key的不同实例，给予不同的权重。&lt;/li&gt;
&lt;li&gt;对于相关性，Softmax是非线性的，它不需要将历史压缩成一个线性算子，因此不存在“写入新Key破坏旧Key结构”的问题（它只是增加了一个新的参考点）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而Linear Attention试图将所有历史压缩进一个固定大小的线性算子 $S$，就必须面对这种压缩带来的冲突。&lt;/p&gt;
&lt;h3 id=&#34;delta-rule-deltanet&#34;&gt;Delta Rule (DeltaNet)&lt;/h3&gt;
&lt;p&gt;为了解决这个问题，&lt;strong&gt;DeltaNet&lt;/strong&gt; 等工作引入了 &lt;strong&gt;Delta Rule&lt;/strong&gt;（也称为Widrow-Hoff规则或LMS算法）。
更新公式变为：
$$ S_t = S_{t-1} + \beta_t (v_t - S_{t-1} k_t) k_t^T $$&lt;/p&gt;
&lt;p&gt;让我们分解这个公式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;预测&lt;/strong&gt;：$S_{t-1} k_t$ 是模型根据旧状态对当前Key $k_t$ 的预测值（Retrieve Value）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;误差&lt;/strong&gt;：$e_t = v_t - S_{t-1} k_t$ 是真实值 $v_t$ 与预测值之间的差异。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;修正&lt;/strong&gt;：我们只将这个“误差”部分更新到状态矩阵中。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在下一个章节中，我们会提到：这本质上是在进行一步&lt;strong&gt;在线梯度下降&lt;/strong&gt;，试图最小化当前输入下的重构误差。这使得模型在有限的容量（$d \times d$）下，能够更精确地存储和检索信息，避免了重复信息的累积。&lt;/p&gt;
&lt;h4 id=&#34;类似的加权平均&#34;&gt;类似的加权平均&lt;/h4&gt;
&lt;p&gt;如果我们观察状态 $S$ 在 $k_t$ 方向上的投影（假设 $k_t$ 是单位向量），更新公式可以近似理解为：
$$ \text{NewValue} \approx (1 - \beta_t) \cdot \text{OldValue} + \beta_t \cdot v_t $$
这实际上是一个&lt;strong&gt;指数移动平均 (Exponential Moving Average, EMA)&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Softmax Attention&lt;/strong&gt;：是&lt;strong&gt;空间上的加权平均&lt;/strong&gt;。在推理时（基于新的Query），它同时看到所有历史，并根据相似度分配权重。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delta Rule&lt;/strong&gt;：是&lt;strong&gt;时间上的加权平均&lt;/strong&gt;。在更新时，它通过 $\beta_t$（类似于学习率）动态决定是“保持旧记忆”还是“重写为新记忆”。
&lt;ul&gt;
&lt;li&gt;当同一个Key出现多次（多义性）时，Delta Rule 允许模型根据 $\beta_t$ 逐渐遗忘旧的含义，聚焦于最新的含义，从而在效果上实现了对不同Value的“选择”或“加权”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;状态的遗忘机制&#34;&gt;状态的遗忘机制&lt;/h2&gt;
&lt;p&gt;标准的线性Attention（如Linear Transformer）通常对所有历史一视同仁，即 $S_t = \sum k_i v_i^T$，通过位置编码来解决不同位置的重要性问题。而在线性注意力中，通常会使用衰减的机制达成类似的效果：在语言建模中，&lt;strong&gt;局部性&lt;/strong&gt; 通常很重要：最近的上下文往往比遥远的上下文更相关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RetNett&lt;/strong&gt; 引入了类似下面的指数衰减机制：
$$ S_t = \gamma S_{t-1} + k_t v_t^T $$
其中 $\gamma \in (0, 1)$ 是衰减因子。&lt;/p&gt;
&lt;p&gt;类似的，Gated DeltaNet在DeltaNet的基础上引入衰减机制，而Kimi Delta Attention使用对角矩阵 $Diag(r_t)$ 进行更细粒度的衰减，增强了模型了表达能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;直观理解&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;相对位置编码&lt;/strong&gt;：这种机制巧妙地编码了相对位置信息。对于第 $n$ 个位置的查询 $q_n$ 和第 $m$ 个位置的键 $k_m$ ($m &amp;lt; n$)，它们之间的相互作用会被乘以 $\gamma^{n-m}$。这自然地表达了“距离越远，影响越弱”的归纳偏置。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;软滑动窗口&lt;/strong&gt;：这相当于给历史信息加了一个指数衰减的权重窗口。越久远的信息，权重越小（$\gamma^k$），逐渐被“遗忘”。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种遗忘机制不仅提高了模型对局部信息的关注度，还增强了数值稳定性，防止状态 $S_t$ 的值无限增长。&lt;/p&gt;
&lt;p&gt;此外，我们还可以通过对&lt;strong&gt;不同的head&lt;/strong&gt;引入&lt;strong&gt;不同的衰减系数&lt;/strong&gt;来捕捉不同的时间尺度敏感性。&lt;/p&gt;
&lt;h1 id=&#34;test-time-regression-ttr-框架&#34;&gt;Test Time Regression (TTR) 框架&lt;/h1&gt;
&lt;p&gt;关于TTR视角有一篇很好的论文：《Test-time regression: a unifying framework for designing sequence models with  associative memory》，该视角将Attention机制视为在测试时（Inference time）进行的回归任务。这个框架帮助我们理解为什么Linear Attention在某些任务上表现不如Softmax Attention。&lt;/p&gt;
&lt;p&gt;我们将Attention看作是学习一个函数 $f: \mathbb{R}^d \to \mathbb{R}^d$，使得 $f(k_i) \approx v_i$。&lt;/p&gt;
&lt;h2 id=&#34;nonparametric-regression-full-attention&#34;&gt;Nonparametric Regression (Full Attention)&lt;/h2&gt;
&lt;p&gt;Full Attention对应于&lt;strong&gt;非参数回归（Nonparametric Regression）&lt;/strong&gt;，或者称为&lt;strong&gt;对偶形式 (Dual Form)&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机制&lt;/strong&gt;：模型显式地存储了所有的历史训练样本 $(K, V)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测&lt;/strong&gt;：当一个新的查询 $q$ 到来时，它通过计算 $q$ 与所有样本 $k_i$ 的相似度（核函数 $K(q, k_i)$）来加权平均 $v_i$。
$$ o = \sum_i \frac{K(q, k_i)}{\sum_j K(q, k_j)} v_i $$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特点&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;背诵&lt;/strong&gt;：直接利用原始数据不进行任何压缩。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;容量&lt;/strong&gt;：随着序列长度 $N$ 线性增长。理论上，只要 $N$ 足够大，它可以完美回忆起任何见过的细节。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代价&lt;/strong&gt;：推理成本高昂 ($O(N)$)，因为每次都要遍历所有样本。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;parametric-regression-linear-attention&#34;&gt;Parametric Regression (Linear Attention)&lt;/h2&gt;
&lt;p&gt;Linear Attention对应于&lt;strong&gt;参数回归（Parametric Regression）&lt;/strong&gt;，或者称为&lt;strong&gt;原始形式 (Primal Form)&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机制&lt;/strong&gt;：我们不再存储所有的原始数据，而是维护一个固定大小的参数矩阵 $S$（即RNN状态）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习&lt;/strong&gt;：这个过程可以看作是使用在线梯度下降来更新模型参数 $S$，使其能够拟合历史数据 $(k, v)$ 的映射关系。
$$ S_t = S_{t-1} + k_t v_t^T $$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测&lt;/strong&gt;：当查询 $q$ 到来时，我们直接使用学习到的模型进行预测：
$$ o = S q $$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特点&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;拟合&lt;/strong&gt;：它试图将所有历史信息压缩进一个固定大小的矩阵 $S$ 中，学习一个函数来通过key来提取value。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;容量&lt;/strong&gt;：受限于状态 $S$ 的大小（$d \times d$）。如果历史信息过于复杂或庞大，状态矩阵可能会饱和，导致旧信息的灾难性遗忘。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代价&lt;/strong&gt;：推理成本固定 ($O(1)$)，极其高效。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;梯度下降视角的详细推导&#34;&gt;梯度下降视角的详细推导&lt;/h3&gt;
&lt;p&gt;为了更深入地理解这一点，我们可以显式地写出优化目标。
假设我们的目标是学习一个矩阵 $S$，使得对于所有的历史时刻 $i$，都有 $S k_i \approx v_i$。
我们可以定义在时刻 $t$ 的瞬时损失函数为最小二乘误差：
$$ L_t(S) = \frac{1}{2} | S k_t - v_t |^2 $$&lt;/p&gt;
&lt;p&gt;我们使用&lt;strong&gt;随机梯度下降 (SGD)&lt;/strong&gt; 来更新 $S$：
$$ S_t = S_{t-1} - \eta \nabla_S L_t(S_{t-1}) $$&lt;/p&gt;
&lt;p&gt;计算梯度：
$$ \nabla_S L_t(S) = (S k_t - v_t) k_t^T $$&lt;/p&gt;
&lt;p&gt;代入更新公式（设学习率 $\eta$ 为步长）：
$$ S_t = S_{t-1} + \eta (v_t - S_{t-1} k_t) k_t^T $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这就是前文中Delta Rule的形式。&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;标准Linear Attention的特例&lt;/strong&gt;：
如果我们假设 $S_{t-1}$ 初始化为0，或者更强地假设 &lt;strong&gt;Key之间是正交的&lt;/strong&gt;（即 $S_{t-1} k_t \approx 0$，旧状态对当前Key没有响应），那么公式简化为：
$$ S_t = S_{t-1} + \eta v_t k_t^T $$
这正是标准的Linear Attention更新规则（通常 $\eta=1$）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标准Linear Attention&lt;/strong&gt; 对应于假设了Key正交性的“懒惰”更新。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeltaNet&lt;/strong&gt; 对应于完整的梯度下降更新，它显式地计算并消除了预测误差（残差）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20251216/ttt-framework.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;&lt;strong&gt;RWKV-7 与 TTT (Test-Time Training)&lt;/strong&gt;：
最新的 &lt;strong&gt;RWKV-7&lt;/strong&gt; 和 &lt;strong&gt;TTT-Linear&lt;/strong&gt; 等架构进一步深化了这个视角。它们将RNN的推理过程明确建模为&lt;strong&gt;测试时的训练过程&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TTT框架&lt;/strong&gt;认为，Hidden State本质上是一个小模型的参数，而处理序列的过程就是在这个小模型上进行梯度下降。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RWKV-7&lt;/strong&gt; 引入了更加动态的状态演化机制，其状态更新不再是简单的线性累加，而是包含了更复杂的、数据依赖的衰减和更新项。这可以被理解为一种广义的、带有自适应学习率的在线优化过程，使得模型在处理长上下文时，能够更智能地决定哪些信息该“学进”状态里，哪些该忽略。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实际上，从这个视角出发，使用不同的优化算法如带动量的SGD、L2正则化等策略，可以推导出其他一些linear attention的算法架构。&lt;/p&gt;
&lt;p&gt;最后，这个视角其实也解释了Linear Attention在处理“大海捞针”任务时的劣势。因为压缩必然带来有损，当需要精确检索某个特定的历史细节时，参数化的Linear Attention可能无法从压缩的状态中完美还原，而非参数化的Softmax Attention则可以直接查阅原始记录。&lt;/p&gt;
&lt;h1 id=&#34;一些trick与优化&#34;&gt;一些Trick与优化&lt;/h1&gt;
&lt;p&gt;最后聊一聊为了弥补Linear Attention相对于Softmax Attention的不足（如缺乏局部关注能力、数值稳定性等），研究者们引入的一些Trick或优化思路。&lt;/p&gt;
&lt;h2 id=&#34;因果卷积与token-shift&#34;&gt;因果卷积与Token-Shift&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：
Softmax Attention由于指数函数的存在，天然倾向于关注局部（最近的邻居通常相似度较高）。而Linear Attention的关注分布通常比较平滑，容易忽略局部的细节。
此外，RNN形式的状态 $S_t$ 只能捕捉到 $t$ 时刻之前的信息，对于“当前时刻”的局部特征捕捉能力较弱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：
像&lt;strong&gt;Mamba&lt;/strong&gt;、&lt;strong&gt;RWKV&lt;/strong&gt;和&lt;strong&gt;RetNet&lt;/strong&gt;等模型通常会在进入状态空间之前，先对输入进行一个短窗口的&lt;strong&gt;因果卷积（Causal Conv1d）&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RWKV-4 (Token Shift)&lt;/strong&gt;: RWKV-4率先引入了极其轻量级的 &lt;strong&gt;Token Shift&lt;/strong&gt; 机制（也称为Time Mixing的一部分）。它简单地将当前时刻的输入 $x_t$ 与上一时刻的输入 $x_{t-1}$ 进行线性插值：$x&amp;rsquo;&lt;em&gt;t = \mu x_t + (1-\mu) x&lt;/em&gt;{t-1}$。这可以看作是核大小为2的因果卷积，极低成本地实现了相邻Token的信息交互。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mamba&lt;/strong&gt;: 使用了显式的1D卷积层（通常kernel size=4），在进入SSM状态方程之前混合局部信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ x&amp;rsquo;&lt;em&gt;t = \text{Conv1d}(x&lt;/em&gt;{t-k:t}) $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;增强局部性&lt;/strong&gt;：这相当于显式地让模型“看到”最近的几个Token。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;平移不变性&lt;/strong&gt;：卷积操作具有平移不变性，有助于捕捉局部的n-gram特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;弥补RNN缺陷&lt;/strong&gt;：它充当了一个“预处理”步骤，捕捉高频的局部特征，而让RNN状态专注于捕捉低频的全局依赖。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;dplr-diagonal-plus-low-rank-矩阵&#34;&gt;DPLR (Diagonal-Plus-Low-Rank) 矩阵&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：
在SSM中，我们需要处理长序列。如果状态转移矩阵 $A$ 初始化不当，梯度可能会在反向传播中消失或爆炸。此外，计算 $A$ 的幂次或卷积核需要高效的算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：
HiPPO理论指出，特定的矩阵结构可以最优地记忆历史。S4模型提出将 $A$ 参数化为&lt;strong&gt;对角矩阵加上低秩矩阵&lt;/strong&gt;(DPLR)的形式：
$$ A = \Lambda - pp^* $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;计算效率&lt;/strong&gt;：这种结构减少了参数量，也允许我们利用Woodbury矩阵恒等式和Cauchy Kernel等算法，大幅降低矩阵逆运算和卷积的计算复杂度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数值稳定性&lt;/strong&gt;：它使得矩阵 $A$ 的特征值分布在左半复平面，保证了系统的稳定性（不会发散）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;虽然在Mamba中，为了硬件效率，作者简化为了纯对角矩阵，但DPLR在理论推导和早期SSM模型中起到了至关重要的作用。&lt;/p&gt;
&lt;h2 id=&#34;位置编码&#34;&gt;位置编码&lt;/h2&gt;
&lt;p&gt;现在的Linear Attention都是使用NoPE的策略，但是这不意味着Linear Attention就一定不可以使用位置编码：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RetNet&lt;/strong&gt; 提出了一种方法，将位置编码融入到状态的衰减中。通过给 $S_t$ 乘以一个复数衰减因子 $e^{i\theta}$，实现了类似RoPE的效果，同时保持了递归形式的高效性。
$$ S_t = \gamma e^{i\theta} S_{t-1} + k_t v_t^T $$
这种方法被称为 &lt;strong&gt;xPos&lt;/strong&gt; 的变体或其在RNN中的自然延伸。它使得模型能够根据Token之间的相对距离动态调整关注强度。&lt;/p&gt;
&lt;h2 id=&#34;rosa&#34;&gt;ROSA&lt;/h2&gt;
&lt;p&gt;最后，ROSA是RWKV-8提出的策略，其目的是通过外挂历史输入数据库的方式帮助模型检索历史信息。
由于RWKV-8的论文尚未发表，目前只能通过一些零散的信息和github上的代码（话说还有最近的RWKV Devday活动）来对这个方案给出一些描述。&lt;/p&gt;
&lt;p&gt;不同于传统 RNN 仅依赖隐状态压缩历史信息，ROSA 在推理过程中在线构建后缀自动机。它不仅作用于输入 Token，更通过二值化机制在每个隐藏层建立特征索引——相当于每一层都维护了一套动态的“特征码表”。&lt;/p&gt;
&lt;p&gt;在推理时，ROSA 能够利用自动机特性，瞬间检索出当前上下文在历史中最长的匹配片段，并将当时的后续状态直接用于下一个token的预测。&lt;/p&gt;
&lt;p&gt;目前看起来，ROSA是一种对历史信息进行记忆的新的思路，跳出了TTT的框架，也许有很大的潜力，等论文和Scale上去的实验结果出来后非常值得深入研究一下。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>VLM与推理能力的进化</title>
        <link>https://sword865.github.io/posts/2025/2025-09-15-vlm%E4%B8%8E%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E8%BF%9B%E5%8C%96/</link>
        <pubDate>Mon, 15 Sep 2025 23:43:26 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-09-15-vlm%E4%B8%8E%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E8%BF%9B%E5%8C%96/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-09-15-vlm%E4%B8%8E%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E8%BF%9B%E5%8C%96/ -&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;
&lt;p&gt;从年初 R1 火起来以后，看到了很多关于智能本质的讨论，目前大部分讨论似乎都认为：&lt;strong&gt;语言才是智能的基础，是通往 AGI 的路径&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如张小珺[1-3]今年采访了很多大佬，就有两个这样的例子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;杨植麟[1]：认为在现有范式下，多模态能力往往难以提升模型的“智商”，甚至可能损伤模型已有的语言智能。他在接受采访时提到：“如果你想给模型加多模态能力，你需要确保这个多模态能力不要损伤它的‘脑子’。多模态能做到不损伤已经很好了。”他指出，在多模态模式下希望模型和纯文本模式共用一个“大脑”——也即多模态部分应尽量借用文本模型已有的智力，而不是开启另一套全新参数，否则可能丢掉原来文本部分学到的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;姚顺雨[2]也强调语言在通向通用智能方面更具潜力。他起初从事计算机视觉研究，但后来直觉告诉他语言才是更核心、更有潜力的方向，因此读博后转向了语言模型的研究。他指出，语言是人类为了实现认知泛化而发明的最重要工具，具有生成和推理的闭环特性，这使其成为构建通用智能系统的关键媒介。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不过还是有一些其他的观点，认为多模态能力的潜力可能被训练范式所限制，而非多模态本身毫无助益。例如阶跃星辰的张祥雨[3]就提到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;图文数据效果不好的原因是噪声数据：如果简单用图文混合数据进行训练，但不解决思维链（CoT）推理或任务复杂度的问题，模型的学习可能是有害的。在缺乏正确推理指导的情况下，模型每一步可能得不到有效信息，产生错乱的梯度，训练效果要么毫无提升，要么甚至更糟。这与他在一个万亿参数多模态模型项目中的发现一致：模型规模增大后，其数学和逻辑推理能力不仅没有提升，反而在达到平台期后开始下降。原因在于模型倾向于&lt;strong&gt;跳步直接给出答案&lt;/strong&gt;而非踏实推演，从而累积误差。简单扩大模型或混合数据并不能自然地融合视觉与文本能力，背后缺少关键环节。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;图文推理的能力同样需要预训练的激活：例如，OpenAI 的 O3 模型在推理过程中能够将图像直接融入思维链，通过动态操作图像（旋转、裁剪等）来辅助解题，在视觉推理基准 V⋆Bench 上取得了 95.7% 的高准确率，刷新了多模态推理上限。令人意外的是，O3 所采用的一些方法（如对原图进行局部放大裁剪）看似原始，却在许多问题上效果很好。他认为：这可能是因为在海量预训练语料中，存在大量图像附带局部放大及文字解释的模式。例如在电子维修论坛中，经常有人上传一张设备照片提问“哪里出了问题”，回答者会圈出图片局部并放大，说明某个电容烧了等。模型在预训练中已经隐式学习了这种“先整体看图 → 再局部看图解释”的模式。因此，像 O3 那样在推理时按需裁剪图像，严格遵循了预训练语料中的分布模式，反而取得了出色效果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;针对这个问题，想写一些自己对这个领域的了解和看法。&lt;/p&gt;
&lt;h1 id=&#34;vision-lm-的推理能力研究&#34;&gt;Vision LM 的推理能力研究&lt;/h1&gt;
&lt;p&gt;在开始讨论前，我先对智商这个事情做一个定义：通常我们可以把模型的能力分为理解、推理、生成三种，我们认为其中推理能力是智商的表现。&lt;/p&gt;
&lt;h2 id=&#34;vision-cot&#34;&gt;Vision CoT&lt;/h2&gt;
&lt;p&gt;链式思维（CoT）是大模型提升复杂推理最早被证明有效的范式之一；其视觉扩展（Vision CoT / Visual CoT）在 2023 年起就开始出现。早期的工作以语言+图像作为输出，但是以CoT只会以文本给出，这个时候图像不参与推理的过程，图像带来的推理能力自然就无从谈起。&lt;/p&gt;
&lt;p&gt;后来O3的兴起带火了一个新的方向：&lt;code&gt;thinking with images&lt;/code&gt;，其核心思想：不要把“看图”当成一次性输入，而是让图像参与到逐步推理循环中，成为中间状态的生成与消费对象。也就是说图像开始参与了推理的过程，因此我们就从这里作为了解Vision LM推理能力的起点。&lt;/p&gt;
&lt;p&gt;（下图是thinking with images的例子）
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250915/thinking_with_images.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;openai-o1--o3-系列&#34;&gt;OpenAI O1 / O3 系列&lt;/h3&gt;
&lt;p&gt;OpenAI 在 GPT‑4 之后推出 o1 系列[4]，算是第一次验证了Test Time Scaling和推理能力的关系。更进一步的 O3 模型在 demo 中展示了新的模式：除了“会看图”理解，还会“用图思考”。推理过程中模型可以：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主动定位相关区域（放大公式、框选局部结构、旋转视角）；&lt;/li&gt;
&lt;li&gt;将局部视觉观察嵌入后续思维链进行推理；
O3 在视觉推理基准 V⋆Bench 上达到 95.7% 的成绩，也算是验证“显式把视觉纳入思维链”是可以提升推理能力的研究方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deepeyes&#34;&gt;DeepEyes&lt;/h3&gt;
&lt;p&gt;DeepEyes[6] 应该是第一个开源再现“thinking with images”能力的项目。它不依赖额外人工 SFT，而是直接用 RL 激活模型的视觉逐步聚焦行为。模型可以按照：生成初始思维链 → 判断是否缺细节 → 自主调用“放大 / 裁剪”工具 → 将裁剪区域重新编码 → 继续推演。循环执行形成类似 O3 的效果，并且得到了验证：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;7B 规模在细粒度视觉推理任务上击败更大（32B）模型。&lt;/li&gt;
&lt;li&gt;在与图片相关的数据集上数学与抽象推理同步提升，显示视觉思维链对语言主干能力存在正迁移。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pixel-reasoner&#34;&gt;Pixel-Reasoner&lt;/h3&gt;
&lt;p&gt;类似的研究还有 Pixel-Reasoner[7]，研究把视觉操作本身（ZoomIn / 选帧 / 划区域等）显式当作“推理 token”，把原本文本空间的推理延伸到像素空间。训练分两阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模板化合成数据指令微调 —— 教会模型视觉操作语法与调用格式；&lt;/li&gt;
&lt;li&gt;好奇心驱动 RL —— 通过探索奖励克服“只靠语言偷懒”惯性，促使其在必要时调用视觉函数。
结果：7B 模型在 V⋆Bench (84%)、TallyQA-Complex (74%)、InfographicsVQA (84%) 等指标上达到 / 逼近开源最优水平，凸显“显式视觉操作”对细节敏感推理的价值。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;小结&#34;&gt;小结&lt;/h3&gt;
&lt;p&gt;从这些 Vision CoT 的研究我们看到，把视觉嵌入到思维链中，似乎是可以提升模型的推理能力的，至少模型在图文数据集上的能力都得到了显著提升。那么为什么很多人仍然认为只有语言才是推理的核心呢？带着这个问题，我们进入下一章节。&lt;/p&gt;
&lt;h2 id=&#34;视觉调用链再看几眼还是用图思考&#34;&gt;视觉调用链：再看几眼，还是“用图思考”？&lt;/h2&gt;
&lt;p&gt;刚刚讨论了很多很多thinking with images的例子，看上去主要的方式还是通过对图像进行不同的操作，来验证推理的流程。那么这种能力是真正的用图思考么？
这里我们回顾一个纯文本CoT的项目作为对比：&lt;/p&gt;
&lt;p&gt;以 R1-V[9] 项目为例，文章在 VLM 上运行了 RLVR（不过推理过程仅仅使用文字），也得到了很大的提升：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R1-V：仅 2B 级模型 + 约 100 步 RL 训练，在 OOD 视觉计数任务上超越未 RL 的 72B 模型；&lt;/li&gt;
&lt;li&gt;核心收益来自：让小模型学会逐步核对视觉线索 → 减少一次性“胡猜” → 在分布偏移场景下保持鲁棒。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个R1-V期望的效果的例子（项目中SFT部分用来蒸馏的数据）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;R1-Distilled Answer: &lt;think&gt; Okay, let&amp;rsquo;s look at the problem. The user is asking about the number of purple metallic balls in the described scene. First, I need to go through each object in the image to check their attributes. The objects listed are: 1. A large blue rubber sphere. 2. A small brown rubber cube. 3. A small green metal cylinder. 4. A large blue metal cube. Each object has details about color, material, shape, rotation, coordinates, etc. The key attributes here are color (&amp;ldquo;purple&amp;rdquo;), material (&amp;ldquo;metallic&amp;rdquo;), and shape (&amp;ldquo;ball&amp;rdquo; or sphere). Looking at each object: - The first is a sphere, which is a ball shape, but it&amp;rsquo;s blue and made of rubber. So color and material don&amp;rsquo;t match. - The second is a cube, so shape doesn&amp;rsquo;t match. Also, it&amp;rsquo;s brown and rubber. - Third is a cylinder, so shape is not a ball. It&amp;rsquo;s green and metal, so color is wrong. - Fourth is a cube again, shape not matching. It&amp;rsquo;s blue and metal, so color is incorrect. None of the objects are purple metallic balls. There&amp;rsquo;s a blue rubber sphere and a blue metal cube, but no purple metallic spheres. The answer should be zero since none fit all three criteria. &lt;/think&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这个实验证明：当我们可以把“多步思考”推广到视觉的有效性，模型可以通过文本的方式，反复聚焦局部、换角度、验证计数、尝试多条视觉解释分支，再进行语言归纳输出。&lt;/p&gt;
&lt;p&gt;通过对这个项目的思路，我们发现其实即使只使用文本作为CoT过程，模型推理的思路和&lt;code&gt;thinking with images&lt;/code&gt;也没有太大的不同。即使仅使用文本，模型也在推理过程中逐步核对视觉线索，从而对推理能力带来帮助。那么看上去在&lt;code&gt;thinking with images&lt;/code&gt;的方法中，图片带来的并不是新的推理能力，而是更有效的信息。而对信息的选择能力，还是通过语言进行的（“First, I need to go through each object in the image to check their attributes.”）。&lt;/p&gt;
&lt;p&gt;如开头所说，通常我们可以把模型的能力分为理解、推理、生成三种。那么当我们集中在模型推理能力时，问题就变成：视觉到底是提升了推理能力，还是只是作为一个新的信息渠道（或者说工具）给模型提供了更多的信息？&lt;/p&gt;
&lt;p&gt;看一下之前分析过的一些例子：&lt;/p&gt;
&lt;h3 id=&#34;自适应视觉调用&#34;&gt;自适应视觉调用&lt;/h3&gt;
&lt;p&gt;Pixel-Reasoner 显示：初始阶段模型倾向忽略新视觉操作——语言路径是一条低阻力捷径。好奇心奖励迫使它探索视觉调用空间；策略成熟后，行为由“盲目多放大”转向“目标化少放大”，视觉操作次数下降但有效信息密度提升，说明视觉步骤被内化为高价值资源。&lt;/p&gt;
&lt;h3 id=&#34;分步求解与跨时间推理&#34;&gt;分步求解与跨时间推理&lt;/h3&gt;
&lt;p&gt;O3、DeepEyes、Kimi-VL 等研究也证明：长时间链 + 中间验证 → 更稳健的整体理解。视觉 test-time scaling 带来的效果似乎还是类似于：允许“再看几眼 + 再算几轮”，提升答案置信度与纠错概率。&lt;/p&gt;
&lt;h3 id=&#34;小结-1&#34;&gt;小结&lt;/h3&gt;
&lt;p&gt;分析下来，现在主流的“视觉推理”其实还是类似这样的范式：优化图片的注意力（可以使用工具，比如生成 HTML[8]、使用 resize / crop 操作等[6]）→ 补充新信息（矫正注意力） → 优化推理思维链，本质是信息补全，不是真正的“用图思考”，而且可以通过提供图片剪裁、放大等工具来进一步提升模型对视觉信息的理解。&lt;/p&gt;
&lt;h2 id=&#34;benchmark视觉对推理的两面性&#34;&gt;Benchmark：视觉对推理的两面性&lt;/h2&gt;
&lt;p&gt;大致分析了视觉起到的作用后，可以再从这个角度分析一下：视觉信息的引入是否只会带来提升？是不是还会引入其他的问题？这似乎取决于：模型是否学会“有条件、选择性”地把视觉纳入推理主干。&lt;/p&gt;
&lt;p&gt;实际上前面提到的论文里也都有一些相关的讨论：&lt;/p&gt;
&lt;h3 id=&#34;正向提供新增证据面&#34;&gt;正向：提供新增证据面&lt;/h3&gt;
&lt;p&gt;典型增益来源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;还原模糊或格式不规则的符号（手写算式、标注、路牌文字）；&lt;/li&gt;
&lt;li&gt;弥补题干省略的结构/空间/数量信息（图表、拓扑、布局关系）；&lt;/li&gt;
&lt;li&gt;给出可核对的外部世界细节（位置、颜色、相对关系）。
DeepEyes 在局部裁剪后解决文本模型无法回答的细节问题；O1 开启视觉后科学问答提升；这些都说明视觉是“信息补全器”。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;负向制造噪声与注意力稀释&#34;&gt;负向：制造噪声与注意力稀释&lt;/h3&gt;
&lt;p&gt;常见失败模式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bypassing：模型直接按语言惯性输出答案，视觉输入被旁路；&lt;/li&gt;
&lt;li&gt;Forced Attention：被迫描述图片却抓不到关键细节，反增幻觉；&lt;/li&gt;
&lt;li&gt;Gradient Noise：噪声图文对使得多步 CoT 每步有效信息稀释，梯度扰动放大；&lt;/li&gt;
&lt;li&gt;Shortcut Overfitting：依赖统计先验跳步作答，反而在 OOD / 细节题失效。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;缓解策略&#34;&gt;缓解策略&lt;/h3&gt;
&lt;p&gt;针对这些问题的一些解决方案。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据筛选：删“过易/过难/无视觉增益”样本（DeepEyes）；&lt;/li&gt;
&lt;li&gt;工具奖励：正确 + 合理调用工具才给额外分（R1-V）；&lt;/li&gt;
&lt;li&gt;行为激励：好奇心 / 探索奖励鼓励早期广撒网，后期渐收敛（Pixel-Reasoner）；&lt;/li&gt;
&lt;li&gt;显式动作 API：把视觉调用结构化，降低黑箱融合混淆；&lt;/li&gt;
&lt;li&gt;失败链过滤：去除绕开视觉的思维轨迹，提升监督信号纯度。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;小结-2&#34;&gt;小结&lt;/h3&gt;
&lt;p&gt;既然我们把视觉作为一种新的信息，那么似乎很自然，我们的研究方向会往降低干扰、教会模型在正确的时机使用正确的数据发展。因此我们在这里做了一些简单的讨论，而很多研究的重点也从“堆多模态数据”转向“学何时、如何、高效地看”。&lt;/p&gt;
&lt;h1 id=&#34;视觉与智能的关系&#34;&gt;视觉与智能的关系&lt;/h1&gt;
&lt;p&gt;这一部分想讨论两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么“视觉强 + 语言强” ≠ “推理显著提升”？&lt;/li&gt;
&lt;li&gt;目前视觉在大模型中到底是“协同认知模块”还是“外接工具层”？&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;能力独立性理解-生成推理不能互补&#34;&gt;能力独立性：理解、 生成、推理不能互补&lt;/h2&gt;
&lt;p&gt;考虑一下理解、推理、生成三种能力，对于文本任务来说它们似乎是天然互惠的，但是对于图像数据这些能力目前似乎还是一个非常割裂的状态。
比如早期实践里（张祥雨回顾）即使加入海量图文混排预训练，图像理解与生成也只是两个“平行模块”：一种能力的增强并不带来另一种能力的提升。而蚂蚁 Ming-Omni 等模型也采用“先感知 / 再生成”双阶段[10]训练模式，在生成训练阶段会冻结主干来避免生成训练污染推理链，进一步说明这二者尚未形成内部循环增益。&lt;/p&gt;
&lt;p&gt;追究其原因，可能包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;语言与视觉的冲突：视觉稠密、局部性强；语言稀疏、语义压缩，两者共享同一优化节奏易冲突。&lt;/li&gt;
&lt;li&gt;导致视觉 embedding 的表示能力不足：跨模态对齐多停留在 embedding / 投影层，没有在策略（如何拆解 / 规划）层深度共享。&lt;/li&gt;
&lt;li&gt;预训练数据中角色分工固化：语言承担抽象组织、验证、反思；视觉更多提供原材料。于是“能生成图” ≠ “内部推理图像化”。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结论：当前视觉的理解、生成和推理能力更像一些协作插件，我们目前训练大模型的方式和数据不足以支持图片数据对推理能力的提升。&lt;/p&gt;
&lt;h2 id=&#34;工具属性视觉仍主要扮演可调用外设&#34;&gt;工具属性：视觉仍主要扮演可调用外设&lt;/h2&gt;
&lt;p&gt;DeepEyes、Pixel-Reasoner 等工作把“看图”拆成显式动作（裁剪 / 放大 / 选帧 / 重定位），本质是把视觉转译为一组离散工具 token。这种模式是一种白盒的方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可控：何时调用、调用几次是显式决策，而非黑箱融合；&lt;/li&gt;
&lt;li&gt;可解释：推理轨迹里留有“检视—确认”痕迹，利于审计；&lt;/li&gt;
&lt;li&gt;可优化：奖励函数直接作用在“是否调用 + 是否有效”上（RL 信号更干净）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是这种白盒的方法也表明，视觉还是只是作为“取证阶段”的插件，模型的核心推理能力还是需要通过语言产生。我们还做不到让视觉参与“假设生成—反事实构造—策略分支”。现有闭环多数是“语言提出疑问 → 视觉取证 → 语言归纳”，链条方向单向，尚未形成“视觉驱动新假设”的双向循环。&lt;/p&gt;
&lt;h2 id=&#34;小结-3&#34;&gt;小结&lt;/h2&gt;
&lt;p&gt;回顾视觉与智能的当前关系，视觉似乎还是只能作为辅助来帮助 LLM 进行推理，但视觉本身并不能帮助模型能力走到下一个阶段。相反，因为模型需要分配参数来帮助自己理解图像，那么很自然就可以得出，多模态在非图文数据下，能不降低模型能力就算成功了。&lt;/p&gt;
&lt;h1 id=&#34;其他一些研究和人的看法&#34;&gt;其他一些研究和人的看法&lt;/h1&gt;
&lt;h2 id=&#34;其他研究&#34;&gt;其他研究&lt;/h2&gt;
&lt;p&gt;我们前面一直在说没有“用图思考”，那什么才是真正的用图思考？其实除了上面的主线外，也有一些这方面更“激进”的探索，试图直接在视觉域中产生与执行推理，尽量减少甚至摆脱语言的中介，从而验证图像产生智能的能力。比较有代表性的文章如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;像是 Thinking with Generated Images [12] 让模型在推理过程中主动生成中间图像作为“视觉化的思维步骤”，并在图文之间往返验证与修正，从而原生地跨模态思考；与传统“先看图再用文字想”的范式不同，这一方法把“用图像来想”纳入了推理主回路。不过这种方式看上去还是把图像生成当作一种中间草稿能力，并非推理链路本身。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更进一步的，Visual Planning: Let’s Think Only with Images [11] 则提出完全基于图像序列(如下图)来进行规划与推理：模型以一连串“中间图像状态”表达和推进思考，通过强化学习在视觉导航等任务中取得相对语言式规划更优的效果，论证了“语言并非所有推理任务的最优载体”，尤其在空间与几何场景中更是如此。该工作明确主张“只用图像来想”。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250915/thinking_only_with_images.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;总的来说，这些研究希望让视觉更进一步参与到推理的逻辑中，但是因为数据、任务等限制，这些能力还是难以被应用在大规模 scaling 的场景。&lt;/p&gt;
&lt;h2 id=&#34;最后补一句个人看法&#34;&gt;最后补一句个人看法&lt;/h2&gt;
&lt;p&gt;个人感觉，在目前的方案中，图片确实不能用来提升推理能力，而只能作为一种特殊的工具进行辅助。想要达到真正通过图像等多模态数据提升推理能力这个目标，我们还是需要有合适的任务/数据能完成视觉等模态信息对“理解、生成、推理”一体化，然后才能通过 scaling law 进行智能的扩展。也许在 “the Era of Experience” 的时代，具身智能通过与现实世界真实的交互所引入的新任务和数据，可以解决这个问题吧。&lt;/p&gt;
&lt;h1 id=&#34;参考文献&#34;&gt;参考文献&lt;/h1&gt;
&lt;p&gt;[1] 和杨植麟时隔1年的对话：K2、Agentic LLM、缸中之脑和“站在无限的开端”
&lt;a href=&#34;https://www.xiaoyuzhoufm.com/episode/68ae86d18ce45d46d49c4d50&#34;&gt;https://www.xiaoyuzhoufm.com/episode/68ae86d18ce45d46d49c4d50&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] 对OpenAI姚顺雨3小时访谈：6年Agent研究、人与系统、吞噬的边界、既单极又多元的世界
&lt;a href=&#34;https://www.xiaoyuzhoufm.com/episode/68c29ca12c82c9dccadba127&#34;&gt;https://www.xiaoyuzhoufm.com/episode/68c29ca12c82c9dccadba127&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] 和张祥雨聊，多模态研究的挣扎史和未来两年的2个“GPT-4时刻”
&lt;a href=&#34;https://www.xiaoyuzhoufm.com/episode/683d2ceb38dcc57c641a7d0f&#34;&gt;https://www.xiaoyuzhoufm.com/episode/683d2ceb38dcc57c641a7d0f&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] OpenAI o1 System Card - arXiv
&lt;a href=&#34;https://arxiv.org/abs/2412.16720&#34;&gt;https://arxiv.org/abs/2412.16720&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Thinking with images
&lt;a href=&#34;https://openai.com/index/thinking-with-images/&#34;&gt;https://openai.com/index/thinking-with-images/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] DeepEyes: Incentivizing &amp;ldquo;Thinking with Images&amp;rdquo; via Reinforcement Learning
&lt;a href=&#34;https://arxiv.org/abs/2505.14362&#34;&gt;https://arxiv.org/abs/2505.14362&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning
&lt;a href=&#34;https://arxiv.org/abs/2505.15966v2&#34;&gt;https://arxiv.org/abs/2505.15966v2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination
&lt;a href=&#34;https://arxiv.org/abs/2401.08025v2&#34;&gt;https://arxiv.org/abs/2401.08025v2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3
&lt;a href=&#34;https://github.com/StarsfieldAI/R1-V&#34;&gt;https://github.com/StarsfieldAI/R1-V&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10] Ming-Omni: A Unified Multimodal Model for Perception and Generation
&lt;a href=&#34;https://arxiv.org/abs/2506.09344&#34;&gt;https://arxiv.org/abs/2506.09344&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[11] Visual Planning: Let&amp;rsquo;s Think Only with Images
&lt;a href=&#34;https://arxiv.org/abs/2505.11409&#34;&gt;https://arxiv.org/abs/2505.11409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[12] Thinking with Generated Images
&lt;a href=&#34;https://arxiv.org/abs/2505.22525&#34;&gt;https://arxiv.org/abs/2505.22525&lt;/a&gt;&lt;/p&gt;
- https://sword865.github.io/posts/2025/2025-09-15-vlm%E4%B8%8E%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E8%BF%9B%E5%8C%96/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>再聊一下RL框架与算法的协同演化</title>
        <link>https://sword865.github.io/posts/2025/2025-08-14-%E5%86%8D%E8%81%8A%E4%B8%80%E4%B8%8Brl%E6%A1%86%E6%9E%B6%E4%B8%8E%E7%AE%97%E6%B3%95%E7%9A%84%E5%8D%8F%E5%90%8C%E6%BC%94%E5%8C%96/</link>
        <pubDate>Thu, 14 Aug 2025 22:53:26 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-08-14-%E5%86%8D%E8%81%8A%E4%B8%80%E4%B8%8Brl%E6%A1%86%E6%9E%B6%E4%B8%8E%E7%AE%97%E6%B3%95%E7%9A%84%E5%8D%8F%E5%90%8C%E6%BC%94%E5%8C%96/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-08-14-%E5%86%8D%E8%81%8A%E4%B8%80%E4%B8%8Brl%E6%A1%86%E6%9E%B6%E4%B8%8E%E7%AE%97%E6%B3%95%E7%9A%84%E5%8D%8F%E5%90%8C%E6%BC%94%E5%8C%96/ -&lt;p&gt;上一篇文章我们聊了一下Ray与LLM强化学习框架设计，探讨了其架构的演进，但是没有提到为什么框架会往这个方向逐渐演进而不是一开始就使用现在的设计。这里面自然有实践中不断优化的结果，但是也是和整个LLM RL需求的变化密切相关的。&lt;/p&gt;
&lt;p&gt;因此，本文会主要讨论一下LLM强化学习中，算法与系统框架是如何相互影响、协同演化的。首先分析两个相对成熟的协同设计案例，然后讨论几个正在不够成熟、但是在笔者看来很有潜力的优化方向。&lt;/p&gt;
&lt;h1 id=&#34;算法与框架协同演化的典型案例&#34;&gt;算法与框架协同演化的典型案例&lt;/h1&gt;
&lt;p&gt;先讲两个目前已经相对得到了共识的问题：&lt;/p&gt;
&lt;h2 id=&#34;案例一推理模型驱动的分离式架构设计&#34;&gt;案例一：推理模型驱动的分离式架构设计&lt;/h2&gt;
&lt;p&gt;在前一篇文章中提到过，之前的强化学习还是希望尽量on-policy的，因此早期的强化学习系统倾向于采用on-policy算法配合co-located的架构设计。这种选择有其合理性：算法层面，on-policy确实具备样本效率优势；系统层面，在CoT和Test-time Scaling兴起之前，模型输出长度差异相对较小，推理引擎产生的计算空泡还算可控。尽管资源利用率的问题客观存在，但on-policy算法的优势在一定程度上抵消了这种系统层面的低效。&lt;/p&gt;
&lt;p&gt;不过，从去年o1发布开始，业界开始重视Test time scaling，加上R1的发布又给推理模型点了一把火，这种范式的改变打破了之前的平衡：模型在推理阶段生成的文本长度显著增加，且不同样本间的长度差异极为悬殊。在这种新的计算模式下，on-policy的算法优势已无法弥补系统资源的巨大浪费——所有并行环境必须等待最长推理任务完成才能进入下一轮迭代，这种同步约束造成了严重的吞吐量瓶颈和大量计算资源闲置。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250814/noCotToCot.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;正是因为这个挑战，业界逐渐开始转向异步RL框架设计。其核心思想是将Generation与Training完全解耦，构建生产者-消费者的流水线架构。以AsyncFlow和AReaL为例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;推理引擎（Rollout Workers）&lt;/strong&gt;：持续异步生成新的数据，彼此间无需同步等待。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;训练引擎（Trainer Workers）&lt;/strong&gt;：训练节点异步地从共享缓冲区获取数据进行模型更新。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种&lt;strong&gt;流式RL&lt;/strong&gt;设计有效避免了慢速推理任务对整体流程的阻塞，确保所有计算设备维持高利用率，从而实现训练吞吐量的显著提升。这正是算法演进与系统架构优化协同设计的一个典型案例：新的需求需要新的取舍，进一步催生了新的架构。&lt;/p&gt;
&lt;h2 id=&#34;案例二moe架构与训练推理引擎的精度对齐挑战&#34;&gt;案例二：MoE架构与训练推理引擎的精度对齐挑战&lt;/h2&gt;
&lt;p&gt;另一个挑战出现在系统层面，尤其是在使用超大模型或混合专家（MoE）模型时：训练引擎和推理引擎之间的不匹配。&lt;/p&gt;
&lt;p&gt;在RL的训练中，通常为了效率，模型的推理（生成）过程可能会在专门的推理引擎上执行（例如使用vLLM），而梯度计算则在训练后端（例如DeepSpeed）上进行。然而，两者在数值精度（如FP16 vs INT8）、算子融合、批处理调度或MoE门控行为上都可能存在差异，进而导致rollout数据和训练计算之间的分布漂移。&lt;/p&gt;
&lt;p&gt;实际上，这个问题并不是在MoE中才刚刚出现，以小红书为例，他们在去年的QCon演讲&lt;a href=&#34;https://mp.weixin.qq.com/s/tG_ktQ0WbZHQavtoJtaXbw&#34;&gt;从0到1构建RLHF系统——小红书大模型团队的探索与实践&lt;/a&gt;中就提到了对训推一致性的要求，通过自研框架进行对齐。此外还有一些公司会选择使用推理引擎生产数据，然后通过训练引擎再次推理拿到logit进行概率计算。&lt;/p&gt;
&lt;p&gt;但是随着MoE的兴起和推理引擎加速技术的不断发展，精度对齐变得越来越困难，比如MoE的路由层引入的不一致可能远高于Dense Model中的精度不一致。同时由于数据的实际采样概率还是由推理引擎决定，以前主要是不同引擎计算的概率不同，但是现在可能连采样出的token都会有巨大的变化，这让通过训练框架再次对齐也更不够用了。&lt;/p&gt;
&lt;p&gt;为了解决这一问题，AREAL框架提出了&lt;a href=&#34;http://arxiv.org/abs/2505.24298&#34;&gt;Decoupled PPO&lt;/a&gt;算法，采用了一种巧妙的双引擎协同策略：利用推理引擎输出的概率进行重要性采样（因为它反映了真实的数据采样分布），同时使用训练引擎输出的概率来计算置信域（Trust Region），确保训练引擎对模型更新仍在训练引擎原来的有效范围内进行。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250814/decoupledPPO.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;类似的，&lt;a href=&#34;https://fengyao.notion.site/flash-rl&#34;&gt;FlashRL&lt;/a&gt; 也在他们的&lt;a href=&#34;https://fengyao.notion.site/off-policy-rl&#34;&gt;blog&lt;/a&gt;中提出了一种截断重要性采样的技术&lt;strong&gt;TIS&lt;/strong&gt;，通过对更新进行重新加权，来修正量化推理（用于采样）和全精度模型（用于优化）之间的策略差异。通过这一技术，即使是高度量化的rollout数据（如INT8/FP8）也能被用于训练，而不会损害最终效果。&lt;/p&gt;
&lt;p&gt;可以看到，在这个例子里，研究者们通过算法的设计，来弥合了系统设计导致的训练和推理之间的鸿沟。&lt;/p&gt;
&lt;h1 id=&#34;可能的发展趋势协同设计的新兴挑战与机遇&#34;&gt;（可能的）发展趋势：协同设计的新兴挑战与机遇&lt;/h1&gt;
&lt;p&gt;讲了有共识的，有相对公认的解决方案的案例以后，我们来看一看有些相对不那么得到共识，或者大家意识到但是解决方案还没有完全收敛的方向：&lt;/p&gt;
&lt;h2 id=&#34;方向一agent-rl-样本效率环境管理与过程奖励&#34;&gt;方向一：Agent RL: 样本效率、环境管理与过程奖励&lt;/h2&gt;
&lt;p&gt;随着Agent RL的发展，强化学习的过程中，LLM需要升级为能够调用工具、API，并与环境持续交互的自主智能体时，新的挑战也随之浮现。&lt;/p&gt;
&lt;p&gt;个人认为，根据当前研究趋势，Agent RL主要可归为两类典型范式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sandbox + Browser：通常用来训练通用Agent，在受控沙箱或浏览器环境中进行任务执行与评估；&lt;/li&gt;
&lt;li&gt;MCP + Tooluse：常见于内部或垂直领域，增强模型使用工具集合使用能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以实际场景为例，可以更清晰地看到Agent RL所面临的关键难题。&lt;/p&gt;
&lt;h3 id=&#34;1-奖励稀疏性与延迟问题&#34;&gt;1. 奖励稀疏性与延迟问题&lt;/h3&gt;
&lt;p&gt;在复杂任务中，类似o1的&lt;strong&gt;结果奖励&lt;/strong&gt;机制效率极低。例如，Anthropic提到Claude Ops 4可在后台连续运行7小时完成软件开发任务，若仅在最终成败时给予奖励信号，这种延迟且稀疏的反馈使得样本的生成变得越加困难，同时单一的结果奖励几乎无法有效指导学习过程。因此，过程级奖励（Process-level Rewards）或者智能体引导机制（Agent Guidance）可能会重新变得重要。&lt;/p&gt;
&lt;p&gt;近期工作如&lt;a href=&#34;https://arxiv.org/pdf/2506.11425&#34;&gt;Agent-RLVR&lt;/a&gt; 框架，在 RL 训练中引入高层指令提示、动态错误反馈等“教学式”奖励信号，显著提升了智能体在复杂编程任务中的成功率。这类方法模拟人类教学过程，通过中间反馈加速策略收敛。&lt;/p&gt;
&lt;p&gt;与之相关的，还有对样本效率的提高：一方面需要构建更高效的样本生成流水线，另一方面要探索高质量数据的重用机制以降低采样成本。相关研究也在逐步涌现，如&lt;a href=&#34;https://arxiv.org/abs/2508.06412v1&#34;&gt;Sample-efficient LLM Optimization with Reset Replay&lt;/a&gt;，为解决Agent RL中的样本稀缺问题提供新的思路。&lt;/p&gt;
&lt;h3 id=&#34;2-复杂状态表示与管理&#34;&gt;2. 复杂状态表示与管理&lt;/h3&gt;
&lt;p&gt;Agent 的&lt;strong&gt;状态&lt;/strong&gt;不仅包括对话历史，还涵盖工具调用输出、环境观测、以及内部思维链等多源异构信息。传统 RL 框架难以有效建模此类高维、长序列的状态空间。&lt;/p&gt;
&lt;p&gt;为此，新兴研究开始探索情景记忆模块与状态压缩技术。例如，&lt;a href=&#34;https://arxiv.org/abs/2405.14751v2&#34;&gt;AGILE&lt;/a&gt; 框架表明，为智能体配备显式的记忆存储与反思机制，能显著增强其跨步推理能力和长期任务一致性。&lt;/p&gt;
&lt;h3 id=&#34;3-环境可观测性与工具可靠性&#34;&gt;3. 环境可观测性与工具可靠性&lt;/h3&gt;
&lt;p&gt;Agent RL 通常依赖多种外部工具或服务（如代码解释器、搜索 API等），但这些组件可能存在不可用、响应延迟或返回错误等问题。这使得环境状态难以准确观测，增加了训练的不稳定性和调试难度。如何实现对工具调用链的可观测性监控与容错恢复机制，也会成为系统设计中的关键挑战。&lt;/p&gt;
&lt;h3 id=&#34;新的rl基础设施&#34;&gt;新的RL基础设施&lt;/h3&gt;
&lt;p&gt;为应对上述挑战，RL 基础设施正朝着更高模块化、更强可扩展性的方向发展。个人认为，未来的 Agent RL 框架需具备：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个支持细粒度追踪的分布式训练引擎，以实现全流程可观测；&lt;/li&gt;
&lt;li&gt;推理引擎支持持续 partial rollout，即分阶段生成轨迹数据，便于中间干预与数据复用；&lt;/li&gt;
&lt;li&gt;更加高效的离线强化学习与数据重用机制，缓解采样成本高、高质量轨迹稀缺的问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;只有构建兼具灵活性、可解释性与工程鲁棒性的新型 RL 架构，才能支撑智能体在真实复杂环境中的持续学习与自主进化。&lt;/p&gt;
&lt;h2 id=&#34;方向二生成式奖励模型与进一步的分离式架构&#34;&gt;方向二：生成式奖励模型与进一步的分离式架构&lt;/h2&gt;
&lt;p&gt;在近期的对齐研究中，另一个相对显著的趋势是&lt;strong&gt;生成式奖励模型(GRM)&lt;/strong&gt;，比如&lt;a href=&#34;http://arxiv.org/abs/2504.02495&#34;&gt;DeepSeek-GRM&lt;/a&gt;。与传统奖励模型仅输出标量分数不同，生成式奖励模型——本质上也是一个大语言模型——能够生成详细的评估文本或推理轨迹来表达其判断，通过test scaling的方式大大提升了奖励模型的鲁棒性。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250814/deepseekGRM.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;实际上，在几年前OpenAI在&lt;a href=&#34;http://arxiv.org/abs/2210.10760&#34;&gt;Scaling Laws for Reward Model Overoptimization&lt;/a&gt;中就强调了scale reward model的重要性，GRM正是这一技术的延伸。&lt;/p&gt;
&lt;p&gt;类似于o1/R1放大了Disaggregated架构的重要性，生成式奖励模型的引入会进一步推动RL训练流水线向更加&lt;strong&gt;分布式、多模型协同&lt;/strong&gt;的方向演进。不同于传统的单一策略模型训练模式，新的架构需要支持策略模型、价值模型、奖励模型等多个大型模型的并行交互与动态调度。这对底层框架的资源管理、任务编排以及模型间通信机制提出了更高要求。&lt;/p&gt;
&lt;p&gt;可以预见，生成式奖励模型代表了RL与LLM内在推理能力的深度融合，它不仅是算法层面的创新，更是对整个RL基础设施提出的系统性挑战。&lt;/p&gt;
&lt;h1 id=&#34;写在最后&#34;&gt;写在最后&lt;/h1&gt;
&lt;p&gt;从技术演进的角度来看，本文讨论的这些案例展现了LLM强化学习领域中算法与系统协同演化的规律。这种演化并非偶然，而是技术发展中系统性约束与算法创新相互作用的必然结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;成熟案例的启示&lt;/strong&gt;：异步RL架构与推理时长不确定性的矛盾、MoE模型训推一致性挑战，都体现了一个共同特征——当系统约束成为瓶颈时，算法创新往往会寻找新的理论基础来突破这些限制。Decoupled PPO和截断重要性采样等方法的出现，本质上是将系统工程中的&amp;quot;分治&amp;quot;思想引入算法设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;新兴趋势的挑战&lt;/strong&gt;：Agent RL中的环境管理复杂性、生成式奖励模型的多模型协同需求，则预示着未来系统设计将面临更高维度的权衡。过程级奖励机制需要更精细的状态管理，而GRM的引入则要求框架具备更强的多模型编排能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;协同演化的深层逻辑&lt;/strong&gt;：这种算法与系统的紧密耦合反映了LLM强化学习的本质特征——它不仅是一个计算问题，更是一个分布式系统工程问题。算法的有效性往往取决于系统能否提供相应的基础设施支撑，而系统的演进方向也会被算法的需求所引导。&lt;/p&gt;
&lt;p&gt;从更宏观的视角来看，这种协同演化正在重新定义强化学习系统的边界。传统的单机RL框架设计原则在大规模分布式环境中需要根本性的重新思考。我们未来应该会看到更多&lt;strong&gt;系统感知&lt;/strong&gt;的算法设计，以及更多&lt;strong&gt;算法优化&lt;/strong&gt;的系统架构。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2025-08-14-%E5%86%8D%E8%81%8A%E4%B8%80%E4%B8%8Brl%E6%A1%86%E6%9E%B6%E4%B8%8E%E7%AE%97%E6%B3%95%E7%9A%84%E5%8D%8F%E5%90%8C%E6%BC%94%E5%8C%96/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Ray与LLM强化学习框架设计</title>
        <link>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</link>
        <pubDate>Sun, 27 Jul 2025 14:53:26 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/ -&lt;p&gt;最近LLM强化学习框架发展特别快，Ray作为被ChatGPT带火的框架，在LLM各个训练阶段中，RL阶段的应用应该是最多的。写篇文章记录一下这块发展的脉络和一些看法。&lt;/p&gt;
&lt;h1 id=&#34;从google-pathways说起&#34;&gt;从Google Pathways说起&lt;/h1&gt;
&lt;p&gt;讨论Ray和RL系统，得从Google的&lt;strong&gt;Pathways&lt;/strong&gt;系统开始：2021年Google提出了Pathways作为下一代AI架构和分布式ML平台，在相关文献中详细讨论了Single-Controller + MPMD的系统设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Single-Controller&lt;/strong&gt;（单控制器）是指用一个中央协调器来管理整个分布式计算流程的架构模式。在这种设计中，有一个&lt;strong&gt;主控制节点&lt;/strong&gt;负责整个计算图的执行，包括任务分发、资源调度、状态监控等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiple-Controller&lt;/strong&gt;（多控制器）则是指使用多个分布式控制节点来协同管理计算任务的架构模式。在这种设计中，没有单一的中央协调器，而是由多个控制器节点分别负责不同的子系统或计算子图，通过分布式协调协议来实现全局一致性。&lt;/p&gt;
&lt;p&gt;在Ray中的Driver Process就可以被作为一个典型的Single Controller来启动不同的任务程序，而通过torchrun运行的PyTorch DDP分布式计算则是在每个node上各自执行自己的程序则属于典型的Multiple Controller范式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MPMD&lt;/strong&gt;（Multiple Program, Multiple Data）是一种分布式计算范式，指在一个计算任务中，不同的节点运行不同的程序来处理不同的数据。这种模式下，各个计算节点执行的代码逻辑可能完全不同，每个节点都有自己特定的任务和职责。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SPMD&lt;/strong&gt;（Single Program, Multiple Data）则是另一种常见的分布式计算范式，指所有节点运行相同的程序，但处理不同的数据分片。&lt;/p&gt;
&lt;p&gt;典型的SPMD任务包括传统的分布式训练，比如PyTorch DDP，每个节点运行相同的程序来处理不同的数据，最多根据rank的值会有一些特别的处理（比如rank=0的节点负责checkpoint）。相比之下，大模型训练包括了流水并行这种更复杂的任务，每个节点组需要运行不同的程序，就更适合用MPMD的方式来实现了。&lt;/p&gt;
&lt;p&gt;一般来说，MPMD系统由于包含众多异构组件，各组件间的协调和同步变得相当复杂。为了简化开发复杂度并确保系统执行的一致性，Single-Controller架构成为了自然的选择——通过引入中心化的控制器来统一管理整个分布式计算流程，包括任务调度、状态同步和异常处理等关键环节。&lt;/p&gt;
&lt;p&gt;更多的细节就不多说了，有兴趣的话可以去看Oneflow团队当年写的两篇文章，非常深刻：&lt;a href=&#34;https://mp.weixin.qq.com/s/roQues5HhRXqGf26DuUOjQ&#34;&gt;解读谷歌Pathways架构（一）：Single-controller与Multi-controller&lt;/a&gt;和&lt;a href=&#34;https://mp.weixin.qq.com/s/N99dRgFYC9zOOcGlg0Ulsw&#34;&gt;解读谷歌 Pathways 架构（二）：向前一步是 OneFlow&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这与LLM强化学习有什么关系呢？用RL训练LLM本质上是一个多阶段、多节点的复杂分布式任务。典型的RLHF流水线涉及多个不同模型，计算流程分为几个关键阶段：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;生成阶段&lt;/strong&gt;：当前策略模型（LLM）对一批输入提示生成响应文本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估阶段&lt;/strong&gt;：这些响应由奖励模型评分，或通过人类/自动化偏好模型进行比较评估&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练阶段&lt;/strong&gt;：基于获得的奖励信号更新策略模型权重（可能还包括价值函数或评论家网络的更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些阶段之间存在明确的数据依赖关系——训练更新必须依赖于生成的样本及其对应的奖励分数。在朴素的实现中，这些阶段只能串行执行，引入大量上下文切换的同时还要求所有的模型使用相同数量的GPU进行计算，计算效率是相当低下的。因此正如Pathways架构所启发的那样，我们希望在保证正确性的前提下，通过良好的系统设计，尽可能地重叠和并行化这些工作阶段，以最大化计算资源的利用效率。&lt;/p&gt;
&lt;p&gt;直接说有点抽象，可以看下面这个从&lt;a href=&#34;http://arxiv.org/abs/2409.19256&#34;&gt;HybridFlow&lt;/a&gt;里截的表格，我截了两个最早的RLHF系统，左边的DeepSpeed-Chat实现了SPMD的串行方式，而右边的OpenRLHF则是典型的MPMD系统。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_SPMD_MPMD.png&#34; class=&#34;center&#34; /&gt;
&lt;h1 id=&#34;ray与llm强化学习框架&#34;&gt;Ray与LLM强化学习框架&lt;/h1&gt;
&lt;p&gt;其实从前面的内容可以看出来，Ray的设计很适合用来开发Single-Controller + MPMD的程序，也就自然适合LLM强化学习的场景了。&lt;/p&gt;
&lt;p&gt;实际上，社区也确实基于Ray开发了大量的强化学习框架，目前主要的设计包括两种：&lt;strong&gt;Colocated架构&lt;/strong&gt;和&lt;strong&gt;Disaggregated架构&lt;/strong&gt;。粗略地说，&lt;strong&gt;Colocated架构&lt;/strong&gt;意味着把生成阶段和训练阶段放在同样的节点上运行；而&lt;strong&gt;Disaggregated架构&lt;/strong&gt;则把它们放在不同的节点上：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RL_architecture.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;一看这个图，我们会发现Disaggregated Architecture中存在大量的计算bubble，甚至可能比不上之前SPMD模式！这也是为什么很多框架如OpenRLHF、Nemo-aligner、VeRL都是按照Colocated架构来设计的。&lt;/p&gt;
&lt;p&gt;需要注意的是，图里的Train和Gen代表的是RLHF的不同阶段，每个阶段内每个GPU可能在运行不同任务，因此整个过程仍然是MPMD的。&lt;/p&gt;
&lt;p&gt;以经典的PPO算法为例，整个Train的阶段包括Actor Model(on training Framework)/Reference Model/Reward Model/Critic Model四个模型，Gen阶段包括Actor Model(on inference framework)一个模型，以&lt;a href=&#34;http://arxiv.org/abs/2405.11143&#34;&gt;OpenRLHF&lt;/a&gt;为例：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_PPO.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;可以看到这里Actor Model会在Deepspeed和vLLM两个引擎间进行切换，因为实际算法需要保存的模型共有5个。&lt;/p&gt;
&lt;h2 id=&#34;colocated-rl框架-解法和问题&#34;&gt;Colocated RL框架 (解法和问题)&lt;/h2&gt;
&lt;p&gt;接下来继续看这个基于Ray的框架：OpenRLHF使用Ray启动和协调组件，但使用Ray的&lt;strong&gt;Placement group&lt;/strong&gt;实现了Colocated架构，在每个节点上在rollout和训练任务之间分割GPU资源。例如，在给定节点上，框架可能将每个GPU的0.75分配给训练actor，0.25分配给生成actor，这样有效地让一个训练进程和一个生成进程&amp;quot;共享&amp;quot;每个GPU而不互相干扰。&lt;/p&gt;
&lt;p&gt;很容易从前面的图看出来，Colocated框架中的&lt;strong&gt;资源共享&lt;/strong&gt;是一个主要的优势，通过设计合适的分组方式，我们可以减少GPU的空闲时间，减少模型offload的频率，同时尽量并行化不同节点的执行，从而最大化提升资源利用效率。&lt;/p&gt;
&lt;p&gt;然而，随着模型大小和集群大小的增长，Colocated框架也显示出了自己的局限性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一个关键问题是&lt;a href=&#34;http://arxiv.org/abs/2504.15930&#34;&gt;StreamRL&lt;/a&gt;中提到的&lt;strong&gt;资源耦合&lt;/strong&gt;。虽然Colocated框架比起SPMD的程序提升了计算任务的并行性，并通过分组来允许每组model使用不同的资源，但是这并不能完全消除共享设备带来的问题：因为生成和训练同时共享相同设备，我们不能独立扩展或为每个阶段定制资源。同时训练任务（计算密集）和生成任务（IO密集）的瓶颈并不相同，这不利于GPU资源的利用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;另一个问题是，LLM生成的文本长度是不固定的，尤其随着thinking model的大火，生成任务中不同组的模型生成结果的时间可能差异很大。比如我们有32块GPU，每4块GPU为一组进行生成，如果其中一组生成任务过长，会导致其他28块GPU空等造成资源浪费。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总的来说，Colocated框架通过精细的资源管理实现了较高的GPU利用率，相对成熟和稳定，确实许多后续框架都借鉴了类似的设计思路。但是，正如前面提到的资源耦合问题，这种架构在可扩展性方面仍有局限。这也为下一代RL框架的发展指明了方向：能否通过打破严格的串行约束，让生成和训练阶段真正独立地并行执行？&lt;/p&gt;
&lt;h2 id=&#34;on-policy和off-policy&#34;&gt;On-Policy和Off-Policy&lt;/h2&gt;
&lt;p&gt;本文的重点是Ray和LLM RL的框架设计，因此不会在这一部分内容做过多阐述，大概而言：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On-Policy&lt;/strong&gt;: 严格要求使用当前最新策略生成的数据进行训练。在RLHF中，这意味着每次训练迭代都必须等待当前actor模型完成新一轮的文本生成，然后立即使用这些&amp;quot;新鲜&amp;quot;的样本来更新模型权重。这种方式能保证训练数据与当前策略的完全一致性，但代价是强制的同步等待，导致前面提到的大量计算bubble。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OffPolicy&lt;/strong&gt;: 允许使用稍微&amp;quot;陈旧&amp;quot;的策略生成的数据进行训练。在实际系统中，这意味着训练阶段可以使用来自之前几个迭代版本的actor模型生成的样本，而不必严格等待当前版本的生成完成。虽然引入了一定的策略陈旧性（policy staleness），但这种设计允许生成和训练阶段真正并行化，大幅提升系统吞吐量。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;On-Policy&lt;/strong&gt;算法正是前文中Disaggregated Architecture存在大量计算空泡的根本原因：训练阶段只能使用最新策略模型生成的样本，这就强制要求生成步骤必须在训练步骤完成后才能开始下一轮迭代。&lt;/p&gt;
&lt;p&gt;从理论角度分析，On-Policy的样本效率确实优于Off-Policy，这主要源于其能够保证训练数据与当前策略分布的完全一致性。在LLM强化学习场景中，策略陈旧性（policy staleness）会引入分布偏移问题——生成数据的策略分布与当前训练策略产生偏差，这种不匹配可能降低训练样本的有效性，并对模型收敛的稳定性和效率产生负面影响。&lt;/p&gt;
&lt;p&gt;不过在RL的工业实践中，这种理论差异往往被系统吞吐量的大幅提升所抵消。现代LLM RL系统通过以下几种方式来缓解策略陈旧性的影响：&lt;strong&gt;增加Experience Buffer的容量、优化batch sampling策略、以及采用更频繁的模型同步机制&lt;/strong&gt;。实际部署中，有团队发现通过合理的超参数调整（如学习率衰减、梯度裁剪），Off-Policy系统在保持训练稳定性的同时，能够获得成倍的训练吞吐量提升。这种工程权衡也推动了RLHF向多轮迭代和DPO向Iterative DPO等更适合并行化的算法变种演进。&lt;/p&gt;
&lt;h2 id=&#34;disaggregated架构-从off-policy到streaming-rl&#34;&gt;Disaggregated架构 (从off-policy到Streaming RL)&lt;/h2&gt;
&lt;p&gt;自然地，业界开始探索在LLM强化学习中采用Off-Policy算法的可能性。&lt;a href=&#34;http://arxiv.org/abs/2410.18252&#34;&gt;ASYNCHRONOUS RLHF&lt;/a&gt;的研究为这一方向提供了积极的信号，该研究表明适度的&lt;strong&gt;策略陈旧性是可以接受的&lt;/strong&gt;，并且不会显著影响训练效果。这一重要发现随后在&lt;a href=&#34;http://arxiv.org/abs/2504.15930&#34;&gt;StreamRL&lt;/a&gt;的工作中得到了进一步的验证和扩展。
类似的，Meta也在&lt;a href=&#34;http://arxiv.org/abs/2505.24034&#34;&gt;LlamaRL&lt;/a&gt;中提出了下面的计算流，我们得到了新的流式的&lt;strong&gt;Disaggregated架构&lt;/strong&gt;：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_offlineRL.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;因此，我们观察到，最新的LLM强化学习系统正在移向&lt;strong&gt;Disaggregated架构 + Off-Policy&lt;/strong&gt;的实现，这与MPMD范式一致。想法是&lt;strong&gt;将生成和训练阶段分离为不同的服务&lt;/strong&gt;，它们并发在不同的GPU资源池上运行。我们有，比如说，一个专门用于生成新rollout的GPU资源池和另一个专门用于训练任务的资源池。两个任务在不同的GPU资源上并行操作，样本不断从生成端持续流向训练端。&lt;/p&gt;
&lt;p&gt;相比传统的Colocated架构，Disaggregated架构能够真正实现&lt;strong&gt;异构资源的独立扩展&lt;/strong&gt;。举个例子，如果发现生成阶段成为瓶颈（比如复杂推理任务需要更长的生成时间），我们可以单独为生成服务增加更多GPU，而不影响训练集群的配置。相反，如果奖励模型计算或PPO更新成为瓶颈，我们可以针对性地扩展训练侧的资源。这种&lt;strong&gt;弹性伸缩&lt;/strong&gt;的能力在云环境和多租户场景中特别有价值。&lt;/p&gt;
&lt;p&gt;通过接受一点&lt;strong&gt;陈旧策略&lt;/strong&gt;的数据，流式框架大大改善了&lt;strong&gt;资源利用率&lt;/strong&gt;。不需要所有GPU在迭代之间同步暂停；相反，生成和训练都可以达到稳定的吞吐量。StreamRL的设计明确解决了困扰同步设计的流水线&amp;quot;空泡&amp;quot;（空闲间隙）和落后任务的长尾。它们实现了完全异步流水线，其中权重更新、生成和训练尽可能重叠。此外，流式框架通常引入弹性优势：可以独立扩展生成服务和训练器服务。例如，如果生成长序列是慢的部分，系统可以为流生成服务分配更多GPU而不增加训练器GPU。或者相反，如果发现奖励模型评分是瓶颈，就扩展那个。这种解耦在多租户或云环境中很强大。&lt;/p&gt;
&lt;p&gt;更进一步，为了更好的对训练集群和推理集群进行协调，我们可以引入通过某种数据缓冲区或队列连接来作为两个集群的交互点。这种设计可以实现生成和训练的&lt;strong&gt;近乎完美重叠&lt;/strong&gt;，从而消除流水线中的大部分空闲时间。这里的一个例子是&lt;a href=&#34;http://arxiv.org/abs/2507.01663&#34;&gt;AsyncFlow&lt;/a&gt;，这个框架基于&lt;code&gt;TransferQueue&lt;/code&gt;来传输数据，并控制训练的执行。&lt;/p&gt;
&lt;img width=&#34;600&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/stream_RL_with_Buffer.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;总之，流式RL框架将RL循环解耦为独立组件并利用异步执行来提升吞吐量。它们通过避免资源耦合并允许更细粒度的可扩展性来解决协同定位的缺点。成本是增加的系统复杂性和需要处理离线策略训练——但通过仔细设计（如AsyncFlow、StreamRL等），这些问题是可管理的。&lt;/p&gt;
&lt;h1 id=&#34;无ray的rl&#34;&gt;无Ray的RL？&lt;/h1&gt;
&lt;p&gt;随着LLM强化学习框架的不断演进，似乎有了一个新的趋势：框架&lt;strong&gt;对Ray的依赖正在逐渐减少&lt;/strong&gt;。回顾一下，Ray最初是这些框架快速原型化的理想选择——它能够轻松处理集群配置、进程启动，并提供简洁的远程函数调用和Actor类API。事实上，像OpenRLHF这样的早期项目，中期的VeRL，以及新的Slime和AsyncFlow，都将Ray作为关键的&amp;quot;胶水层&amp;quot;来协调复杂的训练循环。&lt;/p&gt;
&lt;p&gt;然而，随着这些系统在生产环境中的大规模部署，Ray的一些固有限制开始显现：&lt;/p&gt;
&lt;h2 id=&#34;ray的技术挑战&#34;&gt;Ray的技术挑战&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;调试复杂性&lt;/strong&gt;是Ray面临的首要问题。当某个远程Worker深层出现异常时，你往往只能收到一个经过序列化传输的模糊错误信息，很难追踪问题的真正根源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;通信开销&lt;/strong&gt;同样是Ray架构中的一个关键瓶颈。Ray的核心设计依赖Python对象序列化和gRPC通信，这在RLHF场景下面临巨大压力。想象一下，每个experience batch可能包含数万个token的生成文本、完整的logits分布、以及各种奖励和价值函数的输出——这些数据的体积是相当可观的。当这样的大负载需要在生成阶段和训练阶段之间频繁传输时，序列化开销就变得不可忽视了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然Ray社区在通过各种优化手段（如&lt;a href=&#34;https://mp.weixin.qq.com/s/KehYVdkdEC-9H7jaDMIlow&#34;&gt;Ray Flow Insight&lt;/a&gt;、&lt;a href=&#34;https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html&#34;&gt;Compiled Graph&lt;/a&gt;和&lt;a href=&#34;https://github.com/ray-project/ray/issues/51173&#34;&gt;GPU Objects&lt;/a&gt;等）尝试缓解这些问题，但目前这些技术并不成熟也不够好用。&lt;/p&gt;
&lt;h2 id=&#34;缓冲区驱动的无ray架构趋势&#34;&gt;缓冲区驱动的无Ray架构趋势&lt;/h2&gt;
&lt;p&gt;鉴于这些问题，我们已经在社区看到了很多关于&lt;strong&gt;无Ray&lt;/strong&gt;设计的讨论。Meta的LlamaRL完全建立在原生PyTorch上并在&lt;strong&gt;405B&lt;/strong&gt;的模型上做了验证。其他的框架也会有一些关于去Ray化的讨论，比如&lt;a href=&#34;https://github.com/volcengine/verl/discussions/2202&#34;&gt;VeRL&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;那么这个是否可行呢？仔细想想RL框架的发展趋势，新的Streaming based RL框架的核心概念&lt;strong&gt;数据缓冲区&lt;/strong&gt;——本质上是一个专用模块，它保存当前rollout集合并将它们提供给训练工作器。我们可以将其视为经验的消息队列或共享内存。框架在生成任务中不断将数据放入缓冲区，并在训练任务中不断从缓冲区拉取数据进行消费。两个阶段的解耦程度已经非常高了，我们似乎并不需要一个复杂的调度层来管理所有的任务。&lt;/p&gt;
&lt;p&gt;不过，完全抛弃Ray也不是没有成本的。有些框架还是选择留着Ray，主要是因为它在一些地方确实挺好用的：&lt;strong&gt;集群管理、进程启动、故障恢复和资源调度&lt;/strong&gt;。拿字节跳动的HybridFlow团队来说，他们当时也想过用PyTorch原生的TorchRPC来替换Ray，而且确实也跑起来了。但真正部署的时候，他们发现TorchRPC的维护并不是很积极，还会有一些奇怪的边界情况问题：虽然通过精心的工程设计，我们可以得到更好的性能，但Ray在构建稳定的分布式应用时确实能省不少开发和维护的功夫。&lt;/p&gt;
&lt;p&gt;此外，我觉得另一个问题是训练的规模，随着后训练算力的投入，Ray的核心作用也许会从控制流编排转向容错和动态的资源分配管理：RL比起预训练要灵活得多，我们在预训练中都有通过Ray来优化训练稳定性的例子（虽然用得不多），那么在RL中这个作用应该会变得愈加明显。不过可惜的是这种只有头部公司才玩得起了。&lt;/p&gt;
&lt;h1 id=&#34;写在最后&#34;&gt;写在最后&lt;/h1&gt;
&lt;p&gt;从技术演进的角度来看，LLM强化学习框架在过去一年多的发展轨迹清晰地展现了分布式系统设计的经典权衡。&lt;/p&gt;
&lt;p&gt;早期的Colocated架构通过资源共享实现了较高的GPU利用率，但随着模型规模和集群规模的增长，资源耦合问题逐渐凸显。Streaming RL的出现标志着系统设计思路的根本性转变——通过系统和算法的co-design，从on-policy策略向off-policy策略迁移，通过接受有限的策略陈旧性来换取更好的可扩展性和资源利用效率。&lt;/p&gt;
&lt;p&gt;Ray作为这一代框架的重要基础设施，在快速原型开发和集群管理方面发挥了关键作用。然而，随着系统复杂度的提升和性能要求的提高，其在调试复杂性和通信开销方面的局限性也日益明显。这促使社区开始探索更加专门化的解决方案，包括基于原生PyTorch的实现和混合架构设计。&lt;/p&gt;
&lt;p&gt;从系统架构的发展趋势来看，未来的LLM强化学习框架可能会继续朝着更细粒度的解耦和专门化方向发展。核心计算组件将更多地依赖高效的原生实现，而集群管理和容错机制则可能继续依托成熟的分布式框架。这种分层设计既能满足性能要求，又能保持系统的可维护性。&lt;/p&gt;
&lt;p&gt;值得注意的是，这个领域的快速迭代反映了LLM训练工程化的不断成熟。随着模型规模和训练复杂度的持续增长，我们预期会看到更多针对特定场景优化的专门化框架，以及更加标准化的系统接口和协议。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Ray Data反压机制</title>
        <link>https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/</link>
        <pubDate>Sat, 05 Jul 2025 15:03:26 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/ -&lt;p&gt;做Ray Platform也快2年了，遇到过各种的问题，整理一些踩过的坑看一下。&lt;/p&gt;
&lt;p&gt;先从我们自己最常用的Ray Data开始，看看最常见的OOM/OOD问题，这个问题很多时候都是和反压相关的。&lt;/p&gt;
&lt;p&gt;说是Ray Data，不过这里的反压不止一层，大概包括下面几个地方：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ray Core Generator&lt;/strong&gt;：针对Ray Generators的控制，防止后台生成的数据过多导致OOM/OOD。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Executor + Resource Allocator&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;针对正在执行的任务，控制生成结果的速度，避免单个任务生成的数据过多导致OOM/OOD。&lt;/li&gt;
&lt;li&gt;针对单个Operator，控制提交任务的数量，避免在资源紧张时提交新任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backpressure Policies&lt;/strong&gt;: 其他关于任务提交的反压规则。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面我们逐层分析这些机制的实现。&lt;/p&gt;
&lt;h1 id=&#34;ray-core-generator对象数量反压&#34;&gt;Ray Core Generator：对象数量反压&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/ray-core/ray-generator.html&#34;&gt;Ray Generator&lt;/a&gt; 类似Python Generator，用来作为迭代器进行遍历，但是和Python Generator有一个很大的不同在于：Ray Generator使用&lt;code&gt;ObjectRefGenerator&lt;/code&gt;在后台持续执行。也就是说如果Ray Data的单个read_task需要读取一个很大的文件时，没法通过控制拉取任务产出的速度来控制任务的内存占用。（不管下游是否主动拉取，都会持续读取新的数据block。）&lt;/p&gt;
&lt;p&gt;针对这个问题，Ray Generators支持手动配置一个threshold(_generator_backpressure_num_objects parameter)来对Generators进行反压。&lt;/p&gt;
&lt;p&gt;核心逻辑在&lt;code&gt;task_manager.cc&lt;/code&gt;中的&lt;code&gt;HandleReportGeneratorItemReturns&lt;/code&gt;这个方法里面。这个函数逻辑比较复杂，里面还有比如乱序/幂等等问题的处理，我们只看反压状态的管理：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 请求的item的index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; item_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; request.item_index();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 生成器已生产的对象数量
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; total_generated &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.TotalNumObjectWritten();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;//已被消费的对象数量  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; total_consumed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.TotalNumObjectConsumed();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// item已经被消费了，说明消费速度足够快，不用反压。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.IsObjectConsumed(item_index)) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;OK(), total_consumed);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; false;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Otherwise, follow the regular backpressure logic.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// NOTE, here we check `item_index - last_consumed_index &amp;gt;= backpressure_threshold`,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// instead of the number of unconsumed items, because we may receive the
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// `HandleReportGeneratorItemReturns` requests out of order.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (backpressure_threshold &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      (item_index &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.LastConsumedIndex()) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; backpressure_threshold) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    RAY_LOG(DEBUG) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Stream &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; generator_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; is backpressured. total_generated: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; total_generated
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;. total_consumed: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; total_consumed
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;. threshold: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; backpressure_threshold;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; signal_it &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ref_stream_execution_signal_callbacks_.find(generator_id);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (signal_it &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; ref_stream_execution_signal_callbacks_.end()) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;NotFound(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Stream is deleted.&amp;#34;&lt;/span&gt;), &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      signal_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.push_back(execution_signal_callback);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// No need to backpressure.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;OK(), total_consumed);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;所以未消费对象数量达到阈值时，Ray Generator会暂停任务执行。&lt;/p&gt;
&lt;p&gt;在Ray Data中，taskpool和actor pool都默认设置了&lt;code&gt;_generator_backpressure_num_objects&lt;/code&gt;参数来控制数据的生成，以&lt;code&gt;TaskPoolMapOperator&lt;/code&gt;为例：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;_generator_backpressure_num_objects&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dynamic_ray_remote_args
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_context&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_max_num_blocks_in_streaming_gen_buffer &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;# 2 objects for each block: the block and the block metadata.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            dynamic_ray_remote_args[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;_generator_backpressure_num_objects&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_context&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_max_num_blocks_in_streaming_gen_buffer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;streaming-executor--resource-allocator&#34;&gt;Streaming Executor + Resource Allocator&lt;/h1&gt;
&lt;p&gt;虽然Ray Core提供了基础反压的接口，但是运行Ray Data任务的时候，还是有其他问题，其中最核心的问题就是&lt;em&gt;是否需要消费上游算子生成的结果&lt;/em&gt;？&lt;/p&gt;
&lt;h2 id=&#34;预算分配&#34;&gt;预算分配&lt;/h2&gt;
&lt;p&gt;Ray使用了预算预分配的方式，给Ray Data任务的每个operator都分配了一个预算，这个预算包括2部分：&lt;/p&gt;
&lt;h3 id=&#34;reserved_for_op_outputs&#34;&gt;&lt;code&gt;reserved_for_op_outputs&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;为算子输出数据预留的内存空间。&lt;/li&gt;
&lt;li&gt;用来保证有足够的内存来存储算子的输出数据，防止所有预算都被pending task outputs占用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;_op_reserved和_op_budgets&#34;&gt;&lt;code&gt;_op_reserved&lt;/code&gt;和&lt;code&gt;_op_budgets&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;_op_reserved&lt;/code&gt;：每个算子的预留资源。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;_op_budgets&lt;/code&gt;: 根据实际情况算出来的，算子可以使用的资源，大致上&lt;code&gt;op_budgets[op] = max(_op_reserved[op] - 当前使用量, 0) + 分配的共享资源&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;预算分配的逻辑在&lt;code&gt;resource_manager.py&lt;/code&gt;里，整个逻辑大概包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;把整个object store分为reserved资源(&lt;code&gt;op_total_reserved&lt;/code&gt;)和shared资源(&lt;code&gt;_total_shared&lt;/code&gt;)两部分。&lt;/li&gt;
&lt;li&gt;给每个算子分配一个初始的budget(&lt;code&gt;op_total_reserved&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;把budget分成2份：&lt;code&gt;reserved_for_op_outputs&lt;/code&gt;和&lt;code&gt;_op_reserved&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;根据算子实际使用的内存情况，计算每个算子剩余的budget数量。（从&lt;code&gt;_op_reserved&lt;/code&gt;得到&lt;code&gt;_op_budgets&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;把共享资源按需分配到各个算子的&lt;code&gt;_op_budgets&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;特殊算子处理：对materializing算子如AllToAllOperator不做任何限制。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;单个task生成速度的控制&#34;&gt;单个Task生成速度的控制&lt;/h3&gt;
&lt;p&gt;有了budget以后，就可以对Ray Data中的每个算子进行反压了，先看正在执行的Ray Generator Task的反压：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 对有结果产生的任务，计算还可以输出的bytes，控制任务输出。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; task &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; ready_tasks:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    bytes_read &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; task&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;on_data_ready(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_bytes_to_read_per_op&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(state, &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; state &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; max_bytes_to_read_per_op:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_bytes_to_read_per_op[state] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; bytes_read
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;code&gt;on_data_ready&lt;/code&gt;会从Ray Generator消费数据，并且一旦消费的数据量达到预算限制就会停止消费：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;on_data_ready&lt;/span&gt;(self, max_bytes_to_read: Optional[int]) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; int:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;当数据准备就绪时的回调&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    bytes_read &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; max_bytes_to_read &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; bytes_read &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; max_bytes_to_read:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            block_ref &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_streaming_gen&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_next_sync(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; block_ref&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_nil():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;StopIteration&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_task_done_callback(&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# 处理数据块并累计读取字节数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# bytes_read += process_block(block_ref)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; bytes_read
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;预算的限制则来自&lt;code&gt;max_task_output_bytes_to_read&lt;/code&gt;，计算逻辑就是分配的资源减去使用的资源。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_task_output_bytes_to_read&lt;/span&gt;(self, op: PhysicalOperator) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; Optional[int]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_op_budgets[op]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;object_store_memory
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Add the remaining of `_reserved_for_op_outputs`.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        op_outputs_usage &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_get_op_outputs_usage_with_downstream(op)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        res &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; max(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_reserved_for_op_outputs[op] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; op_outputs_usage, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isinf(res):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# corner case的处理，略。        &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; res        
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这样就控制了每个task的Generator的消费速度，防止任何单个操作符占用过多内存。&lt;/p&gt;
&lt;h2 id=&#34;task提交速度的控制&#34;&gt;Task提交速度的控制&lt;/h2&gt;
&lt;p&gt;除了限制单个任务的消费，Ray Data还会控制任务的提交，即在算子budget不足时停止提交该算子的任务。&lt;/p&gt;
&lt;p&gt;这块逻辑比较简单，由streaming executor的&lt;code&gt;select_operator_to_run&lt;/code&gt;方法控制&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ops &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; op, state &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; topology&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; resource_manager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;op_resource_allocator_enabled(), topology
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        under_resource_limits &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            resource_manager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;op_resource_allocator&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;can_submit_new_task(op)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        in_backpressure &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; under_resource_limits &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; any(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;can_add_input(op) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; backpressure_policies
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;code&gt;can_submit_new_task&lt;/code&gt;就是在判断是否有足够的资源可以提交新的任务。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;can_submit_new_task&lt;/span&gt;(self, op: PhysicalOperator) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; bool:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; op &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_op_budgets:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        budget &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_op_budgets[op]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; op&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;incremental_resource_usage()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;satisfies_limit(budget)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; res
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;backpressure-policies-其他关于任务提交的反压规则&#34;&gt;&lt;strong&gt;Backpressure Policies&lt;/strong&gt;: 其他关于任务提交的反压规则。&lt;/h1&gt;
&lt;p&gt;最后一个&lt;code&gt;Backpressure Policies&lt;/code&gt;其实就是前面&lt;code&gt;select_operator_to_run&lt;/code&gt;方法里提到的&lt;code&gt;backpressure_policies&lt;/code&gt;了：&lt;/p&gt;
&lt;p&gt;回顾一下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        in_backpressure &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; under_resource_limits &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; any(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;can_add_input(op) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; backpressure_policies
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里目前其实只有一个策略，就是并发度的控制策略，没什么好说的，就是看一下正在运行的任务数量是否达到设置的并发上限。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ConcurrencyCapBackpressurePolicy&lt;/span&gt;(BackpressurePolicy):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;A backpressure policy that caps the concurrency of each operator.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    The policy will limit the number of concurrently running tasks based on its
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    concurrency cap parameter.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    NOTE: Only support setting concurrency cap for `TaskPoolMapOperator` for now.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    TODO(chengsu): Consolidate with actor scaling logic of `ActorPoolMapOperator`.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# .....&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;can_add_input&lt;/span&gt;(self, op: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;PhysicalOperator&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; bool:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; op&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;metrics&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_tasks_running &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_concurrency_caps[op]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;- https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Flash MLA Kernel分析</title>
        <link>https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/</link>
        <pubDate>Sat, 03 May 2025 15:51:35 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/ -&lt;p&gt;准备对DeepSeek的开源项目整理一些文档，也顺便强化一下记忆，先从FlashMLA开始。&lt;/p&gt;
&lt;p&gt;FlashMLA是DeepSeek开源的MLA算子实现，这个实现主要给inference decoding用的，Training和prefill应该是另外一个算子。&lt;/p&gt;
&lt;p&gt;先拿下面的图表示一下MLA算子是在计算一个什么东西，这篇文章就不讲具体的推导了，反正这个算子大概就是下面的2个GEMM算子的融合。需要注意的是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这里矩阵K和矩阵V的共享一部分参数。&lt;/li&gt;
&lt;li&gt;图里只画显示了一个Query Head和一对KV Head的计算。在实际计算中还要num_kv_head和batch_size两个维度。&lt;/li&gt;
&lt;li&gt;两个GEMM中间其实还有一个sotfmax，不过这里可以通过online softmax算法把这块逻辑独立处理分块处理，所以不影响主流程。&lt;/li&gt;
&lt;/ol&gt;
&lt;img width=&#34;600&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/mla.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;Kernel的调用主要分两部分&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调用&lt;code&gt;get_mla_metadata&lt;/code&gt;来计算一些metadata，用来优化kernel的执行&lt;/li&gt;
&lt;li&gt;调用&lt;code&gt;flash_mla_with_kvcache&lt;/code&gt;进行计算&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在进入调用前，先大概说一下FlashMLA计算的拆分逻辑。这块和FlashDecoding很像，并没有要求一个thread-block必须处理一个完整的sequence，而是通过一个负载均衡算法，把所有的sequence放到一起，然后拆分成一个个的sequence-block，然后每个thread-block就去处理分配给它的那些block的计算，最后再把这些thread-block的结果用合并，得到正确的输出。&lt;/p&gt;
&lt;p&gt;大概是下面这个图的样子：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/computation-pattern.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;所以为了完成计算，第一步就是决定每个block需要处理哪些sub-sequence，也就是&lt;code&gt;get_mla_metadata&lt;/code&gt;要完成的事情。&lt;/p&gt;
&lt;h1 id=&#34;get_mla_metadata&#34;&gt;get_mla_metadata&lt;/h1&gt;
&lt;p&gt;先看&lt;code&gt;get_mla_metadata&lt;/code&gt;具体提供了哪些元数据，我们从repo提供的测试代码入手，考虑最简单的情况(batch_size=128, query_sequence_len=1, mean_key_sequence_len=4096, MTP=1, num_kv_head=1, num_q_head=16, TP=1, hidden_NoRoPE_dim=512, hidden_RoPE_dim=64, varlen=False)。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# cache_seqlens = tensor([4096, 4096, ..., 4096], dtype=torch.int32),
#                         size=batch_size, value=sequence_len
# s_q=1 (query_sequence_len=1且MTP=1), h_q(num_q_head)=128 (TP=1=128/128) h_kv(num_kv_head)=1
# 基于这些配置，计算mla kernel的metadata
tile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为这里我们是在测试decoding步骤，所以有&lt;code&gt;query_sequence_len=1&lt;/code&gt;，可以看到三个入参：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;kv cache的大小&lt;/li&gt;
&lt;li&gt;类似GQA的Group数量，这个参数表示每个kv head对应多少个query head。&lt;/li&gt;
&lt;li&gt;kv head的数量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;get_mla_metadata&lt;/code&gt;会根据GPU中SM的数量和要处理的数据的大小，给每个SM分配任务。这个注意&lt;code&gt;get_mla_metadata_kernel&lt;/code&gt;的参数为&lt;code&gt;&amp;lt;&amp;lt;&amp;lt;1, 32, 0, stream&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;，因此所有计算会在1个warp中完成。&lt;/p&gt;
&lt;p&gt;这里的关键就是具体怎么给每个(每组)SM分配工作的.&lt;/p&gt;
&lt;p&gt;首先，每几个SM会一起处理一个kv head和一组query head的计算：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;int num_sm_parts = sm_count / num_heads_k / cutlass::ceil_div(num_heads_per_head_k, block_size_m);
&lt;/code&gt;&lt;/pre&gt;&lt;img width=&#34;600&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/flash_mla_sm_part.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;然后，我们计算每组SM需要处理多少个block，然后把block分配到每一个SM，具体任务的分配过程为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据batch size和&lt;code&gt;mean_key_sequence_len&lt;/code&gt;计算出一共有多少个block。&lt;/li&gt;
&lt;li&gt;给每个SM分配工作，包括每个SM要处理的tile的索引和位置。&lt;/li&gt;
&lt;li&gt;记录一下每个sequnce的切分点的位置，用于在计算时把结果正确的合起来才能得到完整的注意力输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OK, 这样我们就完成了对任务的划分，接下来进入关键的计算kernel。&lt;/p&gt;
&lt;h1 id=&#34;flash_mla_with_kvcache&#34;&gt;flash_mla_with_kvcache&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;flash_mla_with_kvcache&lt;/code&gt;函数内部其实也是由2个子kernel组成的&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;flash_fwd_splitkv_mla_kernel&lt;/code&gt;: 通过for循环的方式，计算每个SM分配到的block的GEMM乘法。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flash_fwd_splitkv_mla_combine_kernel&lt;/code&gt;: 负责把多个block的计算结果合起来，得到最终的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;flash_fwd_splitkv_mla_kernel&#34;&gt;flash_fwd_splitkv_mla_kernel&lt;/h2&gt;
&lt;p&gt;先看&lt;code&gt;flash_fwd_splitkv_mla_kernel&lt;/code&gt;，这个kernel包括&lt;code&gt;num_m_block * num_query_head * num_sm_parts&lt;/code&gt; 个thread-block。其中&lt;code&gt;num_m_block=seqlen_q/block_size_m(64)&lt;/code&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kernel&amp;lt;&amp;lt;&amp;lt;dim3(num_m_block, params.h, params.num_sm_parts), Kernel_traits::kNThreads, smem_size, stream&amp;gt;&amp;gt;&amp;gt;(params);&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意，这里的&lt;code&gt;seqlen_q&lt;/code&gt;并不是一开始的1了，实际上它等于&lt;code&gt;num_heads_per_head_k (seqlen_q = seqlen_q_ori * ngroups, 在MTP=1的情况下等于num_heads_per_head_k)&lt;/code&gt;
这样我们会发现：&lt;code&gt;num_m_block=cutlass::ceil_div(num_heads_per_head_k, block_size_m);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;回忆之前的SM分组公式，有&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SM数量 = num_sm_parts * num_heads_k * ceil_div(num_heads_per_head_k, block_size_m)
       = num_sm_parts * ceil_div(num_heads_k * num_heads_per_head_k, block_size_m) 
       = num_sm_parts * ceil_div(num_query_head, block_size_m)
       = num_sm_parts * num_m_block
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因此SM的数量对应了thread-block的第一维和最后一维。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dim3(num_m_block, params.h, params.num_sm_parts)&lt;/code&gt;的这三个维度分别表示：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这个thread-block处理哪一个block。&lt;/li&gt;
&lt;li&gt;这个thread-block应该处理哪的一个query head。&lt;/li&gt;
&lt;li&gt;这个thread-block在对应的SM Group内的编号。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们来看每个thread-block会计算什么，我们知道多个thread-block会共同完成分配给一个SM的block的计算。
看代码发现这里其实有2重循环：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;外层循环会遍历所以分配给这个SM的query block。&lt;/li&gt;
&lt;li&gt;内层循环会遍历对应的KV cache block，计算出O的一个block。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用了Warp Specialization的策略，通过生产者&amp;ndash;&amp;gt;消费者的方式进行计算。
&lt;ul&gt;
&lt;li&gt;Warp Group 1：主要计算线程，负责大部分的注意力得分计算。&lt;/li&gt;
&lt;li&gt;Warp Group 2：使用double buffer的技术进行数据的加载，也参与一些计算。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这块代码比较复杂的，zhihu上有几篇文章写的挺清楚的，我就不一点点写分析了，画个图过来表示一下计算过程，对细节感兴趣的可以去看后面的几个参考的文章。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/flashmla_wap_spec.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;这里可以看到，Warp Group 0会计算GEMM1，但是GEMM2是由两个Warp Group共同计算的，每个Wrap计算其中一半。&lt;/p&gt;
&lt;p&gt;这里比较重要的几块逻辑：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Warp Specialization&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  int warp_group_idx = cutlass::canonical_warp_group_idx();
    if (warp_group_idx == 0) {
        // 主要计算逻辑，包括矩阵乘法、归一化、概率矩阵的计算和输出
        // thread 0 - 127
        ....
    } else {
       // 主要负责加载数据
       // thread 128 - 256
    }
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;上面else逻辑中的双缓冲&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 双缓冲区数据结构定义
template&amp;lt;typename Kernel_traits&amp;gt;
struct SharedStorageMLA {
    union {
        struct {
               // 存储 Query、Key 和中间结果
              ...
              cute::array_aligned&amp;lt;typename Kernel_traits::Element, 
                  cute::cosize_v&amp;lt;typename Kernel_traits::SmemLayoutK&amp;gt; * 2&amp;gt; smem_k;  // Double buffer
              ...
        }
        ...
    }
}
...

 // 双缓冲策略(在warp group 1的代码里)：切换到第二个缓冲区
if (n_block % 2 == 1) {           
       // Double buffer for sK
       constexpr int sK_offset = size(sK);
       tSrK.data() = tSrK.data() + sK_offset / 8;
       tOrVt.data() = tOrVt.data() + sK_offset / 8;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;flash_fwd_splitkv_mla_combine_kernel&#34;&gt;flash_fwd_splitkv_mla_combine_kernel&lt;/h2&gt;
&lt;p&gt;最后的&lt;code&gt;flash_fwd_splitkv_mla_combine_kernel&lt;/code&gt;比较简单，就是负责数据的合并：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在Warp 0中计算各个block的Log-Sum-Exp最大值，获取全局归一化系数。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for (int i = 0; i &amp;lt; kNLsePerThread; ++i) max_lse = max(max_lse, local_lse[i]);
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;在Warp 0中计算缩放因子。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for (int i = 0; i &amp;lt; kNLsePerThread; ++i) {
       const int split = i * 32 + tidx;
       if (split &amp;lt; actual_num_splits) sLseScale[split] = expf(local_lse[i] - global_lse);
}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;按照缩放因子，合并Output的输出，完成计算。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for (int split = 0; split &amp;lt; actual_num_splits; ++split) {
       ...
       ElementAccum lse_scale = sLseScale[split];
       for (int i = 0; i &amp;lt; size(tOrO); ++i) {
              tOrO(i) += lse_scale * tOrOaccum(i);
        }
        ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;把结果写回全局内存。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;参考文章&#34;&gt;参考文章：&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.04434&#34;&gt;DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/26269071923&#34;&gt;DeepSeek: FlashMLA代码解析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/26080342823&#34;&gt;flashMLA 深度解析&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
- https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>vLLM Paged Attention代码分析</title>
        <link>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
        <pubDate>Sun, 20 Apr 2025 15:51:35 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ -&lt;p&gt;3月底整理了一个关于经典Paged Attention算法的ppt, 想起这个几年没写过的blog，把PPT改成一篇文章证明我还活着(-_-)。&lt;/p&gt;
&lt;img width=&#34;500&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/paged_attention.png&#34; class=&#34;center&#34; /&gt;
&lt;h2 id=&#34;vllm-的-paged-attention&#34;&gt;vLLM 的 Paged Attention&lt;/h2&gt;
&lt;p&gt;开始前先说明一下，vLLM里的Paged Attention Kernel是有好几个不同的版本的，大概是下面这样子：&lt;/p&gt;
&lt;p&gt;vLLM早期版本：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prefilling -&amp;gt; Flash Attention的flash_attn_varlen_func&lt;/li&gt;
&lt;li&gt;Dedocding -&amp;gt; 自己实现的Paged Attention
&lt;ul&gt;
&lt;li&gt;paged_attention_v1 : 用于比较短的sequence&lt;/li&gt;
&lt;li&gt;paged_attention_v2 : 用于不想用v1的情况 :)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;源码大概是这样的：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    # NOTE(woosuk): We use a simple heuristic to decide whether to use
    # PagedAttention V1 or V2. If the number of partitions is 1, we use
    # V1 to avoid the overhead of reduction. Also, if the number of
    # sequences or heads is large, we use V1 since there is enough work
    # to parallelize.
    # TODO(woosuk): Tune this heuristic.
    # For context len &amp;gt; 8192, use V2 kernel to avoid shared memory
    # shortage.
    use_v1 = (max_seq_len &amp;lt;= 8192 and (max_num_partitions == 1 or num_seqs * num_heads &amp;gt; 512))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;vLLM 最新版本就已经全部转向Flash Attention， 用cutlass实现了。&lt;/p&gt;
&lt;h2 id=&#34;nvidia-gpu-基础&#34;&gt;NVIDIA GPU 基础&lt;/h2&gt;
&lt;p&gt;在深入 Paged Attention 的实现之前，我们需要了解 NVIDIA GPU的基本架构。（这里我们主要讲A100）&lt;/p&gt;
&lt;p&gt;在做开发时，GPU 的 CUDA 程序包括 Grid -&amp;gt; Thread Block -&amp;gt; Threads三层架构。
这三层架构对应到GPU的硬件：GPU -&amp;gt; SM -&amp;gt; Cuda Core&lt;/p&gt;
&lt;p&gt;在实际执行的时候，Threads会以每32个为一组执行，因此这里还多了一层：Thread Wrap，因此结构变成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CUDA程序：Grid -&amp;gt; Thread Block -&amp;gt; Thread Wrap -&amp;gt; Threads四层架构。&lt;/li&gt;
&lt;li&gt;GPU硬件：GPU -&amp;gt; SM(多次执行) -&amp;gt; SM(一次执行) -&amp;gt; Cuda Core，也是四层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 A100 GPU 上，我们有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;108个SM&lt;/li&gt;
&lt;li&gt;每个SM有4个Wrap scheduler
&lt;ul&gt;
&lt;li&gt;最多有4个Thread Wrap同时在一个SM上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;每个Wrap scheduler有一个长度为16的调度队列
&lt;ul&gt;
&lt;li&gt;一个SM上最多可以调度64个Thread Wrap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基本上这些数字在设计Kernel的时候都可以被考虑到，从而最大化一个Kernel的硬件利用率。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/nvida_gpu.png&#34; class=&#34;center&#34; /&gt;
&lt;h2 id=&#34;vllm-kernel-映射&#34;&gt;vLLM Kernel 映射&lt;/h2&gt;
&lt;p&gt;现在我们看一下vLLM Kernel的设计：(处于简化的目的，我们认为没有TP)&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/vllm_kernel_map.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;设计Kernel的第一步是把程序拆分成不同的Thread Block来简化问题，vLLM中每个Thread Block会负责1个Query Token的一个Query Head的计算。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个设计其实比较粗糙。不过没关系，Flash Attention里有更多优化。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;设计好计算粒度后，是内存布局的优化，vLLM的Kernel对Q，K, V使用了不同的内存布局，看代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
                                          // head_size/x, block_size, x]
    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
                                          // head_size, block_size]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;内存设计的时候，我们一般要考虑的是如何能够更好的做到：1. 每次读取连续的内存块(向量读指令，最好能翻译成_ld.global.b128，以128bit也就是16Bytes为单位)；2. 降低不同thread的读冲突。QKV的内存布局基本上也是考虑这些来做的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q的布局是最简单的，序列长度-&amp;gt;head数量-&amp;gt;head维度。&lt;/li&gt;
&lt;li&gt;K的布局比较复杂，最外层的num_block和num_kv_heads比较好理解，对应了一块KV Cache。但是在没有按照head_size连续存储，还引入了一个参数x。这个布局其实是为了优化K的读取效率，我们在后面再讲。&lt;/li&gt;
&lt;li&gt;最后是V的布局，比K要直接一些，没有额外的维度x。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于这个设计，在列一下相关的代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // 我们希望能用一个thread wrap一次处理一个KV cache block，这里block_size一般是4/8/16这样子。 
  // 因为wrap_size=32，大于cache block size，我们就可以给一个token多分配几个thread来加速计算。
  // 用wrap size 处理 cache_block_size，这样就知道一个wrap thread可以用几个Thread来处理一个token。
  // 这里数字被记作thread group size
  [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
  assert(head_size % thread_group_size == 0);
  ...
  # 这里的Num threads是128，最后算出来4个wrap，应该也是对应了A100个一个SM有4个wrap scheduler。
  constexpr int NUM_WARPS = NUM的_THREADS / WARP_SIZE;
  ...
  # 没啥好说的，就是一个thread block用128个thread，处理一个token的一个head
  dim3 grid(num_heads, num_seqs, 1);
  dim3 block(NUM_THREADS);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;随着Thread Group的提出，这个CUDA Kernel的架构变的更复杂了 :(&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CUDA程序：Grid -&amp;gt; Thread Block -&amp;gt; Thread Wrap -&amp;gt; Thread Group -&amp;gt; Threads五层架构。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;query数据访问&#34;&gt;Query数据访问&lt;/h2&gt;
&lt;p&gt;讲了这么多终于开始计算了，先拿张图演示一下Query Token的访问：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/query_io.png&#34; class=&#34;center&#34; /&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);

  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;

  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
#pragma unroll
  for (int i = thread_group_idx; i &amp;lt; NUM_VECS_PER_THREAD;
       i += NUM_THREAD_GROUPS) {
    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
    q_vecs[thread_group_offset][i] =
        *reinterpret_cast&amp;lt;const Q_vec*&amp;gt;(q_ptr + vec_idx * VEC_SIZE);
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对着这块代码讲一下：
可以看到这里因为一个block是专门处理一个token的一个head的，因此把这块数据存在了shared memory里，这样方便复用。&lt;/p&gt;
&lt;p&gt;然后这里又引入了一个叫VEC_SIZE的东西，这里其实就是说如果我想一次读16Bytes(_ld.global.b128), 那每个thead一次要读几个元素(因为一共有thread group个线程一起读)。&lt;/p&gt;
&lt;p&gt;然后就用各种size来算一下每个thread要读多少次vec，这多么元素又对应多少个VEC，我们就知道一个thread具体要读哪些数据了。&lt;/p&gt;
&lt;p&gt;这里thread每次读取的粒度是vec, 而每个thread group每次则读取16Bytes，最后就可以合并成一个向量读的指令来优化IO。&lt;/p&gt;
&lt;p&gt;按这个方式把数据都读进来吗，这里其实很多thread要读的数据是一样的，只要有一份数据完成读取，然后其他thread group就可以直接复用这个数据了&lt;/p&gt;
&lt;h2 id=&#34;key-cache数据访问&#34;&gt;Key Cache数据访问&lt;/h2&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/key_io.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;和Query一样，每个thread按VEC访问数据，也是希望一次访问16Bytes，这也是Query最内层有一个X维度的原因。&lt;/p&gt;
&lt;p&gt;我们每x个元素的大小是16Bytes，那么每个Thread就可以按16Bytes的方式对数据进行读取。&lt;/p&gt;
&lt;p&gt;因为我们需要处理Query Token和所有Key Token的乘积，因此这里要一个block一个block的把所有Key Token读进来。
整个读取过程大概是这样一个三层循环：&lt;/p&gt;
&lt;p&gt;每个Thread Wrap通过循环的方式处理多个Paged Block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;循环的每次大迭代处理一个Paged Block（内部通过两个小循环处理）
&lt;ul&gt;
&lt;li&gt;内层外循环：遍历block内每个Token (block_size次）
&lt;ul&gt;
&lt;li&gt;内层内循环：遍历每个Token的每个vec  (head_size / (thread_group_size*vec_size))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同样，只要一个Thread Wrap完成读取，Thread Block里其他Thread Wrap会复用这个读取结果。&lt;/p&gt;
&lt;p&gt;看一个这个三重循环的代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // 外层大循环：4个wrap一起遍历所有所有paged blocks
  // 每次循环：每个wrap处理一个paged block
      // 4 个wrap同时执行 （并行度）
  // wrap_idx = thread_is / wrap_size
  for (int block_idx = start_block_idx + warp_idx; block_idx &amp;lt; end_block_idx;
       block_idx += NUM_WARPS) {
    ...
    // Load a key to registers.
    // Each thread in a thread group has a different part of the key.
    // For example, if the the thread group size is 4, then the first thread in
    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
    // has 1, 5, 9, ... th vectors of the key, and so on.
    // 内层外循环：遍历所有paged blocks
    // 每次循环：32个thread一起遍历当前paged block内所有token
        // NUM_TOKENS_PER_THREAD_GROUP =      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
        // 每个thread group负责一个token(token_idx)
    for (int i = 0; i &amp;lt; NUM_TOKENS_PER_THREAD_GROUP; i++) {
      const int physical_block_offset =
          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
      K_vec k_vecs[NUM_VECS_PER_THREAD];

#pragma unroll
        // 内层内循环：按 VEC_SIZE 遍历Token内的所有fp16
        // 每次循环：32个thread一起遍历paged block内每个token
            // NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
            // NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
            // 每个thread group 为一组,处理同一个token
            // 每个threa一次读取一个VEC
            // 每个thread group一共负责 NUM_VECS_PER_THREAD 个VEC
            // K_vec k_vecs[NUM_VECS_PER_THREAD];  (1 Paged Block)
      for (int j = 0; j &amp;lt; NUM_VECS_PER_THREAD; j++) {
        const cache_t* k_ptr =
            k_cache + physical_block_number * kv_block_stride +
            kv_head_idx * kv_head_stride + physical_block_offset * x;
        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
        const int offset1 = (vec_idx * VEC_SIZE) / x;
        const int offset2 = (vec_idx * VEC_SIZE) % x;

        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
          k_vecs[j] = *reinterpret_cast&amp;lt;const K_vec*&amp;gt;(
              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
        } else {
          // Vector conversion from Quant_vec to K_vec.
          Quant_vec k_vec_quant = *reinterpret_cast&amp;lt;const Quant_vec*&amp;gt;(
              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
          k_vecs[j] = fp8::scaled_convert&amp;lt;K_vec, Quant_vec, KV_DTYPE&amp;gt;(
              k_vec_quant, *k_scale);
        }
      }

      // Compute dot product.
      ....
  }
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;qk-计算&#34;&gt;QK 计算&lt;/h2&gt;
&lt;p&gt;Query和Key都读进来了，下一步自然就是矩阵乘了，还是先上个图：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/query_key_compute.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;基础的计算的逻辑就是上面代码里省略的部分&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;      // Compute dot product.
      // This includes a reduction across the threads in the same thread group.
      float qk = scale * Qk_dot&amp;lt;scalar_t, THREAD_GROUP_SIZE&amp;gt;::dot(
                             q_vecs[thread_group_offset], k_vecs);
      // Add the ALiBi bias if slopes are given.
      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;

      if (thread_group_offset == 0) {
        // Store the partial reductions to shared memory.
        // NOTE(woosuk): It is required to zero out the masked logits.
        const bool mask = token_idx &amp;gt;= seq_len;
        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
        // Update the max value.
        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
      }
    }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后要做reduce，因为所有qk的乘积是分布在多个Thread里面的，我们要把Thread block里所有的数据都聚合起来，才好算最后的softmax，这里是经典的两层reduce算法，细节就不解释了。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // Perform reduction across the threads in the same warp to get the
  // max qk value for each &amp;#34;warp&amp;#34; (not across the thread block yet).
  // The 0-th thread of each thread group already has its max qk value.
#pragma unroll
  // 经典的Thread Wrap内reduce算法
  for (int mask = WARP_SIZE / 2; mask &amp;gt;= THREAD_GROUP_SIZE; mask /= 2) {
    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
  }
  if (lane == 0) {
    red_smem[warp_idx] = qk_max;
  }
  __syncthreads();

  // 经典的Thread Block内reduce算法
  // TODO(woosuk): Refactor this part.
  // Get the max qk value for the sequence.
  qk_max = lane &amp;lt; NUM_WARPS ? red_smem[lane] : -FLT_MAX;
#pragma unroll
  for (int mask = NUM_WARPS / 2; mask &amp;gt;= 1; mask /= 2) {
    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
  }
  // Broadcast the max qk value to all threads.
  qk_max = VLLM_SHFL_SYNC(qk_max, 0);

  // 计算softmax
  // Get the sum of the exp values.
  float exp_sum = 0.f;
  for (int i = thread_idx; i &amp;lt; num_tokens; i += NUM_THREADS) {
    float val = __expf(logits[i] - qk_max);
    logits[i] = val;
    exp_sum += val;
  }
  exp_sum = block_sum&amp;lt;NUM_WARPS&amp;gt;(&amp;amp;red_smem[NUM_WARPS], exp_sum);

  // Compute softmax.
  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
  for (int i = thread_idx; i &amp;lt; num_tokens; i += NUM_THREADS) {
    logits[i] *= inv_sum;
  }
  __syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;跑到这里以后，softmax就算完了，剩下来的就是在乘一下V Cache，这块就比较简单了。&lt;/p&gt;
&lt;h2 id=&#34;value访问和attneion计算&#34;&gt;Value访问和Attneion计算&lt;/h2&gt;
&lt;p&gt;Value的访问比较直接，直接上图，就是大家一起把所有Value都读进来。
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/value_io.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;p&gt;同时边读边计算，都在这个图里了：
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/attention_compute.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;p&gt;就是先按block进行遍历，然后每次重block里读所有token的一部分维度进行计算，把所有维度都算出来，分散的存在各个thread里。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // 一样的外层大循环：4个wrap一起遍历所有paged blocks
  for (int block_idx = start_block_idx + warp_idx; block_idx &amp;lt; end_block_idx;
       block_idx += NUM_WARPS) {
    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
    // int64 because int32 can lead to overflow when this variable is multiplied
    // by large numbers (e.g., kv_block_stride).

    const int64_t physical_block_number =
        static_cast&amp;lt;int64_t&amp;gt;(block_table[block_idx]);
    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
    L_vec logits_vec;
    from_float(logits_vec, *reinterpret_cast&amp;lt;Float_L_vec*&amp;gt;(logits + token_idx -
                                                           start_token_idx));

    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
                           kv_head_idx * kv_head_stride;
#pragma unroll

    // NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
    // 一个Paged Block有几个VEC
    // NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
        // 一个WRAP可以同时处理几个 Paged Block 
    // NUM_ROWS_PER_THREAD = DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
        // 遍历所有head纬度需要几个迭代
    for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
      if (row_idx &amp;lt; HEAD_SIZE) {
        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
        V_vec v_vec;

        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
          v_vec = *reinterpret_cast&amp;lt;const V_vec*&amp;gt;(v_ptr + offset);
        } else {
          V_quant_vec v_quant_vec =
              *reinterpret_cast&amp;lt;const V_quant_vec*&amp;gt;(v_ptr + offset);
          // Vector conversion from V_quant_vec to V_vec.
          v_vec = fp8::scaled_convert&amp;lt;V_vec, V_quant_vec, KV_DTYPE&amp;gt;(v_quant_vec,
                                                                    *v_scale);
        }
        if (block_idx == num_seq_blocks - 1) {
          // num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
          // 对最后一个Paged Block特殊处理，防止越界
          // NOTE(woosuk): When v_vec contains the tokens that are out of the
          // context, we should explicitly zero out the values since they may
          // contain NaNs. See
          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
          scalar_t* v_vec_ptr = reinterpret_cast&amp;lt;scalar_t*&amp;gt;(&amp;amp;v_vec);
#pragma unroll
          for (int j = 0; j &amp;lt; V_VEC_SIZE; j++) {
            v_vec_ptr[j] = token_idx + j &amp;lt; seq_len ? v_vec_ptr[j] : zero_value;
          }
        }

        // 按Vec分段计算和V的乘积：由多个thread计算，后续需要进一步聚合。
        accs[i] += dot(logits_vec, v_vec);
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;等算法以后再进行reduce把结果聚合，得到最后的结果。这个是经典的reduce算法：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// Perform reduction within each warp.
// 经典in wrap规约算法
#pragma unroll
  for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
    float acc = accs[i];
#pragma unroll
    for (int mask = NUM_V_VECS_PER_ROW / 2; mask &amp;gt;= 1; mask /= 2) {
      acc += VLLM_SHFL_XOR_SYNC(acc, mask);
    }
    accs[i] = acc;
  }

  // NOTE(woosuk): A barrier is required because the shared memory space for
  // logits is reused for the output.
  __syncthreads();

  // Perform reduction across warps.
  // 经典树状cross wrap规约算法
  float* out_smem = reinterpret_cast&amp;lt;float*&amp;gt;(shared_mem);
#pragma unroll
  for (int i = NUM_WARPS; i &amp;gt; 1; i /= 2) {
    int mid = i / 2;
    // Upper warps write to shared memory.
    if (warp_idx &amp;gt;= mid &amp;amp;&amp;amp; warp_idx &amp;lt; i) {
      float* dst = &amp;amp;out_smem[(warp_idx - mid) * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        if (row_idx &amp;lt; HEAD_SIZE &amp;amp;&amp;amp; lane % NUM_V_VECS_PER_ROW == 0) {
          dst[row_idx] = accs[i];
        }
      }
    }
    __syncthreads();

    // Lower warps update the output.
    if (warp_idx &amp;lt; mid) {
      const float* src = &amp;amp;out_smem[warp_idx * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        if (row_idx &amp;lt; HEAD_SIZE &amp;amp;&amp;amp; lane % NUM_V_VECS_PER_ROW == 0) {
          accs[i] += src[row_idx];
        }
      }
    }
    __syncthreads();
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;算完以后就是把结果写到output了，这块就不贴了。&lt;/p&gt;
&lt;h2 id=&#34;收尾&#34;&gt;收尾&lt;/h2&gt;
&lt;p&gt;大概流程就是这样，整个流程还是设计不少细节的，我也不知道写清楚没有，不过对着代码多看几遍总能看明白的。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
  </channel>
</rss> 