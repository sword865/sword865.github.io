<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>悟剑阁</title>
    <link>https://sword865.github.io/</link>
    <description>Recent content on 悟剑阁</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Sun, 27 Jul 2025 14:53:26 +0800</lastBuildDate>
    
        <atom:link href="https://sword865.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>Ray与LLM强化学习框架设计</title>
        <link>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</link>
        <pubDate>Sun, 27 Jul 2025 14:53:26 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/ -&lt;p&gt;最近LLM强化学习框架发展特别快，Ray作为被ChatGPT带火的框架，在LLM各个训练阶段中，RL阶段的应用应该是最多的。写篇文章记录一下这块发展的脉络和一些看法。&lt;/p&gt;
&lt;h1 id=&#34;从google-pathways说起&#34;&gt;从Google Pathways说起&lt;/h1&gt;
&lt;p&gt;讨论Ray和RL系统，得从Google的&lt;strong&gt;Pathways&lt;/strong&gt;系统开始：2021年Google提出了Pathways作为下一代AI架构和分布式ML平台，在相关文献中详细讨论了Single-Controller + MPMD的系统设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Single-Controller&lt;/strong&gt;（单控制器）是指用一个中央协调器来管理整个分布式计算流程的架构模式。在这种设计中，有一个&lt;strong&gt;主控制节点&lt;/strong&gt;负责整个计算图的执行，包括任务分发、资源调度、状态监控等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiple-Controller&lt;/strong&gt;（多控制器）则是指使用多个分布式控制节点来协同管理计算任务的架构模式。在这种设计中，没有单一的中央协调器，而是由多个控制器节点分别负责不同的子系统或计算子图，通过分布式协调协议来实现全局一致性。&lt;/p&gt;
&lt;p&gt;在Ray中的Driver Process就可以被作为一个典型的Single Controller来启动不同的任务程序，而通过torchrun运行的PyTorch DDP分布式计算则是在每个node上各自执行自己的程序则属于典型的Multiple Controller范式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MPMD&lt;/strong&gt;（Multiple Program, Multiple Data）是一种分布式计算范式，指在一个计算任务中，不同的节点运行不同的程序来处理不同的数据。这种模式下，各个计算节点执行的代码逻辑可能完全不同，每个节点都有自己特定的任务和职责。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SPMD&lt;/strong&gt;（Single Program, Multiple Data）则是另一种常见的分布式计算范式，指所有节点运行相同的程序，但处理不同的数据分片。&lt;/p&gt;
&lt;p&gt;典型的SPMD任务包括传统的分布式训练，比如PyTorch DDP，每个节点运行相同的程序来处理不同的数据，最多根据rank的值会有一些特别的处理（比如rank=0的节点负责checkpoint）。相比之下，大模型训练包括了流水并行这种更复杂的任务，每个节点组需要运行不同的程序，就更适合用MPMD的方式来实现了。&lt;/p&gt;
&lt;p&gt;一般来说，MPMD系统由于包含众多异构组件，各组件间的协调和同步变得相当复杂。为了简化开发复杂度并确保系统执行的一致性，Single-Controller架构成为了自然的选择——通过引入中心化的控制器来统一管理整个分布式计算流程，包括任务调度、状态同步和异常处理等关键环节。&lt;/p&gt;
&lt;p&gt;更多的细节就不多说了，有兴趣的话可以去看Oneflow团队当年写的两篇文章，非常深刻：&lt;a href=&#34;https://mp.weixin.qq.com/s/roQues5HhRXqGf26DuUOjQ&#34;&gt;解读谷歌Pathways架构（一）：Single-controller与Multi-controller&lt;/a&gt;和&lt;a href=&#34;https://mp.weixin.qq.com/s/N99dRgFYC9zOOcGlg0Ulsw&#34;&gt;解读谷歌 Pathways 架构（二）：向前一步是 OneFlow&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这与LLM强化学习有什么关系呢？用RL训练LLM本质上是一个多阶段、多节点的复杂分布式任务。典型的RLHF流水线涉及多个不同模型，计算流程分为几个关键阶段：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;生成阶段&lt;/strong&gt;：当前策略模型（LLM）对一批输入提示生成响应文本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估阶段&lt;/strong&gt;：这些响应由奖励模型评分，或通过人类/自动化偏好模型进行比较评估&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练阶段&lt;/strong&gt;：基于获得的奖励信号更新策略模型权重（可能还包括价值函数或评论家网络的更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些阶段之间存在明确的数据依赖关系——训练更新必须依赖于生成的样本及其对应的奖励分数。在朴素的实现中，这些阶段只能串行执行，引入大量上下文切换的同时还要求所有的模型使用相同数量的GPU进行计算，计算效率是相当低下的。因此正如Pathways架构所启发的那样，我们希望在保证正确性的前提下，通过良好的系统设计，尽可能地重叠和并行化这些工作阶段，以最大化计算资源的利用效率。&lt;/p&gt;
&lt;p&gt;直接说有点抽象，可以看下面这个从&lt;a href=&#34;http://arxiv.org/abs/2409.19256&#34;&gt;HybridFlow&lt;/a&gt;里截的表格，我截了两个最早的RLHF系统，左边的DeepSpeed-Chat实现了SPMD的串行方式，而右边的OpenRLHF则是典型的MPMD系统。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_SPMD_MPMD.png&#34; class=&#34;center&#34; /&gt;
&lt;h1 id=&#34;ray与llm强化学习框架&#34;&gt;Ray与LLM强化学习框架&lt;/h1&gt;
&lt;p&gt;其实从前面的内容可以看出来，Ray的设计很适合用来开发Single-Controller + MPMD的程序，也就自然适合LLM强化学习的场景了。&lt;/p&gt;
&lt;p&gt;实际上，社区也确实基于Ray开发了大量的强化学习框架，目前主要的设计包括两种：&lt;strong&gt;Colocated架构&lt;/strong&gt;和&lt;strong&gt;Disaggregated架构&lt;/strong&gt;。粗略地说，&lt;strong&gt;Colocated架构&lt;/strong&gt;意味着把生成阶段和训练阶段放在同样的节点上运行；而&lt;strong&gt;Disaggregated架构&lt;/strong&gt;则把它们放在不同的节点上：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RL_architecture.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;一看这个图，我们会发现Disaggregated Architecture中存在大量的计算bubble，甚至可能比不上之前SPMD模式！这也是为什么很多框架如OpenRLHF、Nemo-aligner、VeRL都是按照Colocated架构来设计的。&lt;/p&gt;
&lt;p&gt;需要注意的是，图里的Train和Gen代表的是RLHF的不同阶段，每个阶段内每个GPU可能在运行不同任务，因此整个过程仍然是MPMD的。&lt;/p&gt;
&lt;p&gt;以经典的PPO算法为例，整个Train的阶段包括Actor Model(on training Framework)/Reference Model/Reward Model/Critic Model四个模型，Gen阶段包括Actor Model(on inference framework)一个模型，以&lt;a href=&#34;http://arxiv.org/abs/2405.11143&#34;&gt;OpenRLHF&lt;/a&gt;为例：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_PPO.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;可以看到这里Actor Model会在Deepspeed和vLLM两个引擎间进行切换，因为实际算法需要保存的模型共有5个。&lt;/p&gt;
&lt;h2 id=&#34;colocated-rl框架-解法和问题&#34;&gt;Colocated RL框架 (解法和问题)&lt;/h2&gt;
&lt;p&gt;接下来继续看这个基于Ray的框架：OpenRLHF使用Ray启动和协调组件，但使用Ray的&lt;strong&gt;Placement group&lt;/strong&gt;实现了Colocated架构，在每个节点上在rollout和训练任务之间分割GPU资源。例如，在给定节点上，框架可能将每个GPU的0.75分配给训练actor，0.25分配给生成actor，这样有效地让一个训练进程和一个生成进程&amp;quot;共享&amp;quot;每个GPU而不互相干扰。&lt;/p&gt;
&lt;p&gt;很容易从前面的图看出来，Colocated框架中的&lt;strong&gt;资源共享&lt;/strong&gt;是一个主要的优势，通过设计合适的分组方式，我们可以减少GPU的空闲时间，减少模型offload的频率，同时尽量并行化不同节点的执行，从而最大化提升资源利用效率。&lt;/p&gt;
&lt;p&gt;然而，随着模型大小和集群大小的增长，Colocated框架也显示出了自己的局限性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一个关键问题是&lt;a href=&#34;http://arxiv.org/abs/2504.15930&#34;&gt;StreamRL&lt;/a&gt;中提到的&lt;strong&gt;资源耦合&lt;/strong&gt;。虽然Colocated框架比起SPMD的程序提升了计算任务的并行性，并通过分组来允许每组model使用不同的资源，但是这并不能完全消除共享设备带来的问题：因为生成和训练同时共享相同设备，我们不能独立扩展或为每个阶段定制资源。同时训练任务（计算密集）和生成任务（IO密集）的瓶颈并不相同，这不利于GPU资源的利用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;另一个问题是，LLM生成的文本长度是不固定的，尤其随着thinking model的大火，生成任务中不同组的模型生成结果的时间可能差异很大。比如我们有32块GPU，每4块GPU为一组进行生成，如果其中一组生成任务过长，会导致其他28块GPU空等造成资源浪费。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总的来说，Colocated框架通过精细的资源管理实现了较高的GPU利用率，相对成熟和稳定，确实许多后续框架都借鉴了类似的设计思路。但是，正如前面提到的资源耦合问题，这种架构在可扩展性方面仍有局限。这也为下一代RL框架的发展指明了方向：能否通过打破严格的串行约束，让生成和训练阶段真正独立地并行执行？&lt;/p&gt;
&lt;h2 id=&#34;online-policy和offline-policy&#34;&gt;Online Policy和Offline Policy&lt;/h2&gt;
&lt;p&gt;本文的重点是Ray和LLM RL的框架设计，因此不会在这一部分内容做过多阐述，大概而言：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Online Policy&lt;/strong&gt;: 严格要求使用当前最新策略生成的数据进行训练。在RLHF中，这意味着每次训练迭代都必须等待当前actor模型完成新一轮的文本生成，然后立即使用这些&amp;quot;新鲜&amp;quot;的样本来更新模型权重。这种方式能保证训练数据与当前策略的完全一致性，但代价是强制的同步等待，导致前面提到的大量计算bubble。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Offline Policy&lt;/strong&gt;: 允许使用稍微&amp;quot;陈旧&amp;quot;的策略生成的数据进行训练。在实际系统中，这意味着训练阶段可以使用来自之前几个迭代版本的actor模型生成的样本，而不必严格等待当前版本的生成完成。虽然引入了一定的策略陈旧性（policy staleness），但这种设计允许生成和训练阶段真正并行化，大幅提升系统吞吐量。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Online Policy&lt;/strong&gt;算法正是前文中Disaggregated Architecture存在大量计算空泡的根本原因：训练阶段只能使用最新策略模型生成的样本，这就强制要求生成步骤必须在训练步骤完成后才能开始下一轮迭代。&lt;/p&gt;
&lt;p&gt;从理论角度分析，Online Policy的样本效率确实优于Offline Policy，这主要源于其能够保证训练数据与当前策略分布的完全一致性。在LLM强化学习场景中，策略陈旧性（policy staleness）会引入分布偏移问题——生成数据的策略分布与当前训练策略产生偏差，这种不匹配可能降低训练样本的有效性，并对模型收敛的稳定性和效率产生负面影响。&lt;/p&gt;
&lt;p&gt;不过在RL的工业实践中，这种理论差异往往被系统吞吐量的大幅提升所抵消。现代LLM RL系统通过以下几种方式来缓解策略陈旧性的影响：&lt;strong&gt;增加Experience Buffer的容量、优化batch sampling策略、以及采用更频繁的模型同步机制&lt;/strong&gt;。实际部署中，有团队发现通过合理的超参数调整（如学习率衰减、梯度裁剪），Offline Policy系统在保持训练稳定性的同时，能够获得成倍的训练吞吐量提升。这种工程权衡也推动了RLHF向多轮迭代和DPO向Iterative DPO等更适合并行化的算法变种演进。&lt;/p&gt;
&lt;h2 id=&#34;disaggregated架构-从offline-policy到streaming-rl&#34;&gt;Disaggregated架构 (从offline policy到Streaming RL)&lt;/h2&gt;
&lt;p&gt;自然地，业界开始探索在LLM强化学习中采用Offline Policy算法的可能性。&lt;a href=&#34;http://arxiv.org/abs/2410.18252&#34;&gt;ASYNCHRONOUS RLHF&lt;/a&gt;的研究为这一方向提供了积极的信号，该研究表明适度的&lt;strong&gt;策略陈旧性是可以接受的&lt;/strong&gt;，并且不会显著影响训练效果。这一重要发现随后在&lt;a href=&#34;http://arxiv.org/abs/2504.15930&#34;&gt;StreamRL&lt;/a&gt;的工作中得到了进一步的验证和扩展。
类似的，Meta也在&lt;a href=&#34;http://arxiv.org/abs/2505.24034&#34;&gt;LlamaRL&lt;/a&gt;中提出了下面的计算流，我们得到了新的流式的&lt;strong&gt;Disaggregated架构&lt;/strong&gt;：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_offlineRL.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;因此，我们观察到，最新的LLM强化学习系统正在移向&lt;strong&gt;Disaggregated架构 + Offline Policy&lt;/strong&gt;的实现，这与MPMD范式一致。想法是&lt;strong&gt;将生成和训练阶段分离为不同的服务&lt;/strong&gt;，它们并发在不同的GPU资源池上运行。我们有，比如说，一个专门用于生成新rollout的GPU资源池和另一个专门用于训练任务的资源池。两个任务在不同的GPU资源上并行操作，样本不断从生成端持续流向训练端。&lt;/p&gt;
&lt;p&gt;相比传统的Colocated架构，Disaggregated架构能够真正实现&lt;strong&gt;异构资源的独立扩展&lt;/strong&gt;。举个例子，如果发现生成阶段成为瓶颈（比如复杂推理任务需要更长的生成时间），我们可以单独为生成服务增加更多GPU，而不影响训练集群的配置。相反，如果奖励模型计算或PPO更新成为瓶颈，我们可以针对性地扩展训练侧的资源。这种&lt;strong&gt;弹性伸缩&lt;/strong&gt;的能力在云环境和多租户场景中特别有价值。&lt;/p&gt;
&lt;p&gt;通过接受一点&lt;strong&gt;陈旧策略&lt;/strong&gt;的数据，流式框架大大改善了&lt;strong&gt;资源利用率&lt;/strong&gt;。不需要所有GPU在迭代之间同步暂停；相反，生成和训练都可以达到稳定的吞吐量。StreamRL的设计明确解决了困扰同步设计的流水线&amp;quot;空泡&amp;quot;（空闲间隙）和落后任务的长尾。它们实现了完全异步流水线，其中权重更新、生成和训练尽可能重叠。此外，流式框架通常引入弹性优势：可以独立扩展生成服务和训练器服务。例如，如果生成长序列是慢的部分，系统可以为流生成服务分配更多GPU而不增加训练器GPU。或者相反，如果发现奖励模型评分是瓶颈，就扩展那个。这种解耦在多租户或云环境中很强大。&lt;/p&gt;
&lt;p&gt;更进一步，为了更好的对训练集群和推理集群进行协调，我们可以引入通过某种数据缓冲区或队列连接来作为两个集群的交互点。这种设计可以实现生成和训练的&lt;strong&gt;近乎完美重叠&lt;/strong&gt;，从而消除流水线中的大部分空闲时间。这里的一个例子是&lt;a href=&#34;http://arxiv.org/abs/2507.01663&#34;&gt;AsyncFlow&lt;/a&gt;，这个框架基于&lt;code&gt;TransferQueue&lt;/code&gt;来传输数据，并控制训练的执行。&lt;/p&gt;
&lt;img width=&#34;600&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/stream_RL_with_Buffer.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;总之，流式RL框架将RL循环解耦为独立组件并利用异步执行来提升吞吐量。它们通过避免资源耦合并允许更细粒度的可扩展性来解决协同定位的缺点。成本是增加的系统复杂性和需要处理离线策略训练——但通过仔细设计（如AsyncFlow、StreamRL等），这些问题是可管理的。&lt;/p&gt;
&lt;h1 id=&#34;无ray的rl&#34;&gt;无Ray的RL？&lt;/h1&gt;
&lt;p&gt;随着LLM强化学习框架的不断演进，似乎有了一个新的趋势：框架&lt;strong&gt;对Ray的依赖正在逐渐减少&lt;/strong&gt;。回顾一下，Ray最初是这些框架快速原型化的理想选择——它能够轻松处理集群配置、进程启动，并提供简洁的远程函数调用和Actor类API。事实上，像OpenRLHF这样的早期项目，中期的VeRL，以及新的Slime和AsyncFlow，都将Ray作为关键的&amp;quot;胶水层&amp;quot;来协调复杂的训练循环。&lt;/p&gt;
&lt;p&gt;然而，随着这些系统在生产环境中的大规模部署，Ray的一些固有限制开始显现：&lt;/p&gt;
&lt;h2 id=&#34;ray的技术挑战&#34;&gt;Ray的技术挑战&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;调试复杂性&lt;/strong&gt;是Ray面临的首要问题。当某个远程Worker深层出现异常时，你往往只能收到一个经过序列化传输的模糊错误信息，很难追踪问题的真正根源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;通信开销&lt;/strong&gt;同样是Ray架构中的一个关键瓶颈。Ray的核心设计依赖Python对象序列化和gRPC通信，这在RLHF场景下面临巨大压力。想象一下，每个experience batch可能包含数万个token的生成文本、完整的logits分布、以及各种奖励和价值函数的输出——这些数据的体积是相当可观的。当这样的大负载需要在生成阶段和训练阶段之间频繁传输时，序列化开销就变得不可忽视了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然Ray社区在通过各种优化手段（如&lt;a href=&#34;https://mp.weixin.qq.com/s/KehYVdkdEC-9H7jaDMIlow&#34;&gt;Ray Flow Insight&lt;/a&gt;、&lt;a href=&#34;https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html&#34;&gt;Compiled Graph&lt;/a&gt;和&lt;a href=&#34;https://github.com/ray-project/ray/issues/51173&#34;&gt;GPU Objects&lt;/a&gt;等）尝试缓解这些问题，但目前这些技术并不成熟也不够好用。&lt;/p&gt;
&lt;h2 id=&#34;缓冲区驱动的无ray架构趋势&#34;&gt;缓冲区驱动的无Ray架构趋势&lt;/h2&gt;
&lt;p&gt;鉴于这些问题，我们已经在社区看到了很多关于&lt;strong&gt;无Ray&lt;/strong&gt;设计的讨论。Meta的LlamaRL完全建立在原生PyTorch上并在&lt;strong&gt;405B&lt;/strong&gt;的模型上做了验证。其他的框架也会有一些关于去Ray化的讨论，比如&lt;a href=&#34;https://github.com/volcengine/verl/discussions/2202&#34;&gt;VeRL&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;那么这个是否可行呢？仔细想想RL框架的发展趋势，新的Streaming based RL框架的核心概念&lt;strong&gt;数据缓冲区&lt;/strong&gt;——本质上是一个专用模块，它保存当前rollout集合并将它们提供给训练工作器。我们可以将其视为经验的消息队列或共享内存。框架在生成任务中不断将数据放入缓冲区，并在训练任务中不断从缓冲区拉取数据进行消费。两个阶段的解耦程度已经非常高了，我们似乎并不需要一个复杂的调度层来管理所有的任务。&lt;/p&gt;
&lt;p&gt;不过，完全抛弃Ray也不是没有成本的。有些框架还是选择留着Ray，主要是因为它在一些地方确实挺好用的：&lt;strong&gt;集群管理、进程启动、故障恢复和资源调度&lt;/strong&gt;。拿字节跳动的HybridFlow团队来说，他们当时也想过用PyTorch原生的TorchRPC来替换Ray，而且确实也跑起来了。但真正部署的时候，他们发现TorchRPC的维护并不是很积极，还会有一些奇怪的边界情况问题：虽然通过精心的工程设计，我们可以得到更好的性能，但Ray在构建稳定的分布式应用时确实能省不少开发和维护的功夫。&lt;/p&gt;
&lt;p&gt;此外，我觉得另一个问题是训练的规模，随着后训练算力的投入，Ray的核心作用也许会从控制流编排转向容错和动态的资源分配管理：RL比起预训练要灵活得多，我们在预训练中都有通过Ray来优化训练稳定性的例子（虽然用得不多），那么在RL中这个作用应该会变得愈加明显。不过可惜的是这种只有头部公司才玩得起了。&lt;/p&gt;
&lt;h1 id=&#34;写在最后&#34;&gt;写在最后&lt;/h1&gt;
&lt;p&gt;从技术演进的角度来看，LLM强化学习框架在过去一年多的发展轨迹清晰地展现了分布式系统设计的经典权衡。&lt;/p&gt;
&lt;p&gt;早期的Colocated架构通过资源共享实现了较高的GPU利用率，但随着模型规模和集群规模的增长，资源耦合问题逐渐凸显。Streaming RL的出现标志着系统设计思路的根本性转变——通过系统和算法的co-design，从on-policy策略向off-policy策略迁移，通过接受有限的策略陈旧性来换取更好的可扩展性和资源利用效率。&lt;/p&gt;
&lt;p&gt;Ray作为这一代框架的重要基础设施，在快速原型开发和集群管理方面发挥了关键作用。然而，随着系统复杂度的提升和性能要求的提高，其在调试复杂性和通信开销方面的局限性也日益明显。这促使社区开始探索更加专门化的解决方案，包括基于原生PyTorch的实现和混合架构设计。&lt;/p&gt;
&lt;p&gt;从系统架构的发展趋势来看，未来的LLM强化学习框架可能会继续朝着更细粒度的解耦和专门化方向发展。核心计算组件将更多地依赖高效的原生实现，而集群管理和容错机制则可能继续依托成熟的分布式框架。这种分层设计既能满足性能要求，又能保持系统的可维护性。&lt;/p&gt;
&lt;p&gt;值得注意的是，这个领域的快速迭代反映了LLM训练工程化的不断成熟。随着模型规模和训练复杂度的持续增长，我们预期会看到更多针对特定场景优化的专门化框架，以及更加标准化的系统接口和协议。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Ray Data反压机制</title>
        <link>https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/</link>
        <pubDate>Sat, 05 Jul 2025 15:03:26 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/ -&lt;p&gt;做Ray Platform也快2年了，遇到过各种的问题，整理一些踩过的坑看一下。&lt;/p&gt;
&lt;p&gt;先从我们自己最常用的Ray Data开始，看看最常见的OOM/OOD问题，这个问题很多时候都是和反压相关的。&lt;/p&gt;
&lt;p&gt;说是Ray Data，不过这里的反压不止一层，大概包括下面几个地方：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ray Core Generator&lt;/strong&gt;：针对Ray Generators的控制，防止后台生成的数据过多导致OOM/OOD。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Executor + Resource Allocator&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;针对正在执行的任务，控制生成结果的速度，避免单个任务生成的数据过多导致OOM/OOD。&lt;/li&gt;
&lt;li&gt;针对单个Operator，控制提交任务的数量，避免在资源紧张时提交新任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backpressure Policies&lt;/strong&gt;: 其他关于任务提交的反压规则。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面我们逐层分析这些机制的实现。&lt;/p&gt;
&lt;h1 id=&#34;ray-core-generator对象数量反压&#34;&gt;Ray Core Generator：对象数量反压&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/ray-core/ray-generator.html&#34;&gt;Ray Generator&lt;/a&gt; 类似Python Generator，用来作为迭代器进行遍历，但是和Python Generator有一个很大的不同在于：Ray Generator使用&lt;code&gt;ObjectRefGenerator&lt;/code&gt;在后台持续执行。也就是说如果Ray Data的单个read_task需要读取一个很大的文件时，没法通过控制拉取任务产出的速度来控制任务的内存占用。（不管下游是否主动拉取，都会持续读取新的数据block。）&lt;/p&gt;
&lt;p&gt;针对这个问题，Ray Generators支持手动配置一个threshold(_generator_backpressure_num_objects parameter)来对Generators进行反压。&lt;/p&gt;
&lt;p&gt;核心逻辑在&lt;code&gt;task_manager.cc&lt;/code&gt;中的&lt;code&gt;HandleReportGeneratorItemReturns&lt;/code&gt;这个方法里面。这个函数逻辑比较复杂，里面还有比如乱序/幂等等问题的处理，我们只看反压状态的管理：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 请求的item的index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; item_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; request.item_index();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 生成器已生产的对象数量
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; total_generated &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.TotalNumObjectWritten();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;//已被消费的对象数量  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; total_consumed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.TotalNumObjectConsumed();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// item已经被消费了，说明消费速度足够快，不用反压。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.IsObjectConsumed(item_index)) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;OK(), total_consumed);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; false;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Otherwise, follow the regular backpressure logic.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// NOTE, here we check `item_index - last_consumed_index &amp;gt;= backpressure_threshold`,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// instead of the number of unconsumed items, because we may receive the
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// `HandleReportGeneratorItemReturns` requests out of order.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (backpressure_threshold &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      (item_index &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.LastConsumedIndex()) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; backpressure_threshold) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    RAY_LOG(DEBUG) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Stream &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; generator_id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; is backpressured. total_generated: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; total_generated
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;. total_consumed: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; total_consumed
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;. threshold: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; backpressure_threshold;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; signal_it &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ref_stream_execution_signal_callbacks_.find(generator_id);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (signal_it &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; ref_stream_execution_signal_callbacks_.end()) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;NotFound(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Stream is deleted.&amp;#34;&lt;/span&gt;), &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      signal_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.push_back(execution_signal_callback);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// No need to backpressure.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;OK(), total_consumed);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;所以未消费对象数量达到阈值时，Ray Generator会暂停任务执行。&lt;/p&gt;
&lt;p&gt;在Ray Data中，taskpool和actor pool都默认设置了&lt;code&gt;_generator_backpressure_num_objects&lt;/code&gt;参数来控制数据的生成，以&lt;code&gt;TaskPoolMapOperator&lt;/code&gt;为例：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;_generator_backpressure_num_objects&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dynamic_ray_remote_args
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_context&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_max_num_blocks_in_streaming_gen_buffer &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;# 2 objects for each block: the block and the block metadata.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            dynamic_ray_remote_args[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;_generator_backpressure_num_objects&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data_context&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_max_num_blocks_in_streaming_gen_buffer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;streaming-executor--resource-allocator&#34;&gt;Streaming Executor + Resource Allocator&lt;/h1&gt;
&lt;p&gt;虽然Ray Core提供了基础反压的接口，但是运行Ray Data任务的时候，还是有其他问题，其中最核心的问题就是&lt;em&gt;是否需要消费上游算子生成的结果&lt;/em&gt;？&lt;/p&gt;
&lt;h2 id=&#34;预算分配&#34;&gt;预算分配&lt;/h2&gt;
&lt;p&gt;Ray使用了预算预分配的方式，给Ray Data任务的每个operator都分配了一个预算，这个预算包括2部分：&lt;/p&gt;
&lt;h3 id=&#34;reserved_for_op_outputs&#34;&gt;&lt;code&gt;reserved_for_op_outputs&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;为算子输出数据预留的内存空间。&lt;/li&gt;
&lt;li&gt;用来保证有足够的内存来存储算子的输出数据，防止所有预算都被pending task outputs占用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;_op_reserved和_op_budgets&#34;&gt;&lt;code&gt;_op_reserved&lt;/code&gt;和&lt;code&gt;_op_budgets&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;_op_reserved&lt;/code&gt;：每个算子的预留资源。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;_op_budgets&lt;/code&gt;: 根据实际情况算出来的，算子可以使用的资源，大致上&lt;code&gt;op_budgets[op] = max(_op_reserved[op] - 当前使用量, 0) + 分配的共享资源&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;预算分配的逻辑在&lt;code&gt;resource_manager.py&lt;/code&gt;里，整个逻辑大概包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;把整个object store分为reserved资源(&lt;code&gt;op_total_reserved&lt;/code&gt;)和shared资源(&lt;code&gt;_total_shared&lt;/code&gt;)两部分。&lt;/li&gt;
&lt;li&gt;给每个算子分配一个初始的budget(&lt;code&gt;op_total_reserved&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;把budget分成2份：&lt;code&gt;reserved_for_op_outputs&lt;/code&gt;和&lt;code&gt;_op_reserved&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;根据算子实际使用的内存情况，计算每个算子剩余的budget数量。（从&lt;code&gt;_op_reserved&lt;/code&gt;得到&lt;code&gt;_op_budgets&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;把共享资源按需分配到各个算子的&lt;code&gt;_op_budgets&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;特殊算子处理：对materializing算子如AllToAllOperator不做任何限制。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;单个task生成速度的控制&#34;&gt;单个Task生成速度的控制&lt;/h3&gt;
&lt;p&gt;有了budget以后，就可以对Ray Data中的每个算子进行反压了，先看正在执行的Ray Generator Task的反压：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 对有结果产生的任务，计算还可以输出的bytes，控制任务输出。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; task &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; ready_tasks:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    bytes_read &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; task&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;on_data_ready(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_bytes_to_read_per_op&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(state, &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; state &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; max_bytes_to_read_per_op:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_bytes_to_read_per_op[state] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; bytes_read
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;code&gt;on_data_ready&lt;/code&gt;会从Ray Generator消费数据，并且一旦消费的数据量达到预算限制就会停止消费：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;on_data_ready&lt;/span&gt;(self, max_bytes_to_read: Optional[int]) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; int:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;当数据准备就绪时的回调&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    bytes_read &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; max_bytes_to_read &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; bytes_read &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; max_bytes_to_read:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            block_ref &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_streaming_gen&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_next_sync(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; block_ref&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;is_nil():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;StopIteration&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_task_done_callback(&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# 处理数据块并累计读取字节数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# bytes_read += process_block(block_ref)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; bytes_read
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;预算的限制则来自&lt;code&gt;max_task_output_bytes_to_read&lt;/code&gt;，计算逻辑就是分配的资源减去使用的资源。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;max_task_output_bytes_to_read&lt;/span&gt;(self, op: PhysicalOperator) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; Optional[int]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_op_budgets[op]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;object_store_memory
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Add the remaining of `_reserved_for_op_outputs`.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        op_outputs_usage &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_get_op_outputs_usage_with_downstream(op)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        res &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; max(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_reserved_for_op_outputs[op] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; op_outputs_usage, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isinf(res):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# corner case的处理，略。        &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; res        
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这样就控制了每个task的Generator的消费速度，防止任何单个操作符占用过多内存。&lt;/p&gt;
&lt;h2 id=&#34;task提交速度的控制&#34;&gt;Task提交速度的控制&lt;/h2&gt;
&lt;p&gt;除了限制单个任务的消费，Ray Data还会控制任务的提交，即在算子budget不足时停止提交该算子的任务。&lt;/p&gt;
&lt;p&gt;这块逻辑比较简单，由streaming executor的&lt;code&gt;select_operator_to_run&lt;/code&gt;方法控制&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ops &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; op, state &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; topology&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; resource_manager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;op_resource_allocator_enabled(), topology
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        under_resource_limits &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            resource_manager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;op_resource_allocator&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;can_submit_new_task(op)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        in_backpressure &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; under_resource_limits &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; any(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;can_add_input(op) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; backpressure_policies
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;code&gt;can_submit_new_task&lt;/code&gt;就是在判断是否有足够的资源可以提交新的任务。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;can_submit_new_task&lt;/span&gt;(self, op: PhysicalOperator) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; bool:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; op &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_op_budgets:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        budget &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_op_budgets[op]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; op&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;incremental_resource_usage()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;satisfies_limit(budget)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; res
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;backpressure-policies-其他关于任务提交的反压规则&#34;&gt;&lt;strong&gt;Backpressure Policies&lt;/strong&gt;: 其他关于任务提交的反压规则。&lt;/h1&gt;
&lt;p&gt;最后一个&lt;code&gt;Backpressure Policies&lt;/code&gt;其实就是前面&lt;code&gt;select_operator_to_run&lt;/code&gt;方法里提到的&lt;code&gt;backpressure_policies&lt;/code&gt;了：&lt;/p&gt;
&lt;p&gt;回顾一下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        in_backpressure &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; under_resource_limits &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; any(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;can_add_input(op) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; backpressure_policies
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里目前其实只有一个策略，就是并发度的控制策略，没什么好说的，就是看一下正在运行的任务数量是否达到设置的并发上限。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ConcurrencyCapBackpressurePolicy&lt;/span&gt;(BackpressurePolicy):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;A backpressure policy that caps the concurrency of each operator.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    The policy will limit the number of concurrently running tasks based on its
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    concurrency cap parameter.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    NOTE: Only support setting concurrency cap for `TaskPoolMapOperator` for now.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    TODO(chengsu): Consolidate with actor scaling logic of `ActorPoolMapOperator`.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# .....&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;can_add_input&lt;/span&gt;(self, op: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;PhysicalOperator&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; bool:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; op&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;metrics&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_tasks_running &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_concurrency_caps[op]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;- https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Flash MLA Kernel分析</title>
        <link>https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/</link>
        <pubDate>Sat, 03 May 2025 15:51:35 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/ -&lt;p&gt;准备对DeepSeek的开源项目整理一些文档，也顺便强化一下记忆，先从FlashMLA开始。&lt;/p&gt;
&lt;p&gt;FlashMLA是DeepSeek开源的MLA算子实现，这个实现主要给inference decoding用的，Training和prefill应该是另外一个算子。&lt;/p&gt;
&lt;p&gt;先拿下面的图表示一下MLA算子是在计算一个什么东西，这篇文章就不讲具体的推导了，反正这个算子大概就是下面的2个GEMM算子的融合。需要注意的是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这里矩阵K和矩阵V的共享一部分参数。&lt;/li&gt;
&lt;li&gt;图里只画显示了一个Query Head和一对KV Head的计算。在实际计算中还要num_kv_head和batch_size两个维度。&lt;/li&gt;
&lt;li&gt;两个GEMM中间其实还有一个sotfmax，不过这里可以通过online softmax算法把这块逻辑独立处理分块处理，所以不影响主流程。&lt;/li&gt;
&lt;/ol&gt;
&lt;img width=&#34;600&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/mla.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;Kernel的调用主要分两部分&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调用&lt;code&gt;get_mla_metadata&lt;/code&gt;来计算一些metadata，用来优化kernel的执行&lt;/li&gt;
&lt;li&gt;调用&lt;code&gt;flash_mla_with_kvcache&lt;/code&gt;进行计算&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在进入调用前，先大概说一下FlashMLA计算的拆分逻辑。这块和FlashDecoding很像，并没有要求一个thread-block必须处理一个完整的sequence，而是通过一个负载均衡算法，把所有的sequence放到一起，然后拆分成一个个的sequence-block，然后每个thread-block就去处理分配给它的那些block的计算，最后再把这些thread-block的结果用合并，得到正确的输出。&lt;/p&gt;
&lt;p&gt;大概是下面这个图的样子：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/computation-pattern.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;所以为了完成计算，第一步就是决定每个block需要处理哪些sub-sequence，也就是&lt;code&gt;get_mla_metadata&lt;/code&gt;要完成的事情。&lt;/p&gt;
&lt;h1 id=&#34;get_mla_metadata&#34;&gt;get_mla_metadata&lt;/h1&gt;
&lt;p&gt;先看&lt;code&gt;get_mla_metadata&lt;/code&gt;具体提供了哪些元数据，我们从repo提供的测试代码入手，考虑最简单的情况(batch_size=128, query_sequence_len=1, mean_key_sequence_len=4096, MTP=1, num_kv_head=1, num_q_head=16, TP=1, hidden_NoRoPE_dim=512, hidden_RoPE_dim=64, varlen=False)。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# cache_seqlens = tensor([4096, 4096, ..., 4096], dtype=torch.int32), size=batch_size, value=sequence_len
# s_q=1 (query_sequence_len=1且MTP=1), h_q(num_q_head)=128 (TP=1=128/128) h_kv(num_kv_head)=1
# 基于这些配置，计算mla kernel的metadata
tile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为这里我们是在测试decoding步骤，所以有&lt;code&gt;query_sequence_len=1&lt;/code&gt;，可以看到三个入参：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;kv cache的大小&lt;/li&gt;
&lt;li&gt;类似GQA的Group数量，这个参数表示每个kv head对应多少个query head。&lt;/li&gt;
&lt;li&gt;kv head的数量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;get_mla_metadata&lt;/code&gt;会根据GPU中SM的数量和要处理的数据的大小，给每个SM分配任务。这个注意&lt;code&gt;get_mla_metadata_kernel&lt;/code&gt;的参数为&lt;code&gt;&amp;lt;&amp;lt;&amp;lt;1, 32, 0, stream&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;，因此所有计算会在1个warp中完成。&lt;/p&gt;
&lt;p&gt;这里的关键就是具体怎么给每个(每组)SM分配工作的.&lt;/p&gt;
&lt;p&gt;首先，每几个SM会一起处理一个kv head和一组query head的计算：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;int num_sm_parts = sm_count / num_heads_k / cutlass::ceil_div(num_heads_per_head_k, block_size_m);
&lt;/code&gt;&lt;/pre&gt;&lt;img width=&#34;600&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/flash_mla_sm_part.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;然后，我们计算每组SM需要处理多少个block，然后把block分配到每一个SM，具体任务的分配过程为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据batch size和&lt;code&gt;mean_key_sequence_len&lt;/code&gt;计算出一共有多少个block。&lt;/li&gt;
&lt;li&gt;给每个SM分配工作，包括每个SM要处理的tile的索引和位置。&lt;/li&gt;
&lt;li&gt;记录一下每个sequnce的切分点的位置，用于在计算时把结果正确的合起来才能得到完整的注意力输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OK, 这样我们就完成了对任务的划分，接下来进入关键的计算kernel。&lt;/p&gt;
&lt;h1 id=&#34;flash_mla_with_kvcache&#34;&gt;flash_mla_with_kvcache&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;flash_mla_with_kvcache&lt;/code&gt;函数内部其实也是由2个子kernel组成的&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;flash_fwd_splitkv_mla_kernel&lt;/code&gt;: 通过for循环的方式，计算每个SM分配到的block的GEMM乘法。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;flash_fwd_splitkv_mla_combine_kernel&lt;/code&gt;: 负责把多个block的计算结果合起来，得到最终的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;flash_fwd_splitkv_mla_kernel&#34;&gt;flash_fwd_splitkv_mla_kernel&lt;/h2&gt;
&lt;p&gt;先看&lt;code&gt;flash_fwd_splitkv_mla_kernel&lt;/code&gt;，这个kernel包括&lt;code&gt;num_m_block * num_query_head * num_sm_parts&lt;/code&gt; 个thread-block。其中&lt;code&gt;num_m_block=seqlen_q/block_size_m(64)&lt;/code&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kernel&amp;lt;&amp;lt;&amp;lt;dim3(num_m_block, params.h, params.num_sm_parts), Kernel_traits::kNThreads, smem_size, stream&amp;gt;&amp;gt;&amp;gt;(params);&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意，这里的&lt;code&gt;seqlen_q&lt;/code&gt;并不是一开始的1了，实际上它等于&lt;code&gt;num_heads_per_head_k (seqlen_q = seqlen_q_ori * ngroups, 在MTP=1的情况下等于num_heads_per_head_k)&lt;/code&gt;
这样我们会发现：&lt;code&gt;num_m_block=cutlass::ceil_div(num_heads_per_head_k, block_size_m);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;回忆之前的SM分组公式，有&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SM数量 = num_sm_parts * num_heads_k * ceil_div(num_heads_per_head_k, block_size_m)
       = num_sm_parts * ceil_div(num_heads_k * num_heads_per_head_k, block_size_m) 
       = num_sm_parts * ceil_div(num_query_head, block_size_m)
       = num_sm_parts * num_m_block
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因此SM的数量对应了thread-block的第一维和最后一维。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dim3(num_m_block, params.h, params.num_sm_parts)&lt;/code&gt;的这三个维度分别表示：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这个thread-block处理哪一个block。&lt;/li&gt;
&lt;li&gt;这个thread-block应该处理哪的一个query head。&lt;/li&gt;
&lt;li&gt;这个thread-block在对应的SM Group内的编号。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们来看每个thread-block会计算什么，我们知道多个thread-block会共同完成分配给一个SM的block的计算。
看代码发现这里其实有2重循环：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;外层循环会遍历所以分配给这个SM的query block。&lt;/li&gt;
&lt;li&gt;内层循环会遍历对应的KV cache block，计算出O的一个block。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用了Warp Specialization的策略，通过生产者&amp;ndash;&amp;gt;消费者的方式进行计算。
&lt;ul&gt;
&lt;li&gt;Warp Group 1：主要计算线程，负责大部分的注意力得分计算。&lt;/li&gt;
&lt;li&gt;Warp Group 2：使用double buffer的技术进行数据的加载，也参与一些计算。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这块代码比较复杂的，zhihu上有几篇文章写的挺清楚的，我就不一点点写分析了，画个图过来表示一下计算过程，对细节感兴趣的可以去看后面的几个参考的文章。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/flashmla_wap_spec.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;这里可以看到，Warp Group 0会计算GEMM1，但是GEMM2是由两个Warp Group共同计算的，每个Wrap计算其中一半。&lt;/p&gt;
&lt;p&gt;这里比较重要的几块逻辑：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Warp Specialization&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  int warp_group_idx = cutlass::canonical_warp_group_idx();
    if (warp_group_idx == 0) {
        // 主要计算逻辑，包括矩阵乘法、归一化、概率矩阵的计算和输出
        // thread 0 - 127
        ....
    } else {
       // 主要负责加载数据
       // thread 128 - 256
    }
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;上面else逻辑中的双缓冲&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 双缓冲区数据结构定义
template&amp;lt;typename Kernel_traits&amp;gt;
struct SharedStorageMLA {
    union {
        struct {
               // 存储 Query、Key 和中间结果
              ...
              cute::array_aligned&amp;lt;typename Kernel_traits::Element, 
                  cute::cosize_v&amp;lt;typename Kernel_traits::SmemLayoutK&amp;gt; * 2&amp;gt; smem_k;  // Double buffer
              ...
        }
        ...
    }
}
...

 // 双缓冲策略(在warp group 1的代码里)：切换到第二个缓冲区
if (n_block % 2 == 1) {           
       // Double buffer for sK
       constexpr int sK_offset = size(sK);
       tSrK.data() = tSrK.data() + sK_offset / 8;
       tOrVt.data() = tOrVt.data() + sK_offset / 8;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;flash_fwd_splitkv_mla_combine_kernel&#34;&gt;flash_fwd_splitkv_mla_combine_kernel&lt;/h2&gt;
&lt;p&gt;最后的&lt;code&gt;flash_fwd_splitkv_mla_combine_kernel&lt;/code&gt;比较简单，就是负责数据的合并：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在Warp 0中计算各个block的Log-Sum-Exp最大值，获取全局归一化系数。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for (int i = 0; i &amp;lt; kNLsePerThread; ++i) max_lse = max(max_lse, local_lse[i]);
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;在Warp 0中计算缩放因子。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for (int i = 0; i &amp;lt; kNLsePerThread; ++i) {
       const int split = i * 32 + tidx;
       if (split &amp;lt; actual_num_splits) sLseScale[split] = expf(local_lse[i] - global_lse);
}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;按照缩放因子，合并Output的输出，完成计算。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for (int split = 0; split &amp;lt; actual_num_splits; ++split) {
       ...
       ElementAccum lse_scale = sLseScale[split];
       for (int i = 0; i &amp;lt; size(tOrO); ++i) {
              tOrO(i) += lse_scale * tOrOaccum(i);
        }
        ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;把结果写回全局内存。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;参考文章&#34;&gt;参考文章：&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.04434&#34;&gt;DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/26269071923&#34;&gt;DeepSeek: FlashMLA代码解析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/26080342823&#34;&gt;flashMLA 深度解析&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
- https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>vLLM Paged Attention代码分析</title>
        <link>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
        <pubDate>Sun, 20 Apr 2025 15:51:35 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ -&lt;p&gt;3月底整理了一个关于经典Paged Attention算法的ppt, 想起这个几年没写过的blog，把PPT改成一篇文章证明我还活着(-_-)。&lt;/p&gt;
&lt;img width=&#34;500&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/paged_attention.png&#34; class=&#34;center&#34; /&gt;
&lt;h2 id=&#34;vllm-的-paged-attention&#34;&gt;vLLM 的 Paged Attention&lt;/h2&gt;
&lt;p&gt;开始前先说明一下，vLLM里的Paged Attention Kernel是有好几个不同的版本的，大概是下面这样子：&lt;/p&gt;
&lt;p&gt;vLLM早期版本：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prefilling -&amp;gt; Flash Attention的flash_attn_varlen_func&lt;/li&gt;
&lt;li&gt;Dedocding -&amp;gt; 自己实现的Paged Attention
&lt;ul&gt;
&lt;li&gt;paged_attention_v1 : 用于比较短的sequence&lt;/li&gt;
&lt;li&gt;paged_attention_v2 : 用于不想用v1的情况 :)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;源码大概是这样的：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    # NOTE(woosuk): We use a simple heuristic to decide whether to use
    # PagedAttention V1 or V2. If the number of partitions is 1, we use
    # V1 to avoid the overhead of reduction. Also, if the number of
    # sequences or heads is large, we use V1 since there is enough work
    # to parallelize.
    # TODO(woosuk): Tune this heuristic.
    # For context len &amp;gt; 8192, use V2 kernel to avoid shared memory
    # shortage.
    use_v1 = (max_seq_len &amp;lt;= 8192 and (max_num_partitions == 1 or num_seqs * num_heads &amp;gt; 512))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;vLLM 最新版本就已经全部转向Flash Attention， 用cutlass实现了。&lt;/p&gt;
&lt;h2 id=&#34;nvidia-gpu-基础&#34;&gt;NVIDIA GPU 基础&lt;/h2&gt;
&lt;p&gt;在深入 Paged Attention 的实现之前，我们需要了解 NVIDIA GPU的基本架构。（这里我们主要讲A100）&lt;/p&gt;
&lt;p&gt;在做开发时，GPU 的 CUDA 程序包括 Grid -&amp;gt; Thread Block -&amp;gt; Threads三层架构。
这三层架构对应到GPU的硬件：GPU -&amp;gt; SM -&amp;gt; Cuda Core&lt;/p&gt;
&lt;p&gt;在实际执行的时候，Threads会以每32个为一组执行，因此这里还多了一层：Thread Wrap，因此结构变成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CUDA程序：Grid -&amp;gt; Thread Block -&amp;gt; Thread Wrap -&amp;gt; Threads四层架构。&lt;/li&gt;
&lt;li&gt;GPU硬件：GPU -&amp;gt; SM(多次执行) -&amp;gt; SM(一次执行) -&amp;gt; Cuda Core，也是四层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 A100 GPU 上，我们有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;108个SM&lt;/li&gt;
&lt;li&gt;每个SM有4个Wrap scheduler
&lt;ul&gt;
&lt;li&gt;最多有4个Thread Wrap同时在一个SM上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;每个Wrap scheduler有一个长度为16的调度队列
&lt;ul&gt;
&lt;li&gt;一个SM上最多可以调度64个Thread Wrap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基本上这些数字在设计Kernel的时候都可以被考虑到，从而最大化一个Kernel的硬件利用率。&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/nvida_gpu.png&#34; class=&#34;center&#34; /&gt;
&lt;h2 id=&#34;vllm-kernel-映射&#34;&gt;vLLM Kernel 映射&lt;/h2&gt;
&lt;p&gt;现在我们看一下vLLM Kernel的设计：(处于简化的目的，我们认为没有TP)&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/vllm_kernel_map.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;设计Kernel的第一步是把程序拆分成不同的Thread Block来简化问题，vLLM中每个Thread Block会负责1个Query Token的一个Query Head的计算。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个设计其实比较粗糙。不过没关系，Flash Attention里有更多优化。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;设计好计算粒度后，是内存布局的优化，vLLM的Kernel对Q，K, V使用了不同的内存布局，看代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    const scalar_t* __restrict__ q,       // [num_seqs, num_heads, head_size]
    const cache_t* __restrict__ k_cache,  // [num_blocks, num_kv_heads,
                                          // head_size/x, block_size, x]
    const cache_t* __restrict__ v_cache,  // [num_blocks, num_kv_heads,
                                          // head_size, block_size]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;内存设计的时候，我们一般要考虑的是如何能够更好的做到：1. 每次读取连续的内存块(向量读指令，最好能翻译成_ld.global.b128，以128bit也就是16Bytes为单位)；2. 降低不同thread的读冲突。QKV的内存布局基本上也是考虑这些来做的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q的布局是最简单的，序列长度-&amp;gt;head数量-&amp;gt;head维度。&lt;/li&gt;
&lt;li&gt;K的布局比较复杂，最外层的num_block和num_kv_heads比较好理解，对应了一块KV Cache。但是在没有按照head_size连续存储，还引入了一个参数x。这个布局其实是为了优化K的读取效率，我们在后面再讲。&lt;/li&gt;
&lt;li&gt;最后是V的布局，比K要直接一些，没有额外的维度x。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于这个设计，在列一下相关的代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // 我们希望能用一个thread wrap一次处理一个KV cache block，这里block_size一般是4/8/16这样子。 
  // 因为wrap_size=32，大于cache block size，我们就可以给一个token多分配几个thread来加速计算。
  // 用wrap size 处理 cache_block_size，这样就知道一个wrap thread可以用几个Thread来处理一个token。
  // 这里数字被记作thread group size
  [[maybe_unused]] int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
  assert(head_size % thread_group_size == 0);
  ...
  # 这里的Num threads是128，最后算出来4个wrap，应该也是对应了A100个一个SM有4个wrap scheduler。
  constexpr int NUM_WARPS = NUM的_THREADS / WARP_SIZE;
  ...
  # 没啥好说的，就是一个thread block用128个thread，处理一个token的一个head
  dim3 grid(num_heads, num_seqs, 1);
  dim3 block(NUM_THREADS);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;随着Thread Group的提出，这个CUDA Kernel的架构变的更复杂了 :(&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CUDA程序：Grid -&amp;gt; Thread Block -&amp;gt; Thread Wrap -&amp;gt; Thread Group -&amp;gt; Threads五层架构。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;query数据访问&#34;&gt;Query数据访问&lt;/h2&gt;
&lt;p&gt;讲了这么多终于开始计算了，先拿张图演示一下Query Token的访问：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/query_io.png&#34; class=&#34;center&#34; /&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);

  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;

  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
#pragma unroll
  for (int i = thread_group_idx; i &amp;lt; NUM_VECS_PER_THREAD;
       i += NUM_THREAD_GROUPS) {
    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
    q_vecs[thread_group_offset][i] =
        *reinterpret_cast&amp;lt;const Q_vec*&amp;gt;(q_ptr + vec_idx * VEC_SIZE);
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对着这块代码讲一下：
可以看到这里因为一个block是专门处理一个token的一个head的，因此把这块数据存在了shared memory里，这样方便复用。&lt;/p&gt;
&lt;p&gt;然后这里又引入了一个叫VEC_SIZE的东西，这里其实就是说如果我想一次读16Bytes(_ld.global.b128), 那每个thead一次要读几个元素(因为一共有thread group个线程一起读)。&lt;/p&gt;
&lt;p&gt;然后就用各种size来算一下每个thread要读多少次vec，这多么元素又对应多少个VEC，我们就知道一个thread具体要读哪些数据了。&lt;/p&gt;
&lt;p&gt;这里thread每次读取的粒度是vec, 而每个thread group每次则读取16Bytes，最后就可以合并成一个向量读的指令来优化IO。&lt;/p&gt;
&lt;p&gt;按这个方式把数据都读进来吗，这里其实很多thread要读的数据是一样的，只要有一份数据完成读取，然后其他thread group就可以直接复用这个数据了&lt;/p&gt;
&lt;h2 id=&#34;key-cache数据访问&#34;&gt;Key Cache数据访问&lt;/h2&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/key_io.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;和Query一样，每个thread按VEC访问数据，也是希望一次访问16Bytes，这也是Query最内层有一个X维度的原因。&lt;/p&gt;
&lt;p&gt;我们每x个元素的大小是16Bytes，那么每个Thread就可以按16Bytes的方式对数据进行读取。&lt;/p&gt;
&lt;p&gt;因为我们需要处理Query Token和所有Key Token的乘积，因此这里要一个block一个block的把所有Key Token读进来。
整个读取过程大概是这样一个三层循环：&lt;/p&gt;
&lt;p&gt;每个Thread Wrap通过循环的方式处理多个Paged Block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;循环的每次大迭代处理一个Paged Block（内部通过两个小循环处理）
&lt;ul&gt;
&lt;li&gt;内层外循环：遍历block内每个Token (block_size次）
&lt;ul&gt;
&lt;li&gt;内层内循环：遍历每个Token的每个vec  (head_size / (thread_group_size*vec_size))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同样，只要一个Thread Wrap完成读取，Thread Block里其他Thread Wrap会复用这个读取结果。&lt;/p&gt;
&lt;p&gt;看一个这个三重循环的代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // 外层大循环：4个wrap一起遍历所有所有paged blocks
  // 每次循环：每个wrap处理一个paged block
      // 4 个wrap同时执行 （并行度）
  // wrap_idx = thread_is / wrap_size
  for (int block_idx = start_block_idx + warp_idx; block_idx &amp;lt; end_block_idx;
       block_idx += NUM_WARPS) {
    ...
    // Load a key to registers.
    // Each thread in a thread group has a different part of the key.
    // For example, if the the thread group size is 4, then the first thread in
    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
    // has 1, 5, 9, ... th vectors of the key, and so on.
    // 内层外循环：遍历所有paged blocks
    // 每次循环：32个thread一起遍历当前paged block内所有token
        // NUM_TOKENS_PER_THREAD_GROUP =      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
        // 每个thread group负责一个token(token_idx)
    for (int i = 0; i &amp;lt; NUM_TOKENS_PER_THREAD_GROUP; i++) {
      const int physical_block_offset =
          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
      K_vec k_vecs[NUM_VECS_PER_THREAD];

#pragma unroll
        // 内层内循环：按 VEC_SIZE 遍历Token内的所有fp16
        // 每次循环：32个thread一起遍历paged block内每个token
            // NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
            // NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
            // 每个thread group 为一组,处理同一个token
            // 每个threa一次读取一个VEC
            // 每个thread group一共负责 NUM_VECS_PER_THREAD 个VEC
            // K_vec k_vecs[NUM_VECS_PER_THREAD];  (1 Paged Block)
      for (int j = 0; j &amp;lt; NUM_VECS_PER_THREAD; j++) {
        const cache_t* k_ptr =
            k_cache + physical_block_number * kv_block_stride +
            kv_head_idx * kv_head_stride + physical_block_offset * x;
        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
        const int offset1 = (vec_idx * VEC_SIZE) / x;
        const int offset2 = (vec_idx * VEC_SIZE) % x;

        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
          k_vecs[j] = *reinterpret_cast&amp;lt;const K_vec*&amp;gt;(
              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
        } else {
          // Vector conversion from Quant_vec to K_vec.
          Quant_vec k_vec_quant = *reinterpret_cast&amp;lt;const Quant_vec*&amp;gt;(
              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
          k_vecs[j] = fp8::scaled_convert&amp;lt;K_vec, Quant_vec, KV_DTYPE&amp;gt;(
              k_vec_quant, *k_scale);
        }
      }

      // Compute dot product.
      ....
  }
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;qk-计算&#34;&gt;QK 计算&lt;/h2&gt;
&lt;p&gt;Query和Key都读进来了，下一步自然就是矩阵乘了，还是先上个图：&lt;/p&gt;
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/query_key_compute.png&#34; class=&#34;center&#34; /&gt;
&lt;p&gt;基础的计算的逻辑就是上面代码里省略的部分&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;      // Compute dot product.
      // This includes a reduction across the threads in the same thread group.
      float qk = scale * Qk_dot&amp;lt;scalar_t, THREAD_GROUP_SIZE&amp;gt;::dot(
                             q_vecs[thread_group_offset], k_vecs);
      // Add the ALiBi bias if slopes are given.
      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;

      if (thread_group_offset == 0) {
        // Store the partial reductions to shared memory.
        // NOTE(woosuk): It is required to zero out the masked logits.
        const bool mask = token_idx &amp;gt;= seq_len;
        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
        // Update the max value.
        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
      }
    }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后要做reduce，因为所有qk的乘积是分布在多个Thread里面的，我们要把Thread block里所有的数据都聚合起来，才好算最后的softmax，这里是经典的两层reduce算法，细节就不解释了。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // Perform reduction across the threads in the same warp to get the
  // max qk value for each &amp;#34;warp&amp;#34; (not across the thread block yet).
  // The 0-th thread of each thread group already has its max qk value.
#pragma unroll
  // 经典的Thread Wrap内reduce算法
  for (int mask = WARP_SIZE / 2; mask &amp;gt;= THREAD_GROUP_SIZE; mask /= 2) {
    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
  }
  if (lane == 0) {
    red_smem[warp_idx] = qk_max;
  }
  __syncthreads();

  // 经典的Thread Block内reduce算法
  // TODO(woosuk): Refactor this part.
  // Get the max qk value for the sequence.
  qk_max = lane &amp;lt; NUM_WARPS ? red_smem[lane] : -FLT_MAX;
#pragma unroll
  for (int mask = NUM_WARPS / 2; mask &amp;gt;= 1; mask /= 2) {
    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
  }
  // Broadcast the max qk value to all threads.
  qk_max = VLLM_SHFL_SYNC(qk_max, 0);

  // 计算softmax
  // Get the sum of the exp values.
  float exp_sum = 0.f;
  for (int i = thread_idx; i &amp;lt; num_tokens; i += NUM_THREADS) {
    float val = __expf(logits[i] - qk_max);
    logits[i] = val;
    exp_sum += val;
  }
  exp_sum = block_sum&amp;lt;NUM_WARPS&amp;gt;(&amp;amp;red_smem[NUM_WARPS], exp_sum);

  // Compute softmax.
  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
  for (int i = thread_idx; i &amp;lt; num_tokens; i += NUM_THREADS) {
    logits[i] *= inv_sum;
  }
  __syncthreads();
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;跑到这里以后，softmax就算完了，剩下来的就是在乘一下V Cache，这块就比较简单了。&lt;/p&gt;
&lt;h2 id=&#34;value访问和attneion计算&#34;&gt;Value访问和Attneion计算&lt;/h2&gt;
&lt;p&gt;Value的访问比较直接，直接上图，就是大家一起把所有Value都读进来。
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/value_io.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;p&gt;同时边读边计算，都在这个图里了：
&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/attention_compute.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;
&lt;p&gt;就是先按block进行遍历，然后每次重block里读所有token的一部分维度进行计算，把所有维度都算出来，分散的存在各个thread里。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  // 一样的外层大循环：4个wrap一起遍历所有paged blocks
  for (int block_idx = start_block_idx + warp_idx; block_idx &amp;lt; end_block_idx;
       block_idx += NUM_WARPS) {
    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
    // int64 because int32 can lead to overflow when this variable is multiplied
    // by large numbers (e.g., kv_block_stride).

    const int64_t physical_block_number =
        static_cast&amp;lt;int64_t&amp;gt;(block_table[block_idx]);
    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
    L_vec logits_vec;
    from_float(logits_vec, *reinterpret_cast&amp;lt;Float_L_vec*&amp;gt;(logits + token_idx -
                                                           start_token_idx));

    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
                           kv_head_idx * kv_head_stride;
#pragma unroll

    // NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
    // 一个Paged Block有几个VEC
    // NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
        // 一个WRAP可以同时处理几个 Paged Block 
    // NUM_ROWS_PER_THREAD = DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
        // 遍历所有head纬度需要几个迭代
    for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
      if (row_idx &amp;lt; HEAD_SIZE) {
        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
        V_vec v_vec;

        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
          v_vec = *reinterpret_cast&amp;lt;const V_vec*&amp;gt;(v_ptr + offset);
        } else {
          V_quant_vec v_quant_vec =
              *reinterpret_cast&amp;lt;const V_quant_vec*&amp;gt;(v_ptr + offset);
          // Vector conversion from V_quant_vec to V_vec.
          v_vec = fp8::scaled_convert&amp;lt;V_vec, V_quant_vec, KV_DTYPE&amp;gt;(v_quant_vec,
                                                                    *v_scale);
        }
        if (block_idx == num_seq_blocks - 1) {
          // num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
          // 对最后一个Paged Block特殊处理，防止越界
          // NOTE(woosuk): When v_vec contains the tokens that are out of the
          // context, we should explicitly zero out the values since they may
          // contain NaNs. See
          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
          scalar_t* v_vec_ptr = reinterpret_cast&amp;lt;scalar_t*&amp;gt;(&amp;amp;v_vec);
#pragma unroll
          for (int j = 0; j &amp;lt; V_VEC_SIZE; j++) {
            v_vec_ptr[j] = token_idx + j &amp;lt; seq_len ? v_vec_ptr[j] : zero_value;
          }
        }

        // 按Vec分段计算和V的乘积：由多个thread计算，后续需要进一步聚合。
        accs[i] += dot(logits_vec, v_vec);
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;等算法以后再进行reduce把结果聚合，得到最后的结果。这个是经典的reduce算法：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// Perform reduction within each warp.
// 经典in wrap规约算法
#pragma unroll
  for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
    float acc = accs[i];
#pragma unroll
    for (int mask = NUM_V_VECS_PER_ROW / 2; mask &amp;gt;= 1; mask /= 2) {
      acc += VLLM_SHFL_XOR_SYNC(acc, mask);
    }
    accs[i] = acc;
  }

  // NOTE(woosuk): A barrier is required because the shared memory space for
  // logits is reused for the output.
  __syncthreads();

  // Perform reduction across warps.
  // 经典树状cross wrap规约算法
  float* out_smem = reinterpret_cast&amp;lt;float*&amp;gt;(shared_mem);
#pragma unroll
  for (int i = NUM_WARPS; i &amp;gt; 1; i /= 2) {
    int mid = i / 2;
    // Upper warps write to shared memory.
    if (warp_idx &amp;gt;= mid &amp;amp;&amp;amp; warp_idx &amp;lt; i) {
      float* dst = &amp;amp;out_smem[(warp_idx - mid) * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        if (row_idx &amp;lt; HEAD_SIZE &amp;amp;&amp;amp; lane % NUM_V_VECS_PER_ROW == 0) {
          dst[row_idx] = accs[i];
        }
      }
    }
    __syncthreads();

    // Lower warps update the output.
    if (warp_idx &amp;lt; mid) {
      const float* src = &amp;amp;out_smem[warp_idx * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i &amp;lt; NUM_ROWS_PER_THREAD; i++) {
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        if (row_idx &amp;lt; HEAD_SIZE &amp;amp;&amp;amp; lane % NUM_V_VECS_PER_ROW == 0) {
          accs[i] += src[row_idx];
        }
      }
    }
    __syncthreads();
  }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;算完以后就是把结果写到output了，这块就不贴了。&lt;/p&gt;
&lt;h2 id=&#34;收尾&#34;&gt;收尾&lt;/h2&gt;
&lt;p&gt;大概流程就是这样，整个流程还是设计不少细节的，我也不知道写清楚没有，不过对着代码多看几遍总能看明白的。&lt;/p&gt;
- https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Google Small Towers中多目标优化的探索</title>
        <link>https://sword865.github.io/posts/2021/2021-03-08-google-small-towers%E4%B8%AD%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%9A%84%E6%8E%A2%E7%B4%A2/</link>
        <pubDate>Mon, 08 Mar 2021 15:51:35 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2021/2021-03-08-google-small-towers%E4%B8%AD%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%9A%84%E6%8E%A2%E7%B4%A2/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2021/2021-03-08-google-small-towers%E4%B8%AD%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%9A%84%E6%8E%A2%E7%B4%A2/ -&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;多目标优化中有一个很常见的跷跷板问题，就是说在训练时，多个目标会相互影响，导致震荡&amp;mdash;你降我升，我升你降。有时间还会出现Nan的结果，需要很仔细的调参测试+清洗数据才能训练出一个理想的模型。&lt;/p&gt;
&lt;p&gt;针对这种问题，自然就有了一些尝试，比如从帕累托最优的角度寻找优化方向（阿里PEA），修改模型结构使Shared部分存储更泛化的信息（腾讯PLE）。不过这两个写的人都挺多了，就写一下Google Small Towers的这篇文章吧。&lt;/p&gt;
&lt;h2 id=&#34;主要问题讨论&#34;&gt;主要问题讨论&lt;/h2&gt;
&lt;p&gt;文章首先讨论了两个问题：&lt;/p&gt;
&lt;h3 id=&#34;1-over-parameterization对多任务模型的适用性&#34;&gt;1. Over-parameterization对多任务模型的适用性&lt;/h3&gt;
&lt;p&gt;我们都知道over-parameterization对单任务模型是有价值的，那边对多任务模型是否成立？&lt;/p&gt;
&lt;p&gt;这里以将多个目标的线性组合作为优化目标的例子，认为over-parameterization能够帮助处理各任务优化目标之间的冲突问题（既减少跷跷板问题的出现）。&lt;/p&gt;
&lt;h3 id=&#34;2-大型模型和小型模型的多目标学习表现对比&#34;&gt;2. 大型模型和小型模型的多目标学习表现对比&lt;/h3&gt;
&lt;p&gt;通过实验对比了大型模型和小型模型进行多目标学习中的不同表现。&lt;/p&gt;
&lt;p&gt;实验中，不论是增加任务相关结构的复杂度，还是增加任务共享结构的复杂度，Pareto frontier都会呈现先变好在变差的趋势。&lt;/p&gt;
&lt;p&gt;因此，文章认为over-parameterization并不利于多目标学习中的共享性，进而伤害了多目标学习中的泛化能力。因此，在多目标学习中，模型大小实质上是对模型有效性和泛化能力的一种平衡。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To summarize our insights, for a multi-task learning model, small models benefit from good multi-task generalization but hurts Pareto efficiency; big models theoretically have better Pareto efficiency but could suffer from loss of generalization.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;under-parameterized-self-auxiliaries模型结构&#34;&gt;Under-parameterized Self-auxiliaries模型结构&lt;/h2&gt;
&lt;p&gt;文章提出了under-parameterized self-auxiliaries的模型结构：&lt;/p&gt;
&lt;p&gt;首先假设模型的共享方式是所有任务共享最下面的表示层（Hard Sharded，MMOE这种，PLE就不行）,既对任务t，有：&lt;/p&gt;
&lt;p&gt;$$f_{t}(x; \theta_{sh}, \theta_{t})=f_{t}(h(x; \theta_{sh}); \theta_{t}), \forall t$$&lt;/p&gt;
&lt;p&gt;其中 $\theta_t$ 是任务相关的参数， $\theta_sh$ 为共享参数， $h(x;\theta_sh)$ 既为共享的表示层输出。&lt;/p&gt;
&lt;p&gt;文章在这个基础上对每个任务t增加了self-auxiliary tower的附属结构(既一个辅助任务的small tower)，该小塔输出和原来的任务相同，但参数 $\theta_t^{a}$ 很小（既小塔是一个很简单的结构）:&lt;/p&gt;
&lt;p&gt;$$f_t^a(x; \theta_{sh}, \theta_t^a))=f_t(h(x;\theta_{sh}); \theta_t^a), \forall t$$&lt;/p&gt;
&lt;h3 id=&#34;修改后的多目标模型结构&#34;&gt;修改后的多目标模型结构&lt;/h3&gt;
&lt;p&gt;修改后多目标模型的结构是这样的：&lt;/p&gt;
&lt;img width=&#34;500&#34;  src=&#34;https://sword865.github.io/images/2021/google_m_towner.png&#34; class=&#34;center&#34; /&gt;
&lt;h3 id=&#34;损失函数&#34;&gt;损失函数&lt;/h3&gt;
&lt;p&gt;最后的Loss则是在原来的基础上加上了小塔的Loss:&lt;/p&gt;
&lt;p&gt;$$\hat{L}(\theta)=\sum_{t=1}^Tw_t(\hat{L}(\theta_{sh},\theta_t)+\gamma \hat{L}(\theta_{sh},\theta_t^a))$$&lt;/p&gt;
&lt;p&gt;其中： $$\hat{L}(\theta_{sh},\theta_t^a))=\frac{1}{n}\sum_{i=1}^nL_t(f_t^a(x; \theta_{sh}, \theta_t^a)), y_i^t)$$&lt;/p&gt;
&lt;p&gt;这么改的原因自然就是前面的推理了：通过使用较小的模型来提高任务对不同的泛化能力。（任务量变成了原来的两倍，但是其中有一半任务是under-parameterized，因此也就降低了模型over-parameterized对共享性的破坏力）。&lt;/p&gt;
&lt;p&gt;文章认为，这种通过强迫模型学习共享层来提高复数任务的的结构也是一种正则化的手段。&lt;/p&gt;
&lt;h2 id=&#34;小塔结构示例&#34;&gt;小塔结构示例&lt;/h2&gt;
&lt;p&gt;这里对小塔的结构没有什么限制，唯一的要求就是要比模型简单，下面是文章里的一些例子：&lt;/p&gt;
&lt;img src=&#34;https://sword865.github.io/images/2021/google_m_tower_small_tower.png&#34; /&gt;
&lt;p&gt;最后主要就是附录中的证明与实验细节了，建议直接读原文了解。&lt;/p&gt;
&lt;h2 id=&#34;注释与思考&#34;&gt;注释与思考&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;注1&lt;/strong&gt;: Over-Parameterization的说明见&amp;quot;&lt;a href=&#34;https://arxiv.org/abs/1812.11118&#34;&gt;Reconciling modern machine-learning practice and the classical bias–variance trade-off&lt;/a&gt;&amp;ldquo;和&amp;rdquo;&lt;a href=&#34;https://arxiv.org/abs/1903.07571&#34;&gt;Two models of double descent for weak features&lt;/a&gt;&amp;ldquo;等文章，或者&lt;a href=&#34;https://www.zhihu.com/question/434311126&#34;&gt;知乎相关讨论&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注2&lt;/strong&gt;: 有意思的是Small Tower通过小塔来提升大塔多任务的效果，而阿里的Rocket Launching则是通过大塔来提高小塔的效果，这两个模型放在一起会怎么样呢。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注3&lt;/strong&gt;: 最近发现还有一篇修改优化算法的文章：Gradient Surgery for Multi-Task Learning，不过还没来得急细看，等有空了试试对我们的任务有没有帮助。&lt;/p&gt;
- https://sword865.github.io/posts/2021/2021-03-08-google-small-towers%E4%B8%AD%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%9A%84%E6%8E%A2%E7%B4%A2/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>推荐系统周边设施--特征商店</title>
        <link>https://sword865.github.io/posts/2021/2021-03-07-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%91%A8%E8%BE%B9%E8%AE%BE%E6%96%BD--%E7%89%B9%E5%BE%81%E5%95%86%E5%BA%97/</link>
        <pubDate>Sun, 07 Mar 2021 15:51:35 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2021/2021-03-07-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%91%A8%E8%BE%B9%E8%AE%BE%E6%96%BD--%E7%89%B9%E5%BE%81%E5%95%86%E5%BA%97/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2021/2021-03-07-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%91%A8%E8%BE%B9%E8%AE%BE%E6%96%BD--%E7%89%B9%E5%BE%81%E5%95%86%E5%BA%97/ -&lt;p&gt;好久没写博客了，今天写一点推荐系统周边设施的东西。&lt;/p&gt;
&lt;h2 id=&#34;特征管理&#34;&gt;特征管理&lt;/h2&gt;
&lt;p&gt;特征商店会存储特征元数据，比如特征的计算逻辑、血缘关系、数据类型。 一般来说，这些元数据用于管理特征的生命周期、计算任务和使用方式。&lt;/p&gt;
&lt;h2 id=&#34;离线训练数据生成&#34;&gt;离线训练数据生成&lt;/h2&gt;
&lt;p&gt;为了保证线上线下数据的一致性，推荐系统的训练数据通常有两个数据流Join得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在Ranking中即实时打点：数据流以&lt;code&gt;traceId&lt;/code&gt;为Key，排序时特征为Value。&lt;/li&gt;
&lt;li&gt;客户端日志：记录了&lt;code&gt;traceId&lt;/code&gt;和事件类型(曝光、点击、分享等）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于客户端日志必然晚于服务端日志，因此两个数据流Join时需要一定的窗口。&lt;/p&gt;
&lt;h2 id=&#34;训练数据扩展&#34;&gt;训练数据扩展&lt;/h2&gt;
&lt;p&gt;但是作为调参工程师，我们必然会遇到需要的特征没有记录在实时打点中，导致训练时缺少相关数据的情况，这个时候，就需要想办法来处理这个问题。&lt;/p&gt;
&lt;p&gt;按照Uber的方法，我们可以把特征分为三类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;离线特征&lt;/li&gt;
&lt;li&gt;实时特征&lt;/li&gt;
&lt;li&gt;RPC特征&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;离线特征&#34;&gt;离线特征&lt;/h3&gt;
&lt;p&gt;对于离线特征：我们可以使用Spark读取数据仓库中的历史数据，以天为单位进行生成历史数据，然后放在一个分区的Hive表中。&lt;/p&gt;
&lt;h3 id=&#34;实时特征&#34;&gt;实时特征&lt;/h3&gt;
&lt;p&gt;对于实时特征：基于kappa的思想，我们可以在Flink中编写实时特征计算逻辑，然后启动重跑一段时间以前的历史数据，并记录这个过程中特征的每一次变化（有点类似数据库中的WAL日志流），将其输出到Kafka中去，这样我们也就有一个特征在历史时间段中的值。(这里我们最好有一个服务化的Flink平台，来进行任务的添加、删除、修改等工作)&lt;/p&gt;
&lt;p&gt;这里，特征的计算任务就可以通过特征元数据库进行管理。&lt;/p&gt;
&lt;p&gt;接下来，我们就可以通过带时间戳的Join来完成训练数据和特征数据的拼接，并将特征回写到训练数据中去了。 需要注意的是，为了保证线上线下数据的一致性，我们需要引入一定的延时机制来模拟客户端日志的延迟。&lt;/p&gt;
&lt;h3 id=&#34;rpc特征&#34;&gt;RPC特征&lt;/h3&gt;
&lt;p&gt;最后对于来自外部系统的RPC特征：就没有什么好办法了，我们只能在线上添加这个特征的打点，然后跑上一段时间来得到有这个特征的训练数据了。&lt;/p&gt;
&lt;p&gt;这里推荐一个比较新的开源项目可以完成类似的工作: &lt;a href=&#34;https://github.com/feast-dev/feast&#34;&gt;Feast&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;在线特征推送&#34;&gt;在线特征推送&lt;/h2&gt;
&lt;p&gt;特征的线上存储可以使用KV数据库比如Redis，数据的来源和上面训练数据的扩展可以使用同一套代码，只需要在计算时根据元数据配置来决定是否推送上线。&lt;/p&gt;
&lt;p&gt;另外，这里一般会做很多工程上的优化，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把多个特征作为一个特征组存在一个key里减少请求的次数&lt;/li&gt;
&lt;li&gt;使用一些算法（比如XXHash32）对过长的特征名(比如&lt;code&gt;spu$realtime$orders_last_2w$spu_id&lt;/code&gt;)进行压缩&lt;/li&gt;
&lt;/ul&gt;
- https://sword865.github.io/posts/2021/2021-03-07-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%91%A8%E8%BE%B9%E8%AE%BE%E6%96%BD--%E7%89%B9%E5%BE%81%E5%95%86%E5%BA%97/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>NPE问题与一些语言中的解决方案</title>
        <link>https://sword865.github.io/posts/2018/2018-11-08-npe%E9%97%AE%E9%A2%98%E4%B8%8E%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</link>
        <pubDate>Thu, 08 Nov 2018 23:51:35 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2018/2018-11-08-npe%E9%97%AE%E9%A2%98%E4%B8%8E%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2018/2018-11-08-npe%E9%97%AE%E9%A2%98%E4%B8%8E%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/ -&lt;p&gt;NPE(NullPointerException)是一个很烦人的问题，这里简单列举了一些语言中对NPE的处理。&lt;/p&gt;
&lt;h2 id=&#34;1-通过语法标记进行检查&#34;&gt;1. 通过语法标记进行检查&lt;/h2&gt;
&lt;h3 id=&#34;kotlin&#34;&gt;Kotlin&lt;/h3&gt;
&lt;p&gt;Kotlin要求可以为null的变量必需在定义时声明，同时在读取该类型变量属性时必须进行空值判断。例：String 和 String?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-kotlin&#34; data-lang=&#34;kotlin&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;var&lt;/span&gt; a: String = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;abc&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;a = &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// compilation error, a can not be null
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;var&lt;/span&gt; b: String? = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;abc&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;b = &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// ok
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;val&lt;/span&gt; l = b.length &lt;span style=&#34;color:#75715e&#34;&gt;// compiler error: variable &amp;#39;b&amp;#39; can be null
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;val&lt;/span&gt; l = &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (b &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;) b.length &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; -&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// ok
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;jetbrains-annotations-for-java&#34;&gt;Jetbrains annotations for Java&lt;/h3&gt;
&lt;p&gt;IntelliJ IDEA提供了一些工具，比如可以对@NotNull的参数进行检查，当出现null赋值时在IDE中会给出提示。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; org.jetbrains.annotations.NotNull;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; java.util.ArrayList;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Test&lt;/span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;foo&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;@NotNull&lt;/span&gt; Object param){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; param.&lt;span style=&#34;color:#a6e22e&#34;&gt;hashCode&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;test&lt;/span&gt;(){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        foo(&lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;// warn in IntelliJ IDEA&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;（类似的，FindBugs也提供了@Nonnull注释，用于检查）&lt;/p&gt;
&lt;h3 id=&#34;lombok-for-java&#34;&gt;Lombok for Java&lt;/h3&gt;
&lt;p&gt;Lombok通过在编译时改写字节码对原始代码进行优化，其中的@NonNull，会自动插入运行时检查代码，发现错误抛出异常。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;NonNullExample&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;@NonNull&lt;/span&gt; Person person) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;super&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;name&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; person.&lt;span style=&#34;color:#a6e22e&#34;&gt;getName&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;等价于&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;NonNullExample&lt;/span&gt;(Person person) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (person &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;throw&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; NullPointerException(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;person is marked @NonNull but is null&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;super&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;name&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; person.&lt;span style=&#34;color:#a6e22e&#34;&gt;getName&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;2-用更好的错误处理代替null&#34;&gt;2. 用更好的错误处理代替null&lt;/h2&gt;
&lt;p&gt;空值通常都是由错误导致的无法赋值，因此更好的错误处理也是NPE的一种应对。&lt;/p&gt;
&lt;h3 id=&#34;rust基于result错误处理&#34;&gt;Rust：基于Result错误处理&lt;/h3&gt;
&lt;p&gt;Rust通过Result类型提供了强大的错误处理机制。&lt;/p&gt;
&lt;h3 id=&#34;基于monad处理错误&#34;&gt;基于Monad处理错误&lt;/h3&gt;
&lt;p&gt;Scala等FP语言基于Monad(Option, Either, Try&amp;hellip;)提供了错误处理，其中Optional是最基础的一种。在Option中，定义了专门的None来表示计算失败。这样，在得不到结果时，就会得到None，因此在后续的使用中可以使用isDefined先判断是否有值，再进行处理：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-scala&#34; data-lang=&#34;scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;val&lt;/span&gt; name&lt;span style=&#34;color:#66d9ef&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Option&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;String&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;=&lt;/span&gt; request getParameter &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;name&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isDefined&lt;span style=&#34;color:#f92672&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;//do some stuff with name.get
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;但是这么写很不方便，还是更推荐使用flatMap乃至for推导式来进行计算。(for-yield推导式其实就是flatmap和map的语法糖)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-scala&#34; data-lang=&#34;scala&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;val&lt;/span&gt; upper &lt;span style=&#34;color:#66d9ef&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  name &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;lt;-&lt;/span&gt; request getParameter &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  trimmed &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Some&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;name&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;trim&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  upper &lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Some&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;trimmed&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;toUpperCase&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; trimmed&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;length &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;yield&lt;/span&gt; upper
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;println&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;upper getOrElse &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;由于Option只能用None表示失败，不能记录错误信息，所以scala中还提供了Either用来携带更多的信息。&lt;/p&gt;
&lt;h3 id=&#34;optional-in-java&#34;&gt;Optional in Java&lt;/h3&gt;
&lt;p&gt;Java中的Optional跟scala里的Option是很相似的，同样提供了flatMap操作。但是因为没有for推导式，用起来就感觉不太方便。另外，Java中也缺少可以携带错误信息的Either。&lt;/p&gt;
- https://sword865.github.io/posts/2018/2018-11-08-npe%E9%97%AE%E9%A2%98%E4%B8%8E%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>比较一下spark2的DataFrame和RDD</title>
        <link>https://sword865.github.io/posts/2017/2017-01-19-%E6%AF%94%E8%BE%83%E4%B8%80%E4%B8%8Bspark2%E7%9A%84dataframe%E5%92%8Crdd/</link>
        <pubDate>Sun, 12 Mar 2017 15:49:45 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2017/2017-01-19-%E6%AF%94%E8%BE%83%E4%B8%80%E4%B8%8Bspark2%E7%9A%84dataframe%E5%92%8Crdd/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2017/2017-01-19-%E6%AF%94%E8%BE%83%E4%B8%80%E4%B8%8Bspark2%E7%9A%84dataframe%E5%92%8Crdd/ -&lt;p&gt;前段时间把spark集群升级到2.x，使用起来感觉相对1.x的版本最大的改动就是DataFrame正式开始替代RDD成为主流，包括我们最常用到的mllib的官方文档也提到：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.
After reaching feature parity (roughly estimated for Spark 2.2), the RDD-based API will be deprecated.
The RDD-based API is expected to be removed in Spark 3.0.
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;rdd的结构&#34;&gt;RDD的结构&lt;/h4&gt;
&lt;p&gt;RDD可以看成是一个分布式的无序列表，这个列表内的元素是一个object，RDD并不关心每个object的内部结构。因此所有操作都必须对这个object进行，不利于算子的复用。&lt;/p&gt;
&lt;p&gt;比起DataFrame，RDD更方便我们对数据做一些底层的操作，也可以用于unstructured的数据。&lt;/p&gt;
&lt;h4 id=&#34;dataframe的结构&#34;&gt;DataFrame的结构&lt;/h4&gt;
&lt;p&gt;DataFrame不同于RDD，框架会去了解object中的数据是什么样的结构，这样每个算子就可以单独实现在某个列上，复用起来就更加简单。&lt;/p&gt;
&lt;p&gt;因为DataFrame比RDD多个更多的限制，对内部的元素也有了更多的了解，可以使用SQL语句进行操作，因此也就可以在对DataFrame进行操作时使用Spark SQL的Catalyst优化器进行优化。&lt;/p&gt;
&lt;p&gt;Catalyst一个易于扩展的查询优化器，同时支持基于规则(rule-based)和基于代价(cost-based)的优化方法，我们可以基于相关API自己定义优化规则。&lt;/p&gt;
&lt;p&gt;最后，Spark的Tungsten目前还只支持DataFrame API, 因此在使用RDD时不能享受到Tungsten带来的效率优化。（Tungsten做的优化概括起来说就是由Spark自己来管理内存而不是使用JVM，这样可以避免JVM GC带来的性能损失）&lt;/p&gt;
&lt;h4 id=&#34;dataset数据结构&#34;&gt;DataSet数据结构&lt;/h4&gt;
&lt;p&gt;前面提到DataFrame每一个record对应了一个Row。而Dataset的定义更加宽松，每一个record对应了一个任意的类型。实际上，从源码中可以看到，DataFrame就是Dataset的一种特例。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package object sql {
    ...
    type DataFrame = Dataset[Row]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DataSet和DataFrame可以通过df.as和ds.toDF方法方便的进行转化。&lt;/p&gt;
&lt;p&gt;不同于Row是一个泛化的无类型JVM object, Dataset是由一系列的强类型JVM object组成的，因此DataSet可以在编译时进行类型检查。&lt;/p&gt;
&lt;p&gt;比起RDD，DataSet的API也以Spark SQL引擎为基础，因此在对DataSet进行操作时，同样可以从Catalyst优化器中受益。&lt;/p&gt;
&lt;p&gt;基本上，我觉得DataSet集合了RDD和DataSet两者的优点。&lt;/p&gt;
&lt;h4 id=&#34;关于效率&#34;&gt;关于效率&lt;/h4&gt;
&lt;p&gt;最后，在效率上，在使用RDD的API时候，使用Python明显比Scala要慢上很多（据我们测试是慢了2倍以上）。但是在使用DataFame时，这个缺陷就不复存在了，换句话说，喜欢Python或者放不下各种Python扩展的同志们可以把Python写起来了，哈哈。这里放个国外网友测试的效率比较吧：&lt;/p&gt;
&lt;img src=&#34;https://sword865.github.io/images/2017/Spark_Dataframe_Official_Benchmark.png&#34; /&gt;
&lt;p&gt;可以看到，速度上大致是 Scala DF = Python DF &amp;gt; Scala RDD &amp;gt; Python RDD，并且DF优势很显著。&lt;/p&gt;
&lt;h4 id=&#34;其他参考资料&#34;&gt;其他参考资料&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/hustnn/TungstenSecret&#34;&gt;探索Spark Tungsten的秘密&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.iteblog.com/archives/1706.html&#34;&gt;Spark 2.0介绍：在Spark SQL中定义查询优化规则&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.infoq.com/cn/articles/2015-Review-Spark&#34;&gt;http://www.infoq.com/cn/articles/2015-Review-Spark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/37301226/difference-between-dataset-api-and-dataframe&#34;&gt;http://stackoverflow.com/questions/37301226/difference-between-dataset-api-and-dataframe&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html&#34;&gt;https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://0x0fff.com/spark-dataframes-are-faster-arent-they/&#34;&gt;https://0x0fff.com/spark-dataframes-are-faster-arent-they/&lt;/a&gt;&lt;/p&gt;
- https://sword865.github.io/posts/2017/2017-01-19-%E6%AF%94%E8%BE%83%E4%B8%80%E4%B8%8Bspark2%E7%9A%84dataframe%E5%92%8Crdd/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>谈谈Factorization Machine</title>
        <link>https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/</link>
        <pubDate>Fri, 04 Nov 2016 22:47:21 +0800</pubDate>
        
        <guid>https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/ -&lt;p&gt;因子分解机(Factorization Machine, 简称FM)是一种不错的CTR预估模型，也是我们现在在使用的广告点击率预估模型，比起著名的Logistic Regression, FM能够把握一些组合的高阶特征，因此拥有更强的表现力。&lt;/p&gt;
&lt;p&gt;在做点击率预估时，我们的特征往往来自于用户(user)、广告(item)和上下文环境(context)，在线性模型中，这些特征不进行组合的话，就会发生一个很尴尬的情况，因为：&lt;/p&gt;
&lt;div&gt;$$score = f(w_{user} * x_{user} + w_{item} * x_{item} + w_{contex} * x_{contex})$$&lt;/div&gt;
&lt;p&gt;所以，对所有用户&amp;ndash;我们将得到相同的排序。&lt;/p&gt;
&lt;p&gt;因此我们需要引入一些组合特征作为输入模型，然而仅二阶特征组合的可能性就是原始特征的平方之多，但是由于很多特征其实是相互独立的，他们的组合并没有什么价值。FM就是一种能够自动把握一些高阶特征的点击率预估模型，可以自动帮助使用者选择合适的高阶输入。&lt;/p&gt;
&lt;p&gt;我们先写出带有所有二阶组合特征的目标函数，其中矩阵W中有n^2个参数，求解非常复杂：&lt;/p&gt;
&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j$$&lt;/div&gt;
&lt;p&gt;我们都知道在矩阵分解的协同过滤中中，我们认为每个用户可以表示成一个K维的特征向量$u_k$,每个物品也能表示成一个高维向量$i_k$，这样用户与物品的相关性就可以用两个向量的点击表示，所有用户与所有物品的相关性就可以用两个矩阵相乘的形式表示出来了。&lt;/p&gt;
&lt;p&gt;FM的模型参考了矩阵分解的思想，认为每个特征 $x_i$都可以用一个K维的特征向量$v_i$表示，那么所有特征之前的相关性就也就可以用两个矩阵的相乘进行表示了，模型表示如下：&lt;/p&gt;
&lt;p&gt;其中矩阵W中代表了特征的特征向量的內积，既：$w_{ij}=v_i v_j$，所以，公式可以改写为:&lt;/p&gt;
&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} v_{i} v_{j} x_i x_j$$&lt;/div&gt;
&lt;p&gt;也就是：&lt;/p&gt;
&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \sum_{h=i}^{k} v_{i,h} v_{j,h} x_i x_j$$&lt;/div&gt;
&lt;p&gt;可以看出，W中的参数现在只有nk个了，因为一般有k&amp;laquo;n，所以FM大大降低的目标函数的复杂度。推导可得梯度公式：&lt;/p&gt;
&lt;div&gt;$δy/δw_{0}=1$&lt;/div&gt;
&lt;div&gt;$δy/δw_{i}=x_{i},i=1...n$&lt;/div&gt;
&lt;div&gt;$δy/δv_{i,k}=x_{i}\sum{j \neq i} v_{j,k}x_{j}$&lt;/div&gt;
&lt;p&gt;FM的优化可以用SGD来做，不过这里推荐带动量（momentum）的min-batch SGD算法，试验中比普通的SGD效果似乎更好。带momentum的SGD模拟了物品运动中的惯性。&lt;/p&gt;
&lt;p&gt;在传统的SGD中：&lt;span&gt;$x_{t+1}=x_t+Δx_t,Δx_t=-ŋg_t$&lt;/span&gt; ,其中 $g_t$代表了梯度。而在momentum的SGD中：$Δx_t=px_{t-1}-ŋg_t$。&lt;/p&gt;
&lt;p&gt;不过比起LR, FM有一个缺点&amp;ndash;目标函数是非凸的，虽然针对这个缺点也有一些研究比如一些凸函数形式的FM，不过在实践中似乎这并不是一个很严重的问题，合适的初始化方式和优化方法一般是能够给出一个可以接受的解的。&lt;/p&gt;
&lt;p&gt;FM的另外一个缺点是有点耗费内存，对于每个特征都要用一个K维的向量表示导致参数数量是LR的K倍，这方面也是有一些研究的，比如李沐针对这个问题提出的DiFacto就是一个很好的能够降低内存消耗的优化方案。&lt;/p&gt;
&lt;p&gt;最后，还有一种著名的策略是FFM模型，FFM模型被称为FM的升级版，把同一类的特征（比如一些用01向量编码的的离散特征）归到一个field里去，然后要求每个特征在每个field下都有一个不同的K维表示方式，这一下把参数的数量从K变成了FK(F是field的数量)，模型复杂度变的更高了。不过这样做的效果确实不错。&lt;/p&gt;
- https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
  </channel>
</rss> 