<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on 悟剑阁</title>
    <link>https://sword865.github.io/topics/llm/</link>
    <description>Recent content in LLM on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Thu, 29 Jan 2026 21:55:26 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/topics/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>模型分片，KV Cache和推理加速的一些思考：计算与数据</title>
      <link>https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</link>
      <pubDate>Thu, 29 Jan 2026 21:55:26 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</guid>
      <description>&lt;p&gt;这段时间主要在做推理加速相关的工作，也做了一些实验，准备写点文章做一些记录。这篇文章就从“移动计算”和“移动存储”的视角开个头，聊聊&lt;strong&gt;并行策略的动态切换&lt;/strong&gt;和&lt;strong&gt;KV Cache 流动管理&lt;/strong&gt;的一些实践，梳理一下在 Agentic Workflow 日益普及的当下，我们能否通过对&lt;strong&gt;算力、存储与带宽&lt;/strong&gt;的调度，在大规模集群上更好地提升推理效率。&lt;/p&gt;&#xA;&lt;p&gt;在传统的大数据时代，“计算中心”还是“数据中心”一直是个有趣的问题。随着技术的发展，大家也逐渐总结出了 &lt;strong&gt;&amp;ldquo;Move Compute to Data&amp;rdquo;&lt;/strong&gt; 的实践经验：因为 SSD 很贵、IO 很贵、带宽也很贵。相比之下，不如把代码调度过去，使用本地的 CPU 进行计算。因此，调度器的核心任务是保证 Data Locality，尽量把计算分发到数据所在的硬盘旁边。&lt;/p&gt;&#xA;&lt;p&gt;但在 LLM 的时代，我们面对的是超大模型在 GPU 上的推理，移动计算已经变成了移动模型（权重）这个巨无霸。相比之下，似乎还是移动数据（KV Cache）更现实一些——但什么才是最合适的解法呢？&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-需求的演变从-chat-到-agent&#34;&gt;1. 需求的演变：从 Chat 到 Agent&lt;/h2&gt;&#xA;&lt;p&gt;系统架构的演进，必然是随着业务不断演变的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;第一阶段单轮指令&#34;&gt;第一阶段：单轮指令&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：用户发送一句指令任务（如翻译、总结），模型执行并回复。请求之间几乎独立。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：纯粹的算力（Prefill）或显存带宽（Decoding）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;：最简单的加权轮询。此时 &lt;strong&gt;KV Cache&lt;/strong&gt; 的存在感很低，除了每台机器都有的 System Prompt，几乎没有状态复用的需求，我们可以随意把请求调度到任一台机器上执行。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;第二阶段多轮对话&#34;&gt;第二阶段：多轮对话&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：多轮对话可以通过 Prefix Caching 复用前面上文。Context 越来越长，每次对话对应一次交互。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：显存容量 &amp;ndash;&amp;gt; Prefill 时间&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;：开始引入 &lt;strong&gt;Affinity（亲和性）&lt;/strong&gt; 调度——为了命中 Cache，我们尽量把请求发给存储了该用户历史数据的节点。也就是 &lt;strong&gt;&amp;ldquo;Move Compute to Data&amp;rdquo;&lt;/strong&gt;，因为此时 Prefill（重算数据）太贵，而搬运 KV Cache 也还没在大规模集群中普及。但是这也会导致热点问题。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;第三阶段agentic-workflow&#34;&gt;第三阶段：Agentic Workflow&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：系统提示词、工具定义、思维链、上下文可以在并行的分支任务中共享，多轮对话可以并行执行，但对应一次交互（用户从感知多次交互变成感知任务完成）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：极其复杂的依赖关系，以及直接复用 KV Cache 带来的负载不均衡。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;调度（面临的问题）&lt;/strong&gt;：如果 &amp;ldquo;Move Compute to Data&amp;rdquo; 会导致严重的热点问题——存有热门 Context 的节点会被打爆，而其他空闲节点却因为没有数据而帮不上忙。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;可以看到，随着需求的变化，我们不再只关注单次请求的 TTFT/TPOT，而是开始关注整个 Agent 任务的 &lt;strong&gt;任务完成时间&lt;/strong&gt; 以及系统的 &lt;strong&gt;总吞吐&lt;/strong&gt;量。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Attention基础-工程篇</title>
      <link>https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/</link>
      <pubDate>Thu, 01 Jan 2026 15:23:41 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/</guid>
      <description>&lt;p&gt;本文重点参考了文章&lt;a href=&#34;https://srush.github.io/annotated-mamba/hard.html&#34;&gt;Mamba: The Hard Way&lt;/a&gt;和开源项目&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention&#34;&gt;flash-linear-attention&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;h1 id=&#34;prefill与decoding&#34;&gt;Prefill与Decoding&lt;/h1&gt;&#xA;&lt;p&gt;我们都知道，在Attention的计算中，Prefill和Decoding是两个不同的场景，具体特性如下：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;特性&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Prefill&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Decoding&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;输入&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;长序列（长度 $L$）&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 个新 token + 历史状态&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;常见瓶颈&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Compute bound（Tensor Core 利用）&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Memory/Latency bound（状态读写 + 小矩阵计算）&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;在回忆一下理论篇的介绍，特别是关于Mamba章节中的推导，常见的Linear Attention有两种表示格式：&lt;/p&gt;&#xA;&lt;p&gt;矩阵格式（Attention视角）：&lt;/p&gt;&#xA;&lt;p&gt;$$ y_i = \sum_{j=0}^i (CausalMask(Q_i K_j^T)) V_j $$&lt;/p&gt;&#xA;&lt;p&gt;递推格式（SSM视角）：&lt;/p&gt;&#xA;&lt;p&gt;$$ h_t = A_t h_{t-1} + B_t x_t $$&#xA;$$ y_t = C h_t $$&lt;/p&gt;&#xA;&lt;p&gt;其中Decoding的算子可以比较直接的使用递归格式进行计算，因此我们本文重点还是看Prefill的实现。&lt;/p&gt;&#xA;&lt;h1 id=&#34;linear-attention常见算法&#34;&gt;Linear Attention常见算法&lt;/h1&gt;&#xA;&lt;p&gt;在Linear Attention的计算中，有一些常见的思路，本章结合&lt;code&gt;flash-linear-attention&lt;/code&gt;的实现，对这些思路进行讲解。&lt;/p&gt;&#xA;&lt;h2 id=&#34;prefix-scan--cumsum-前缀和&#34;&gt;Prefix Scan / Cumsum 前缀和&lt;/h2&gt;&#xA;&lt;h3 id=&#34;算法简介&#34;&gt;算法简介&lt;/h3&gt;&#xA;&lt;p&gt;先讲一下什么是Prefix Scan算法，简单来说Prefix是针对有结合性的算子提出的一种优化方式&lt;/p&gt;&#xA;&lt;p&gt;假设有 $y_t = x_1 ⊗ x_2 ⊗ x_3 &amp;hellip; ⊗ x_t$, 如果⊗支持结合律，那么y_t就可以用一个简单的Reduce进行求解&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Attention基础-理论篇</title>
      <link>https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/</link>
      <pubDate>Tue, 16 Dec 2025 22:09:11 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/</guid>
      <description>&lt;p&gt;准备写一些关于线性注意力的文章，对相关理论和工程(Kernel)做一些梳理，这一篇是关于基础理论的。&lt;/p&gt;&#xA;&lt;h1 id=&#34;softmax-attention到线性注意力&#34;&gt;Softmax Attention到线性注意力&lt;/h1&gt;&#xA;&lt;h2 id=&#34;softmax-attention与on2复杂度&#34;&gt;Softmax Attention与O(N^2)复杂度&lt;/h2&gt;&#xA;&lt;p&gt;标准的Transformer使用的是Softmax Attention。给定查询（Query）、键（Key）、值（Value）矩阵 $Q, K, V \in \mathbb{R}^{N \times d}$，其中 $N$ 是序列长度，$d$ 是特征维度（通常 $d \ll N$）。Attention的计算公式为：&lt;/p&gt;&#xA;&lt;p&gt;$$ Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V $$&lt;/p&gt;&#xA;&lt;p&gt;让我们仔细分析一下这个计算过程的维度变化：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;计算相似度矩阵&lt;/strong&gt;：$QK^T$。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$Q$ 是 $N \times d$，$K^T$ 是 $d \times N$。&lt;/li&gt;&#xA;&lt;li&gt;相乘得到 $N \times N$ 的矩阵。这一步的计算复杂度是 $O(N^2 d)$。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;应用Softmax&lt;/strong&gt;：对每一行进行归一化，维度不变，仍为 $N \times N$。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;加权求和&lt;/strong&gt;：乘以 $V$。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$N \times N$ 的矩阵乘以 $N \times d$ 的矩阵 $V$。&lt;/li&gt;&#xA;&lt;li&gt;结果是 $N \times d$。这一步的计算复杂度也是 $O(N^2 d)$。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;瓶颈所在&lt;/strong&gt;：&#xA;由于Softmax是非线性的，我们必须先完整地计算出 $N \times N$ 的Attention Matrix。&lt;/p&gt;</description>
    </item>
    <item>
      <title>VLM与推理能力的进化</title>
      <link>https://sword865.github.io/posts/2025/2025-09-15-vlm%E4%B8%8E%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E8%BF%9B%E5%8C%96/</link>
      <pubDate>Mon, 15 Sep 2025 23:43:26 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-09-15-vlm%E4%B8%8E%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E8%BF%9B%E5%8C%96/</guid>
      <description>&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;&#xA;&lt;p&gt;从年初 R1 火起来以后，看到了很多关于智能本质的讨论，目前大部分讨论似乎都认为：&lt;strong&gt;语言才是智能的基础，是通往 AGI 的路径&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;比如张小珺[1-3]今年采访了很多大佬，就有两个这样的例子：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;杨植麟[1]：认为在现有范式下，多模态能力往往难以提升模型的“智商”，甚至可能损伤模型已有的语言智能。他在接受采访时提到：“如果你想给模型加多模态能力，你需要确保这个多模态能力不要损伤它的‘脑子’。多模态能做到不损伤已经很好了。”他指出，在多模态模式下希望模型和纯文本模式共用一个“大脑”——也即多模态部分应尽量借用文本模型已有的智力，而不是开启另一套全新参数，否则可能丢掉原来文本部分学到的能力。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;姚顺雨[2]也强调语言在通向通用智能方面更具潜力。他起初从事计算机视觉研究，但后来直觉告诉他语言才是更核心、更有潜力的方向，因此读博后转向了语言模型的研究。他指出，语言是人类为了实现认知泛化而发明的最重要工具，具有生成和推理的闭环特性，这使其成为构建通用智能系统的关键媒介。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;不过还是有一些其他的观点，认为多模态能力的潜力可能被训练范式所限制，而非多模态本身毫无助益。例如阶跃星辰的张祥雨[3]就提到：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;图文数据效果不好的原因是噪声数据：如果简单用图文混合数据进行训练，但不解决思维链（CoT）推理或任务复杂度的问题，模型的学习可能是有害的。在缺乏正确推理指导的情况下，模型每一步可能得不到有效信息，产生错乱的梯度，训练效果要么毫无提升，要么甚至更糟。这与他在一个万亿参数多模态模型项目中的发现一致：模型规模增大后，其数学和逻辑推理能力不仅没有提升，反而在达到平台期后开始下降。原因在于模型倾向于&lt;strong&gt;跳步直接给出答案&lt;/strong&gt;而非踏实推演，从而累积误差。简单扩大模型或混合数据并不能自然地融合视觉与文本能力，背后缺少关键环节。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;图文推理的能力同样需要预训练的激活：例如，OpenAI 的 O3 模型在推理过程中能够将图像直接融入思维链，通过动态操作图像（旋转、裁剪等）来辅助解题，在视觉推理基准 V⋆Bench 上取得了 95.7% 的高准确率，刷新了多模态推理上限。令人意外的是，O3 所采用的一些方法（如对原图进行局部放大裁剪）看似原始，却在许多问题上效果很好。他认为：这可能是因为在海量预训练语料中，存在大量图像附带局部放大及文字解释的模式。例如在电子维修论坛中，经常有人上传一张设备照片提问“哪里出了问题”，回答者会圈出图片局部并放大，说明某个电容烧了等。模型在预训练中已经隐式学习了这种“先整体看图 → 再局部看图解释”的模式。因此，像 O3 那样在推理时按需裁剪图像，严格遵循了预训练语料中的分布模式，反而取得了出色效果。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;针对这个问题，想写一些自己对这个领域的了解和看法。&lt;/p&gt;&#xA;&lt;h1 id=&#34;vision-lm-的推理能力研究&#34;&gt;Vision LM 的推理能力研究&lt;/h1&gt;&#xA;&lt;p&gt;在开始讨论前，我先对智商这个事情做一个定义：通常我们可以把模型的能力分为理解、推理、生成三种，我们认为其中推理能力是智商的表现。&lt;/p&gt;&#xA;&lt;h2 id=&#34;vision-cot&#34;&gt;Vision CoT&lt;/h2&gt;&#xA;&lt;p&gt;链式思维（CoT）是大模型提升复杂推理最早被证明有效的范式之一；其视觉扩展（Vision CoT / Visual CoT）在 2023 年起就开始出现。早期的工作以语言+图像作为输出，但是以CoT只会以文本给出，这个时候图像不参与推理的过程，图像带来的推理能力自然就无从谈起。&lt;/p&gt;&#xA;&lt;p&gt;后来O3的兴起带火了一个新的方向：&lt;code&gt;thinking with images&lt;/code&gt;，其核心思想：不要把“看图”当成一次性输入，而是让图像参与到逐步推理循环中，成为中间状态的生成与消费对象。也就是说图像开始参与了推理的过程，因此我们就从这里作为了解Vision LM推理能力的起点。&lt;/p&gt;&#xA;&lt;p&gt;（下图是thinking with images的例子）&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250915/thinking_with_images.png&#34; class=&#34;center&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;openai-o1--o3-系列&#34;&gt;OpenAI O1 / O3 系列&lt;/h3&gt;&#xA;&lt;p&gt;OpenAI 在 GPT‑4 之后推出 o1 系列[4]，算是第一次验证了Test Time Scaling和推理能力的关系。更进一步的 O3 模型在 demo 中展示了新的模式：除了“会看图”理解，还会“用图思考”。推理过程中模型可以：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;主动定位相关区域（放大公式、框选局部结构、旋转视角）；&lt;/li&gt;&#xA;&lt;li&gt;将局部视觉观察嵌入后续思维链进行推理；&#xA;O3 在视觉推理基准 V⋆Bench 上达到 95.7% 的成绩，也算是验证“显式把视觉纳入思维链”是可以提升推理能力的研究方向。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;deepeyes&#34;&gt;DeepEyes&lt;/h3&gt;&#xA;&lt;p&gt;DeepEyes[6] 应该是第一个开源再现“thinking with images”能力的项目。它不依赖额外人工 SFT，而是直接用 RL 激活模型的视觉逐步聚焦行为。模型可以按照：生成初始思维链 → 判断是否缺细节 → 自主调用“放大 / 裁剪”工具 → 将裁剪区域重新编码 → 继续推演。循环执行形成类似 O3 的效果，并且得到了验证：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
