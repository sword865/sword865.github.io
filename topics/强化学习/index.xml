<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>强化学习 on 悟剑阁</title>
    <link>https://sword865.github.io/topics/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 强化学习 on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Thu, 14 Aug 2025 22:53:26 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/topics/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>再聊一下RL框架与算法的协同演化</title>
      <link>https://sword865.github.io/posts/2025/2025-08-14-%E5%86%8D%E8%81%8A%E4%B8%80%E4%B8%8Brl%E6%A1%86%E6%9E%B6%E4%B8%8E%E7%AE%97%E6%B3%95%E7%9A%84%E5%8D%8F%E5%90%8C%E6%BC%94%E5%8C%96/</link>
      <pubDate>Thu, 14 Aug 2025 22:53:26 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-08-14-%E5%86%8D%E8%81%8A%E4%B8%80%E4%B8%8Brl%E6%A1%86%E6%9E%B6%E4%B8%8E%E7%AE%97%E6%B3%95%E7%9A%84%E5%8D%8F%E5%90%8C%E6%BC%94%E5%8C%96/</guid>
      <description>&lt;p&gt;上一篇文章我们聊了一下Ray与LLM强化学习框架设计，探讨了其架构的演进，但是没有提到为什么框架会往这个方向逐渐演进而不是一开始就使用现在的设计。这里面自然有实践中不断优化的结果，但是也是和整个LLM RL需求的变化密切相关的。&lt;/p&gt;&#xA;&lt;p&gt;因此，本文会主要讨论一下LLM强化学习中，算法与系统框架是如何相互影响、协同演化的。首先分析两个相对成熟的协同设计案例，然后讨论几个正在不够成熟、但是在笔者看来很有潜力的优化方向。&lt;/p&gt;&#xA;&lt;h1 id=&#34;算法与框架协同演化的典型案例&#34;&gt;算法与框架协同演化的典型案例&lt;/h1&gt;&#xA;&lt;p&gt;先讲两个目前已经相对得到了共识的问题：&lt;/p&gt;&#xA;&lt;h2 id=&#34;案例一推理模型驱动的分离式架构设计&#34;&gt;案例一：推理模型驱动的分离式架构设计&lt;/h2&gt;&#xA;&lt;p&gt;在前一篇文章中提到过，之前的强化学习还是希望尽量on-policy的，因此早期的强化学习系统倾向于采用on-policy算法配合co-located的架构设计。这种选择有其合理性：算法层面，on-policy确实具备样本效率优势；系统层面，在CoT和Test-time Scaling兴起之前，模型输出长度差异相对较小，推理引擎产生的计算空泡还算可控。尽管资源利用率的问题客观存在，但on-policy算法的优势在一定程度上抵消了这种系统层面的低效。&lt;/p&gt;&#xA;&lt;p&gt;不过，从去年o1发布开始，业界开始重视Test time scaling，加上R1的发布又给推理模型点了一把火，这种范式的改变打破了之前的平衡：模型在推理阶段生成的文本长度显著增加，且不同样本间的长度差异极为悬殊。在这种新的计算模式下，on-policy的算法优势已无法弥补系统资源的巨大浪费——所有并行环境必须等待最长推理任务完成才能进入下一轮迭代，这种同步约束造成了严重的吞吐量瓶颈和大量计算资源闲置。&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250814/noCotToCot.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;正是因为这个挑战，业界逐渐开始转向异步RL框架设计。其核心思想是将Generation与Training完全解耦，构建生产者-消费者的流水线架构。以AsyncFlow和AReaL为例：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;推理引擎（Rollout Workers）&lt;/strong&gt;：持续异步生成新的数据，彼此间无需同步等待。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;训练引擎（Trainer Workers）&lt;/strong&gt;：训练节点异步地从共享缓冲区获取数据进行模型更新。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这种&lt;strong&gt;流式RL&lt;/strong&gt;设计有效避免了慢速推理任务对整体流程的阻塞，确保所有计算设备维持高利用率，从而实现训练吞吐量的显著提升。这正是算法演进与系统架构优化协同设计的一个典型案例：新的需求需要新的取舍，进一步催生了新的架构。&lt;/p&gt;&#xA;&lt;h2 id=&#34;案例二moe架构与训练推理引擎的精度对齐挑战&#34;&gt;案例二：MoE架构与训练推理引擎的精度对齐挑战&lt;/h2&gt;&#xA;&lt;p&gt;另一个挑战出现在系统层面，尤其是在使用超大模型或混合专家（MoE）模型时：训练引擎和推理引擎之间的不匹配。&lt;/p&gt;&#xA;&lt;p&gt;在RL的训练中，通常为了效率，模型的推理（生成）过程可能会在专门的推理引擎上执行（例如使用vLLM），而梯度计算则在训练后端（例如DeepSpeed）上进行。然而，两者在数值精度（如FP16 vs INT8）、算子融合、批处理调度或MoE门控行为上都可能存在差异，进而导致rollout数据和训练计算之间的分布漂移。&lt;/p&gt;&#xA;&lt;p&gt;实际上，这个问题并不是在MoE中才刚刚出现，以小红书为例，他们在去年的QCon演讲&lt;a href=&#34;https://mp.weixin.qq.com/s/tG_ktQ0WbZHQavtoJtaXbw&#34;&gt;从0到1构建RLHF系统——小红书大模型团队的探索与实践&lt;/a&gt;中就提到了对训推一致性的要求，通过自研框架进行对齐。此外还有一些公司会选择使用推理引擎生产数据，然后通过训练引擎再次推理拿到logit进行概率计算。&lt;/p&gt;&#xA;&lt;p&gt;但是随着MoE的兴起和推理引擎加速技术的不断发展，精度对齐变得越来越困难，比如MoE的路由层引入的不一致可能远高于Dense Model中的精度不一致。同时由于数据的实际采样概率还是由推理引擎决定，以前主要是不同引擎计算的概率不同，但是现在可能连采样出的token都会有巨大的变化，这让通过训练框架再次对齐也更不够用了。&lt;/p&gt;&#xA;&lt;p&gt;为了解决这一问题，AREAL框架提出了&lt;a href=&#34;http://arxiv.org/abs/2505.24298&#34;&gt;Decoupled PPO&lt;/a&gt;算法，采用了一种巧妙的双引擎协同策略：利用推理引擎输出的概率进行重要性采样（因为它反映了真实的数据采样分布），同时使用训练引擎输出的概率来计算置信域（Trust Region），确保训练引擎对模型更新仍在训练引擎原来的有效范围内进行。&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250814/decoupledPPO.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;类似的，&lt;a href=&#34;https://fengyao.notion.site/flash-rl&#34;&gt;FlashRL&lt;/a&gt; 也在他们的&lt;a href=&#34;https://fengyao.notion.site/off-policy-rl&#34;&gt;blog&lt;/a&gt;中提出了一种截断重要性采样的技术&lt;strong&gt;TIS&lt;/strong&gt;，通过对更新进行重新加权，来修正量化推理（用于采样）和全精度模型（用于优化）之间的策略差异。通过这一技术，即使是高度量化的rollout数据（如INT8/FP8）也能被用于训练，而不会损害最终效果。&lt;/p&gt;&#xA;&lt;p&gt;可以看到，在这个例子里，研究者们通过算法的设计，来弥合了系统设计导致的训练和推理之间的鸿沟。&lt;/p&gt;&#xA;&lt;h1 id=&#34;可能的发展趋势协同设计的新兴挑战与机遇&#34;&gt;（可能的）发展趋势：协同设计的新兴挑战与机遇&lt;/h1&gt;&#xA;&lt;p&gt;讲了有共识的，有相对公认的解决方案的案例以后，我们来看一看有些相对不那么得到共识，或者大家意识到但是解决方案还没有完全收敛的方向：&lt;/p&gt;&#xA;&lt;h2 id=&#34;方向一agent-rl-样本效率环境管理与过程奖励&#34;&gt;方向一：Agent RL: 样本效率、环境管理与过程奖励&lt;/h2&gt;&#xA;&lt;p&gt;随着Agent RL的发展，强化学习的过程中，LLM需要升级为能够调用工具、API，并与环境持续交互的自主智能体时，新的挑战也随之浮现。&lt;/p&gt;&#xA;&lt;p&gt;个人认为，根据当前研究趋势，Agent RL主要可归为两类典型范式：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Sandbox + Browser：通常用来训练通用Agent，在受控沙箱或浏览器环境中进行任务执行与评估；&lt;/li&gt;&#xA;&lt;li&gt;MCP + Tooluse：常见于内部或垂直领域，增强模型使用工具集合使用能力。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;以实际场景为例，可以更清晰地看到Agent RL所面临的关键难题。&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-奖励稀疏性与延迟问题&#34;&gt;1. 奖励稀疏性与延迟问题&lt;/h3&gt;&#xA;&lt;p&gt;在复杂任务中，类似o1的&lt;strong&gt;结果奖励&lt;/strong&gt;机制效率极低。例如，Anthropic提到Claude Ops 4可在后台连续运行7小时完成软件开发任务，若仅在最终成败时给予奖励信号，这种延迟且稀疏的反馈使得样本的生成变得越加困难，同时单一的结果奖励几乎无法有效指导学习过程。因此，过程级奖励（Process-level Rewards）或者智能体引导机制（Agent Guidance）可能会重新变得重要。&lt;/p&gt;&#xA;&lt;p&gt;近期工作如&lt;a href=&#34;https://arxiv.org/pdf/2506.11425&#34;&gt;Agent-RLVR&lt;/a&gt; 框架，在 RL 训练中引入高层指令提示、动态错误反馈等“教学式”奖励信号，显著提升了智能体在复杂编程任务中的成功率。这类方法模拟人类教学过程，通过中间反馈加速策略收敛。&lt;/p&gt;&#xA;&lt;p&gt;与之相关的，还有对样本效率的提高：一方面需要构建更高效的样本生成流水线，另一方面要探索高质量数据的重用机制以降低采样成本。相关研究也在逐步涌现，如&lt;a href=&#34;https://arxiv.org/abs/2508.06412v1&#34;&gt;Sample-efficient LLM Optimization with Reset Replay&lt;/a&gt;，为解决Agent RL中的样本稀缺问题提供新的思路。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-复杂状态表示与管理&#34;&gt;2. 复杂状态表示与管理&lt;/h3&gt;&#xA;&lt;p&gt;Agent 的&lt;strong&gt;状态&lt;/strong&gt;不仅包括对话历史，还涵盖工具调用输出、环境观测、以及内部思维链等多源异构信息。传统 RL 框架难以有效建模此类高维、长序列的状态空间。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ray与LLM强化学习框架设计</title>
      <link>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Sun, 27 Jul 2025 14:53:26 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</guid>
      <description>&lt;p&gt;最近LLM强化学习框架发展特别快，Ray作为被ChatGPT带火的框架，在LLM各个训练阶段中，RL阶段的应用应该是最多的。写篇文章记录一下这块发展的脉络和一些看法。&lt;/p&gt;&#xA;&lt;h1 id=&#34;从google-pathways说起&#34;&gt;从Google Pathways说起&lt;/h1&gt;&#xA;&lt;p&gt;讨论Ray和RL系统，得从Google的&lt;strong&gt;Pathways&lt;/strong&gt;系统开始：2021年Google提出了Pathways作为下一代AI架构和分布式ML平台，在相关文献中详细讨论了Single-Controller + MPMD的系统设计。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Single-Controller&lt;/strong&gt;（单控制器）是指用一个中央协调器来管理整个分布式计算流程的架构模式。在这种设计中，有一个&lt;strong&gt;主控制节点&lt;/strong&gt;负责整个计算图的执行，包括任务分发、资源调度、状态监控等。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Multiple-Controller&lt;/strong&gt;（多控制器）则是指使用多个分布式控制节点来协同管理计算任务的架构模式。在这种设计中，没有单一的中央协调器，而是由多个控制器节点分别负责不同的子系统或计算子图，通过分布式协调协议来实现全局一致性。&lt;/p&gt;&#xA;&lt;p&gt;在Ray中的Driver Process就可以被作为一个典型的Single Controller来启动不同的任务程序，而通过torchrun运行的PyTorch DDP分布式计算则是在每个node上各自执行自己的程序则属于典型的Multiple Controller范式。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;MPMD&lt;/strong&gt;（Multiple Program, Multiple Data）是一种分布式计算范式，指在一个计算任务中，不同的节点运行不同的程序来处理不同的数据。这种模式下，各个计算节点执行的代码逻辑可能完全不同，每个节点都有自己特定的任务和职责。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;SPMD&lt;/strong&gt;（Single Program, Multiple Data）则是另一种常见的分布式计算范式，指所有节点运行相同的程序，但处理不同的数据分片。&lt;/p&gt;&#xA;&lt;p&gt;典型的SPMD任务包括传统的分布式训练，比如PyTorch DDP，每个节点运行相同的程序来处理不同的数据，最多根据rank的值会有一些特别的处理（比如rank=0的节点负责checkpoint）。相比之下，大模型训练包括了流水并行这种更复杂的任务，每个节点组需要运行不同的程序，就更适合用MPMD的方式来实现了。&lt;/p&gt;&#xA;&lt;p&gt;一般来说，MPMD系统由于包含众多异构组件，各组件间的协调和同步变得相当复杂。为了简化开发复杂度并确保系统执行的一致性，Single-Controller架构成为了自然的选择——通过引入中心化的控制器来统一管理整个分布式计算流程，包括任务调度、状态同步和异常处理等关键环节。&lt;/p&gt;&#xA;&lt;p&gt;更多的细节就不多说了，有兴趣的话可以去看Oneflow团队当年写的两篇文章，非常深刻：&lt;a href=&#34;https://mp.weixin.qq.com/s/roQues5HhRXqGf26DuUOjQ&#34;&gt;解读谷歌Pathways架构（一）：Single-controller与Multi-controller&lt;/a&gt;和&lt;a href=&#34;https://mp.weixin.qq.com/s/N99dRgFYC9zOOcGlg0Ulsw&#34;&gt;解读谷歌 Pathways 架构（二）：向前一步是 OneFlow&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;这与LLM强化学习有什么关系呢？用RL训练LLM本质上是一个多阶段、多节点的复杂分布式任务。典型的RLHF流水线涉及多个不同模型，计算流程分为几个关键阶段：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;生成阶段&lt;/strong&gt;：当前策略模型（LLM）对一批输入提示生成响应文本&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;评估阶段&lt;/strong&gt;：这些响应由奖励模型评分，或通过人类/自动化偏好模型进行比较评估&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;训练阶段&lt;/strong&gt;：基于获得的奖励信号更新策略模型权重（可能还包括价值函数或评论家网络的更新）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;这些阶段之间存在明确的数据依赖关系——训练更新必须依赖于生成的样本及其对应的奖励分数。在朴素的实现中，这些阶段只能串行执行，引入大量上下文切换的同时还要求所有的模型使用相同数量的GPU进行计算，计算效率是相当低下的。因此正如Pathways架构所启发的那样，我们希望在保证正确性的前提下，通过良好的系统设计，尽可能地重叠和并行化这些工作阶段，以最大化计算资源的利用效率。&lt;/p&gt;&#xA;&lt;p&gt;直接说有点抽象，可以看下面这个从&lt;a href=&#34;http://arxiv.org/abs/2409.19256&#34;&gt;HybridFlow&lt;/a&gt;里截的表格，我截了两个最早的RLHF系统，左边的DeepSpeed-Chat实现了SPMD的串行方式，而右边的OpenRLHF则是典型的MPMD系统。&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_SPMD_MPMD.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;h1 id=&#34;ray与llm强化学习框架&#34;&gt;Ray与LLM强化学习框架&lt;/h1&gt;&#xA;&lt;p&gt;其实从前面的内容可以看出来，Ray的设计很适合用来开发Single-Controller + MPMD的程序，也就自然适合LLM强化学习的场景了。&lt;/p&gt;&#xA;&lt;p&gt;实际上，社区也确实基于Ray开发了大量的强化学习框架，目前主要的设计包括两种：&lt;strong&gt;Colocated架构&lt;/strong&gt;和&lt;strong&gt;Disaggregated架构&lt;/strong&gt;。粗略地说，&lt;strong&gt;Colocated架构&lt;/strong&gt;意味着把生成阶段和训练阶段放在同样的节点上运行；而&lt;strong&gt;Disaggregated架构&lt;/strong&gt;则把它们放在不同的节点上：&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RL_architecture.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;一看这个图，我们会发现Disaggregated Architecture中存在大量的计算bubble，甚至可能比不上之前SPMD模式！这也是为什么很多框架如OpenRLHF、Nemo-aligner、VeRL都是按照Colocated架构来设计的。&lt;/p&gt;&#xA;&lt;p&gt;需要注意的是，图里的Train和Gen代表的是RLHF的不同阶段，每个阶段内每个GPU可能在运行不同任务，因此整个过程仍然是MPMD的。&lt;/p&gt;&#xA;&lt;p&gt;以经典的PPO算法为例，整个Train的阶段包括Actor Model(on training Framework)/Reference Model/Reward Model/Critic Model四个模型，Gen阶段包括Actor Model(on inference framework)一个模型，以&lt;a href=&#34;http://arxiv.org/abs/2405.11143&#34;&gt;OpenRLHF&lt;/a&gt;为例：&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_PPO.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;可以看到这里Actor Model会在Deepspeed和vLLM两个引擎间进行切换，因为实际算法需要保存的模型共有5个。&lt;/p&gt;&#xA;&lt;h2 id=&#34;colocated-rl框架-解法和问题&#34;&gt;Colocated RL框架 (解法和问题)&lt;/h2&gt;&#xA;&lt;p&gt;接下来继续看这个基于Ray的框架：OpenRLHF使用Ray启动和协调组件，但使用Ray的&lt;strong&gt;Placement group&lt;/strong&gt;实现了Colocated架构，在每个节点上在rollout和训练任务之间分割GPU资源。例如，在给定节点上，框架可能将每个GPU的0.75分配给训练actor，0.25分配给生成actor，这样有效地让一个训练进程和一个生成进程&amp;quot;共享&amp;quot;每个GPU而不互相干扰。&lt;/p&gt;&#xA;&lt;p&gt;很容易从前面的图看出来，Colocated框架中的&lt;strong&gt;资源共享&lt;/strong&gt;是一个主要的优势，通过设计合适的分组方式，我们可以减少GPU的空闲时间，减少模型offload的频率，同时尽量并行化不同节点的执行，从而最大化提升资源利用效率。&lt;/p&gt;&#xA;&lt;p&gt;然而，随着模型大小和集群大小的增长，Colocated框架也显示出了自己的局限性：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;第一个关键问题是&lt;a href=&#34;http://arxiv.org/abs/2504.15930&#34;&gt;StreamRL&lt;/a&gt;中提到的&lt;strong&gt;资源耦合&lt;/strong&gt;。虽然Colocated框架比起SPMD的程序提升了计算任务的并行性，并通过分组来允许每组model使用不同的资源，但是这并不能完全消除共享设备带来的问题：因为生成和训练同时共享相同设备，我们不能独立扩展或为每个阶段定制资源。同时训练任务（计算密集）和生成任务（IO密集）的瓶颈并不相同，这不利于GPU资源的利用。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;另一个问题是，LLM生成的文本长度是不固定的，尤其随着thinking model的大火，生成任务中不同组的模型生成结果的时间可能差异很大。比如我们有32块GPU，每4块GPU为一组进行生成，如果其中一组生成任务过长，会导致其他28块GPU空等造成资源浪费。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;总的来说，Colocated框架通过精细的资源管理实现了较高的GPU利用率，相对成熟和稳定，确实许多后续框架都借鉴了类似的设计思路。但是，正如前面提到的资源耦合问题，这种架构在可扩展性方面仍有局限。这也为下一代RL框架的发展指明了方向：能否通过打破严格的串行约束，让生成和训练阶段真正独立地并行执行？&lt;/p&gt;&#xA;&lt;h2 id=&#34;on-policy和off-policy&#34;&gt;On-Policy和Off-Policy&lt;/h2&gt;&#xA;&lt;p&gt;本文的重点是Ray和LLM RL的框架设计，因此不会在这一部分内容做过多阐述，大概而言：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
