<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式计算 on 悟剑阁</title>
    <link>https://sword865.github.io/topics/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/</link>
    <description>Recent content in 分布式计算 on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Sun, 27 Jul 2025 14:53:26 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/topics/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ray与LLM强化学习框架设计</title>
      <link>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Sun, 27 Jul 2025 14:53:26 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/</guid>
      <description>&lt;p&gt;最近LLM强化学习框架发展特别快，Ray作为被ChatGPT带火的框架，在LLM各个训练阶段中，RL阶段的应用应该是最多的。写篇文章记录一下这块发展的脉络和一些看法。&lt;/p&gt;&#xA;&lt;h1 id=&#34;从google-pathways说起&#34;&gt;从Google Pathways说起&lt;/h1&gt;&#xA;&lt;p&gt;讨论Ray和RL系统，得从Google的&lt;strong&gt;Pathways&lt;/strong&gt;系统开始：2021年Google提出了Pathways作为下一代AI架构和分布式ML平台，在相关文献中详细讨论了Single-Controller + MPMD的系统设计。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Single-Controller&lt;/strong&gt;（单控制器）是指用一个中央协调器来管理整个分布式计算流程的架构模式。在这种设计中，有一个&lt;strong&gt;主控制节点&lt;/strong&gt;负责整个计算图的执行，包括任务分发、资源调度、状态监控等。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Multiple-Controller&lt;/strong&gt;（多控制器）则是指使用多个分布式控制节点来协同管理计算任务的架构模式。在这种设计中，没有单一的中央协调器，而是由多个控制器节点分别负责不同的子系统或计算子图，通过分布式协调协议来实现全局一致性。&lt;/p&gt;&#xA;&lt;p&gt;在Ray中的Driver Process就可以被作为一个典型的Single Controller来启动不同的任务程序，而通过torchrun运行的PyTorch DDP分布式计算则是在每个node上各自执行自己的程序则属于典型的Multiple Controller范式。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;MPMD&lt;/strong&gt;（Multiple Program, Multiple Data）是一种分布式计算范式，指在一个计算任务中，不同的节点运行不同的程序来处理不同的数据。这种模式下，各个计算节点执行的代码逻辑可能完全不同，每个节点都有自己特定的任务和职责。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;SPMD&lt;/strong&gt;（Single Program, Multiple Data）则是另一种常见的分布式计算范式，指所有节点运行相同的程序，但处理不同的数据分片。&lt;/p&gt;&#xA;&lt;p&gt;典型的SPMD任务包括传统的分布式训练，比如PyTorch DDP，每个节点运行相同的程序来处理不同的数据，最多根据rank的值会有一些特别的处理（比如rank=0的节点负责checkpoint）。相比之下，大模型训练包括了流水并行这种更复杂的任务，每个节点组需要运行不同的程序，就更适合用MPMD的方式来实现了。&lt;/p&gt;&#xA;&lt;p&gt;一般来说，MPMD系统由于包含众多异构组件，各组件间的协调和同步变得相当复杂。为了简化开发复杂度并确保系统执行的一致性，Single-Controller架构成为了自然的选择——通过引入中心化的控制器来统一管理整个分布式计算流程，包括任务调度、状态同步和异常处理等关键环节。&lt;/p&gt;&#xA;&lt;p&gt;更多的细节就不多说了，有兴趣的话可以去看Oneflow团队当年写的两篇文章，非常深刻：&lt;a href=&#34;https://mp.weixin.qq.com/s/roQues5HhRXqGf26DuUOjQ&#34;&gt;解读谷歌Pathways架构（一）：Single-controller与Multi-controller&lt;/a&gt;和&lt;a href=&#34;https://mp.weixin.qq.com/s/N99dRgFYC9zOOcGlg0Ulsw&#34;&gt;解读谷歌 Pathways 架构（二）：向前一步是 OneFlow&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;这与LLM强化学习有什么关系呢？用RL训练LLM本质上是一个多阶段、多节点的复杂分布式任务。典型的RLHF流水线涉及多个不同模型，计算流程分为几个关键阶段：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;生成阶段&lt;/strong&gt;：当前策略模型（LLM）对一批输入提示生成响应文本&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;评估阶段&lt;/strong&gt;：这些响应由奖励模型评分，或通过人类/自动化偏好模型进行比较评估&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;训练阶段&lt;/strong&gt;：基于获得的奖励信号更新策略模型权重（可能还包括价值函数或评论家网络的更新）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;这些阶段之间存在明确的数据依赖关系——训练更新必须依赖于生成的样本及其对应的奖励分数。在朴素的实现中，这些阶段只能串行执行，引入大量上下文切换的同时还要求所有的模型使用相同数量的GPU进行计算，计算效率是相当低下的。因此正如Pathways架构所启发的那样，我们希望在保证正确性的前提下，通过良好的系统设计，尽可能地重叠和并行化这些工作阶段，以最大化计算资源的利用效率。&lt;/p&gt;&#xA;&lt;p&gt;直接说有点抽象，可以看下面这个从&lt;a href=&#34;http://arxiv.org/abs/2409.19256&#34;&gt;HybridFlow&lt;/a&gt;里截的表格，我截了两个最早的RLHF系统，左边的DeepSpeed-Chat实现了SPMD的串行方式，而右边的OpenRLHF则是典型的MPMD系统。&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_SPMD_MPMD.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;h1 id=&#34;ray与llm强化学习框架&#34;&gt;Ray与LLM强化学习框架&lt;/h1&gt;&#xA;&lt;p&gt;其实从前面的内容可以看出来，Ray的设计很适合用来开发Single-Controller + MPMD的程序，也就自然适合LLM强化学习的场景了。&lt;/p&gt;&#xA;&lt;p&gt;实际上，社区也确实基于Ray开发了大量的强化学习框架，目前主要的设计包括两种：&lt;strong&gt;Colocated架构&lt;/strong&gt;和&lt;strong&gt;Disaggregated架构&lt;/strong&gt;。粗略地说，&lt;strong&gt;Colocated架构&lt;/strong&gt;意味着把生成阶段和训练阶段放在同样的节点上运行；而&lt;strong&gt;Disaggregated架构&lt;/strong&gt;则把它们放在不同的节点上：&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RL_architecture.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;一看这个图，我们会发现Disaggregated Architecture中存在大量的计算bubble，甚至可能比不上之前SPMD模式！这也是为什么很多框架如OpenRLHF、Nemo-aligner、VeRL都是按照Colocated架构来设计的。&lt;/p&gt;&#xA;&lt;p&gt;需要注意的是，图里的Train和Gen代表的是RLHF的不同阶段，每个阶段内每个GPU可能在运行不同任务，因此整个过程仍然是MPMD的。&lt;/p&gt;&#xA;&lt;p&gt;以经典的PPO算法为例，整个Train的阶段包括Actor Model(on training Framework)/Reference Model/Reward Model/Critic Model四个模型，Gen阶段包括Actor Model(on inference framework)一个模型，以&lt;a href=&#34;http://arxiv.org/abs/2405.11143&#34;&gt;OpenRLHF&lt;/a&gt;为例：&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250726/RLHF_PPO.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;可以看到这里Actor Model会在Deepspeed和vLLM两个引擎间进行切换，因为实际算法需要保存的模型共有5个。&lt;/p&gt;&#xA;&lt;h2 id=&#34;colocated-rl框架-解法和问题&#34;&gt;Colocated RL框架 (解法和问题)&lt;/h2&gt;&#xA;&lt;p&gt;接下来继续看这个基于Ray的框架：OpenRLHF使用Ray启动和协调组件，但使用Ray的&lt;strong&gt;Placement group&lt;/strong&gt;实现了Colocated架构，在每个节点上在rollout和训练任务之间分割GPU资源。例如，在给定节点上，框架可能将每个GPU的0.75分配给训练actor，0.25分配给生成actor，这样有效地让一个训练进程和一个生成进程&amp;quot;共享&amp;quot;每个GPU而不互相干扰。&lt;/p&gt;&#xA;&lt;p&gt;很容易从前面的图看出来，Colocated框架中的&lt;strong&gt;资源共享&lt;/strong&gt;是一个主要的优势，通过设计合适的分组方式，我们可以减少GPU的空闲时间，减少模型offload的频率，同时尽量并行化不同节点的执行，从而最大化提升资源利用效率。&lt;/p&gt;&#xA;&lt;p&gt;然而，随着模型大小和集群大小的增长，Colocated框架也显示出了自己的局限性：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;第一个关键问题是&lt;a href=&#34;http://arxiv.org/abs/2504.15930&#34;&gt;StreamRL&lt;/a&gt;中提到的&lt;strong&gt;资源耦合&lt;/strong&gt;。虽然Colocated框架比起SPMD的程序提升了计算任务的并行性，并通过分组来允许每组model使用不同的资源，但是这并不能完全消除共享设备带来的问题：因为生成和训练同时共享相同设备，我们不能独立扩展或为每个阶段定制资源。同时训练任务（计算密集）和生成任务（IO密集）的瓶颈并不相同，这不利于GPU资源的利用。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;另一个问题是，LLM生成的文本长度是不固定的，尤其随着thinking model的大火，生成任务中不同组的模型生成结果的时间可能差异很大。比如我们有32块GPU，每4块GPU为一组进行生成，如果其中一组生成任务过长，会导致其他28块GPU空等造成资源浪费。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;总的来说，Colocated框架通过精细的资源管理实现了较高的GPU利用率，相对成熟和稳定，确实许多后续框架都借鉴了类似的设计思路。但是，正如前面提到的资源耦合问题，这种架构在可扩展性方面仍有局限。这也为下一代RL框架的发展指明了方向：能否通过打破严格的串行约束，让生成和训练阶段真正独立地并行执行？&lt;/p&gt;&#xA;&lt;h2 id=&#34;online-policy和offline-policy&#34;&gt;Online Policy和Offline Policy&lt;/h2&gt;&#xA;&lt;p&gt;本文的重点是Ray和LLM RL的框架设计，因此不会在这一部分内容做过多阐述，大概而言：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ray Data反压机制</title>
      <link>https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Sat, 05 Jul 2025 15:03:26 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/</guid>
      <description>&lt;p&gt;做Ray Platform也快2年了，遇到过各种的问题，整理一些踩过的坑看一下。&lt;/p&gt;&#xA;&lt;p&gt;先从我们自己最常用的Ray Data开始，看看最常见的OOM/OOD问题，这个问题很多时候都是和反压相关的。&lt;/p&gt;&#xA;&lt;p&gt;说是Ray Data，不过这里的反压不止一层，大概包括下面几个地方：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Ray Core Generator&lt;/strong&gt;：针对Ray Generators的控制，防止后台生成的数据过多导致OOM/OOD。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Streaming Executor + Resource Allocator&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;针对正在执行的任务，控制生成结果的速度，避免单个任务生成的数据过多导致OOM/OOD。&lt;/li&gt;&#xA;&lt;li&gt;针对单个Operator，控制提交任务的数量，避免在资源紧张时提交新任务。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Backpressure Policies&lt;/strong&gt;: 其他关于任务提交的反压规则。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;下面我们逐层分析这些机制的实现。&lt;/p&gt;&#xA;&lt;h1 id=&#34;ray-core-generator对象数量反压&#34;&gt;Ray Core Generator：对象数量反压&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/ray-core/ray-generator.html&#34;&gt;Ray Generator&lt;/a&gt; 类似Python Generator，用来作为迭代器进行遍历，但是和Python Generator有一个很大的不同在于：Ray Generator使用&lt;code&gt;ObjectRefGenerator&lt;/code&gt;在后台持续执行。也就是说如果Ray Data的单个read_task需要读取一个很大的文件时，没法通过控制拉取任务产出的速度来控制任务的内存占用。（不管下游是否主动拉取，都会持续读取新的数据block。）&lt;/p&gt;&#xA;&lt;p&gt;针对这个问题，Ray Generators支持手动配置一个threshold(_generator_backpressure_num_objects parameter)来对Generators进行反压。&lt;/p&gt;&#xA;&lt;p&gt;核心逻辑在&lt;code&gt;task_manager.cc&lt;/code&gt;中的&lt;code&gt;HandleReportGeneratorItemReturns&lt;/code&gt;这个方法里面。这个函数逻辑比较复杂，里面还有比如乱序/幂等等问题的处理，我们只看反压状态的管理：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 请求的item的index&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; item_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; request.item_index();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 生成器已生产的对象数量&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; total_generated &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.TotalNumObjectWritten();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;//已被消费的对象数量  &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; total_consumed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.TotalNumObjectConsumed();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// item已经被消费了，说明消费速度足够快，不用反压。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.IsObjectConsumed(item_index)) {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;OK(), total_consumed);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; false;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Otherwise, follow the regular backpressure logic.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// NOTE, here we check `item_index - last_consumed_index &amp;gt;= backpressure_threshold`,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// instead of the number of unconsumed items, because we may receive the&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// `HandleReportGeneratorItemReturns` requests out of order.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (backpressure_threshold &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      (item_index &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; stream_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.LastConsumedIndex()) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; backpressure_threshold) {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    RAY_LOG(DEBUG) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Stream &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; generator_id&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; is backpressured. total_generated: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; total_generated&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;. total_consumed: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; total_consumed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;. threshold: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; backpressure_threshold;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; signal_it &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ref_stream_execution_signal_callbacks_.find(generator_id);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (signal_it &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; ref_stream_execution_signal_callbacks_.end()) {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;NotFound(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Stream is deleted.&amp;#34;&lt;/span&gt;), &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      signal_it&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;second.push_back(execution_signal_callback);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// No need to backpressure.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    execution_signal_callback(Status&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;OK(), total_consumed);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;所以未消费对象数量达到阈值时，Ray Generator会暂停任务执行。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
