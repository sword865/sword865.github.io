<!DOCTYPE html>
<html lang="en"><head>
	<meta name="generator" content="Hugo 0.145.0">
<title>悟剑阁</title>



  


<meta charset="utf-8">
<meta name="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="">
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5, user-scalable=5" name="viewport">
<meta content="telephone=no" name="format-detection">
<meta name="description" content="">
<meta name="renderer" content="webkit">
<meta name="theme-color" content="#ffffff">















  






    <link type="text/css" rel="stylesheet" href="/vendor/css/bootstrap.min.css">

<link rel="stylesheet" href="/scss/dark-mode.min.cb53f1bee2b8900cb4f082afbf00175d6618f281cf9a2fe8619e3b52d20b5721.css" integrity="sha256-y1PxvuK4kAy08IKvvwAXXWYY8oHPmi/oYZ47UtILVyE=" media="screen">


<link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Material+Icons">


















<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>


</head>
<body>
    	<div id="app"><div class="single-column-drawer-container" id="drawer"
     v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }">
    <div class="drawer-content">
        <div class="drawer-menu">
            
            
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/posts">
                    Archive
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/tags">
                    Tags
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/about/">
                    关于
                </a>
                
            
            
        </div>
    </div>
</div>
<transition name="fade">
    <div id="drawer-mask" v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div>
</transition>
<nav id="navBar" class="navbar sticky-top navbar-light single-column-nav-container">
    <div id="navBackground" class="nav-background"></div>
    <div class="container container-narrow nav-content">
        <button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer">
            <i class="material-icons">
                menu
            </i>
        </button>
        <a id="navTitle" class="navbar-brand" href="https://sword865.github.io/">
            悟剑阁
        </a>
        
        <button type="button" class="nav-darkmode-toggle" id="darkModeToggleButton2">
            <i class="material-icons" id="darkModeToggleIcon2">
                dark_mode
            </i>
        </button>
        
    </div>
</nav>
<div class="single-column-header-container" id="pageHead"
     v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }">
    <a href="https://sword865.github.io/">
        <div class="single-column-header-title">悟剑阁</div>
        

    </a>
</div>

            <div id="content">
                <div id="streamContainer" class="stream-container">

    <div class="post-list-container post-list-container-no-background">
        
        
            <a href="/posts/2025/2025-07-26-ray%E4%B8%8Ellm-rl%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                Ray与LLM强化学习框架设计
                            </div>
                            <div class="post-item-summary">
                                
                                    <p>最近LLM强化学习框架发展特别快，Ray作为被ChatGPT带火的框架，在LLM各个训练阶段中，RL阶段的应用应该是最多的。写篇文章记录一下这块发展的脉络和一些看法。</p>
<h1 id="从google-pathways说起">从Google Pathways说起</h1>
<p>讨论Ray和RL系统，得从Google的<strong>Pathways</strong>系统开始：2021年Google提出了Pathways作为下一代AI架构和分布式ML平台，在相关文献中详细讨论了Single-Controller + MPMD的系统设计。</p>
<p><strong>Single-Controller</strong>（单控制器）是指用一个中央协调器来管理整个分布式计算流程的架构模式。在这种设计中，有一个<strong>主控制节点</strong>负责整个计算图的执行，包括任务分发、资源调度、状态监控等。</p>
<p><strong>Multiple-Controller</strong>（多控制器）则是指使用多个分布式控制节点来协同管理计算任务的架构模式。在这种设计中，没有单一的中央协调器，而是由多个控制器节点分别负责不同的子系统或计算子图，通过分布式协调协议来实现全局一致性。</p>
<p>在Ray中的Driver Process就可以被作为一个典型的Single Controller来启动不同的任务程序，而通过torchrun运行的PyTorch DDP分布式计算则是在每个node上各自执行自己的程序则属于典型的Multiple Controller范式。</p>
<p><strong>MPMD</strong>（Multiple Program, Multiple Data）是一种分布式计算范式，指在一个计算任务中，不同的节点运行不同的程序来处理不同的数据。这种模式下，各个计算节点执行的代码逻辑可能完全不同，每个节点都有自己特定的任务和职责。</p>
<p><strong>SPMD</strong>（Single Program, Multiple Data）则是另一种常见的分布式计算范式，指所有节点运行相同的程序，但处理不同的数据分片。</p>
<p>典型的SPMD任务包括传统的分布式训练，比如PyTorch DDP，每个节点运行相同的程序来处理不同的数据，最多根据rank的值会有一些特别的处理（比如rank=0的节点负责checkpoint）。相比之下，大模型训练包括了流水并行这种更复杂的任务，每个节点组需要运行不同的程序，就更适合用MPMD的方式来实现了。</p>
<p>一般来说，MPMD系统由于包含众多异构组件，各组件间的协调和同步变得相当复杂。为了简化开发复杂度并确保系统执行的一致性，Single-Controller架构成为了自然的选择——通过引入中心化的控制器来统一管理整个分布式计算流程，包括任务调度、状态同步和异常处理等关键环节。</p>
<p>更多的细节就不多说了，有兴趣的话可以去看Oneflow团队当年写的两篇文章，非常深刻：<a href="https://mp.weixin.qq.com/s/roQues5HhRXqGf26DuUOjQ">解读谷歌Pathways架构（一）：Single-controller与Multi-controller</a>和<a href="https://mp.weixin.qq.com/s/N99dRgFYC9zOOcGlg0Ulsw">解读谷歌 Pathways 架构（二）：向前一步是 OneFlow</a>。</p>
<p>这与LLM强化学习有什么关系呢？用RL训练LLM本质上是一个多阶段、多节点的复杂分布式任务。典型的RLHF流水线涉及多个不同模型，计算流程分为几个关键阶段：</p>
<ol>
<li><strong>生成阶段</strong>：当前策略模型（LLM）对一批输入提示生成响应文本</li>
<li><strong>评估阶段</strong>：这些响应由奖励模型评分，或通过人类/自动化偏好模型进行比较评估</li>
<li><strong>训练阶段</strong>：基于获得的奖励信号更新策略模型权重（可能还包括价值函数或评论家网络的更新）</li>
</ol>
<p>这些阶段之间存在明确的数据依赖关系——训练更新必须依赖于生成的样本及其对应的奖励分数。在朴素的实现中，这些阶段只能串行执行，引入大量上下文切换的同时还要求所有的模型使用相同数量的GPU进行计算，计算效率是相当低下的。因此正如Pathways架构所启发的那样，我们希望在保证正确性的前提下，通过良好的系统设计，尽可能地重叠和并行化这些工作阶段，以最大化计算资源的利用效率。</p>
<p>直接说有点抽象，可以看下面这个从<a href="http://arxiv.org/abs/2409.19256">HybridFlow</a>里截的表格，我截了两个最早的RLHF系统，左边的DeepSpeed-Chat实现了SPMD的串行方式，而右边的OpenRLHF则是典型的MPMD系统。</p>
<img width="800"  src="/images/2025/20250726/RLHF_SPMD_MPMD.png" class="center" />
<h1 id="ray与llm强化学习框架">Ray与LLM强化学习框架</h1>
<p>其实从前面的内容可以看出来，Ray的设计很适合用来开发Single-Controller + MPMD的程序，也就自然适合LLM强化学习的场景了。</p>
<p>实际上，社区也确实基于Ray开发了大量的强化学习框架，目前主要的设计包括两种：<strong>Colocated架构</strong>和<strong>Disaggregated架构</strong>。粗略地说，<strong>Colocated架构</strong>意味着把生成阶段和训练阶段放在同样的节点上运行；而<strong>Disaggregated架构</strong>则把它们放在不同的节点上：</p>
<img width="800"  src="/images/2025/20250726/RL_architecture.png" class="center" />
<p>一看这个图，我们会发现Disaggregated Architecture中存在大量的计算bubble，甚至可能比不上之前SPMD模式！这也是为什么很多框架如OpenRLHF、Nemo-aligner、VeRL都是按照Colocated架构来设计的。</p>
<p>需要注意的是，图里的Train和Gen代表的是RLHF的不同阶段，每个阶段内每个GPU可能在运行不同任务，因此整个过程仍然是MPMD的。</p>
<p>以经典的PPO算法为例，整个Train的阶段包括Actor Model(on training Framework)/Reference Model/Reward Model/Critic Model四个模型，Gen阶段包括Actor Model(on inference framework)一个模型，以<a href="http://arxiv.org/abs/2405.11143">OpenRLHF</a>为例：</p>
<img width="800"  src="/images/2025/20250726/RLHF_PPO.png" class="center" />
<p>可以看到这里Actor Model会在Deepspeed和vLLM两个引擎间进行切换，因为实际算法需要保存的模型共有5个。</p>
<h2 id="colocated-rl框架-解法和问题">Colocated RL框架 (解法和问题)</h2>
<p>接下来继续看这个基于Ray的框架：OpenRLHF使用Ray启动和协调组件，但使用Ray的<strong>Placement group</strong>实现了Colocated架构，在每个节点上在rollout和训练任务之间分割GPU资源。例如，在给定节点上，框架可能将每个GPU的0.75分配给训练actor，0.25分配给生成actor，这样有效地让一个训练进程和一个生成进程&quot;共享&quot;每个GPU而不互相干扰。</p>
<p>很容易从前面的图看出来，Colocated框架中的<strong>资源共享</strong>是一个主要的优势，通过设计合适的分组方式，我们可以减少GPU的空闲时间，减少模型offload的频率，同时尽量并行化不同节点的执行，从而最大化提升资源利用效率。</p>
<p>然而，随着模型大小和集群大小的增长，Colocated框架也显示出了自己的局限性：</p>
<ul>
<li>
<p>第一个关键问题是<a href="http://arxiv.org/abs/2504.15930">StreamRL</a>中提到的<strong>资源耦合</strong>。虽然Colocated框架比起SPMD的程序提升了计算任务的并行性，并通过分组来允许每组model使用不同的资源，但是这并不能完全消除共享设备带来的问题：因为生成和训练同时共享相同设备，我们不能独立扩展或为每个阶段定制资源。同时训练任务（计算密集）和生成任务（IO密集）的瓶颈并不相同，这不利于GPU资源的利用。</p>
</li>
<li>
<p>另一个问题是，LLM生成的文本长度是不固定的，尤其随着thinking model的大火，生成任务中不同组的模型生成结果的时间可能差异很大。比如我们有32块GPU，每4块GPU为一组进行生成，如果其中一组生成任务过长，会导致其他28块GPU空等造成资源浪费。</p>
</li>
</ul>
<p>总的来说，Colocated框架通过精细的资源管理实现了较高的GPU利用率，相对成熟和稳定，确实许多后续框架都借鉴了类似的设计思路。但是，正如前面提到的资源耦合问题，这种架构在可扩展性方面仍有局限。这也为下一代RL框架的发展指明了方向：能否通过打破严格的串行约束，让生成和训练阶段真正独立地并行执行？</p>
<h2 id="online-policy和offline-policy">Online Policy和Offline Policy</h2>
<p>本文的重点是Ray和LLM RL的框架设计，因此不会在这一部分内容做过多阐述，大概而言：</p>
                                
                            </div>
                            <div class="post-item-meta">
    2025-07-27
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2025/2025-07-05-ray-data%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                Ray Data反压机制
                            </div>
                            <div class="post-item-summary">
                                
                                    <p>做Ray Platform也快2年了，遇到过各种的问题，整理一些踩过的坑看一下。</p>
<p>先从我们自己最常用的Ray Data开始，看看最常见的OOM/OOD问题，这个问题很多时候都是和反压相关的。</p>
<p>说是Ray Data，不过这里的反压不止一层，大概包括下面几个地方：</p>
<ol>
<li><strong>Ray Core Generator</strong>：针对Ray Generators的控制，防止后台生成的数据过多导致OOM/OOD。</li>
<li><strong>Streaming Executor + Resource Allocator</strong>:
<ul>
<li>针对正在执行的任务，控制生成结果的速度，避免单个任务生成的数据过多导致OOM/OOD。</li>
<li>针对单个Operator，控制提交任务的数量，避免在资源紧张时提交新任务。</li>
</ul>
</li>
<li><strong>Backpressure Policies</strong>: 其他关于任务提交的反压规则。</li>
</ol>
<p>下面我们逐层分析这些机制的实现。</p>
<h1 id="ray-core-generator对象数量反压">Ray Core Generator：对象数量反压</h1>
<p><a href="https://docs.ray.io/en/latest/ray-core/ray-generator.html">Ray Generator</a> 类似Python Generator，用来作为迭代器进行遍历，但是和Python Generator有一个很大的不同在于：Ray Generator使用<code>ObjectRefGenerator</code>在后台持续执行。也就是说如果Ray Data的单个read_task需要读取一个很大的文件时，没法通过控制拉取任务产出的速度来控制任务的内存占用。（不管下游是否主动拉取，都会持续读取新的数据block。）</p>
<p>针对这个问题，Ray Generators支持手动配置一个threshold(_generator_backpressure_num_objects parameter)来对Generators进行反压。</p>
<p>核心逻辑在<code>task_manager.cc</code>中的<code>HandleReportGeneratorItemReturns</code>这个方法里面。这个函数逻辑比较复杂，里面还有比如乱序/幂等等问题的处理，我们只看反压状态的管理：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>  <span style="color:#75715e">// 请求的item的index
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">int64_t</span> item_index <span style="color:#f92672">=</span> request.item_index();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// 生成器已生产的对象数量
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">auto</span> total_generated <span style="color:#f92672">=</span> stream_it<span style="color:#f92672">-&gt;</span>second.TotalNumObjectWritten();
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">//已被消费的对象数量  
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">auto</span> total_consumed <span style="color:#f92672">=</span> stream_it<span style="color:#f92672">-&gt;</span>second.TotalNumObjectConsumed();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// item已经被消费了，说明消费速度足够快，不用反压。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">if</span> (stream_it<span style="color:#f92672">-&gt;</span>second.IsObjectConsumed(item_index)) {
</span></span><span style="display:flex;"><span>    execution_signal_callback(Status<span style="color:#f92672">::</span>OK(), total_consumed);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> false;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Otherwise, follow the regular backpressure logic.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#75715e">// NOTE, here we check `item_index - last_consumed_index &gt;= backpressure_threshold`,
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#75715e">// instead of the number of unconsumed items, because we may receive the
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#75715e">// `HandleReportGeneratorItemReturns` requests out of order.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">if</span> (backpressure_threshold <span style="color:#f92672">!=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span> <span style="color:#f92672">&amp;&amp;</span>
</span></span><span style="display:flex;"><span>      (item_index <span style="color:#f92672">-</span> stream_it<span style="color:#f92672">-&gt;</span>second.LastConsumedIndex()) <span style="color:#f92672">&gt;=</span> backpressure_threshold) {
</span></span><span style="display:flex;"><span>    RAY_LOG(DEBUG) <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;Stream &#34;</span> <span style="color:#f92672">&lt;&lt;</span> generator_id
</span></span><span style="display:flex;"><span>                   <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34; is backpressured. total_generated: &#34;</span> <span style="color:#f92672">&lt;&lt;</span> total_generated
</span></span><span style="display:flex;"><span>                   <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;. total_consumed: &#34;</span> <span style="color:#f92672">&lt;&lt;</span> total_consumed
</span></span><span style="display:flex;"><span>                   <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;. threshold: &#34;</span> <span style="color:#f92672">&lt;&lt;</span> backpressure_threshold;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> signal_it <span style="color:#f92672">=</span> ref_stream_execution_signal_callbacks_.find(generator_id);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (signal_it <span style="color:#f92672">==</span> ref_stream_execution_signal_callbacks_.end()) {
</span></span><span style="display:flex;"><span>      execution_signal_callback(Status<span style="color:#f92672">::</span>NotFound(<span style="color:#e6db74">&#34;Stream is deleted.&#34;</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>    } <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>      signal_it<span style="color:#f92672">-&gt;</span>second.push_back(execution_signal_callback);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  } <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// No need to backpressure.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    execution_signal_callback(Status<span style="color:#f92672">::</span>OK(), total_consumed);
</span></span><span style="display:flex;"><span>  }
</span></span></code></pre></div><p>所以未消费对象数量达到阈值时，Ray Generator会暂停任务执行。</p>
                                
                            </div>
                            <div class="post-item-meta">
    2025-07-05
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                Flash MLA Kernel分析
                            </div>
                            <div class="post-item-summary">
                                
                                    <p>准备对DeepSeek的开源项目整理一些文档，也顺便强化一下记忆，先从FlashMLA开始。</p>
<p>FlashMLA是DeepSeek开源的MLA算子实现，这个实现主要给inference decoding用的，Training和prefill应该是另外一个算子。</p>
<p>先拿下面的图表示一下MLA算子是在计算一个什么东西，这篇文章就不讲具体的推导了，反正这个算子大概就是下面的2个GEMM算子的融合。需要注意的是：</p>
<ol>
<li>这里矩阵K和矩阵V的共享一部分参数。</li>
<li>图里只画显示了一个Query Head和一对KV Head的计算。在实际计算中还要num_kv_head和batch_size两个维度。</li>
<li>两个GEMM中间其实还有一个sotfmax，不过这里可以通过online softmax算法把这块逻辑独立处理分块处理，所以不影响主流程。</li>
</ol>
<img width="600"  src="/images/2025/20250503/mla.png" class="center" />
<p>Kernel的调用主要分两部分</p>
<ol>
<li>调用<code>get_mla_metadata</code>来计算一些metadata，用来优化kernel的执行</li>
<li>调用<code>flash_mla_with_kvcache</code>进行计算</li>
</ol>
<p>在进入调用前，先大概说一下FlashMLA计算的拆分逻辑。这块和FlashDecoding很像，并没有要求一个thread-block必须处理一个完整的sequence，而是通过一个负载均衡算法，把所有的sequence放到一起，然后拆分成一个个的sequence-block，然后每个thread-block就去处理分配给它的那些block的计算，最后再把这些thread-block的结果用合并，得到正确的输出。</p>
<p>大概是下面这个图的样子：</p>
<img width="800"  src="/images/2025/20250503/computation-pattern.png" class="center" />
<p>所以为了完成计算，第一步就是决定每个block需要处理哪些sub-sequence，也就是<code>get_mla_metadata</code>要完成的事情。</p>
<h1 id="get_mla_metadata">get_mla_metadata</h1>
<p>先看<code>get_mla_metadata</code>具体提供了哪些元数据，我们从repo提供的测试代码入手，考虑最简单的情况(batch_size=128, query_sequence_len=1, mean_key_sequence_len=4096, MTP=1, num_kv_head=1, num_q_head=16, TP=1, hidden_NoRoPE_dim=512, hidden_RoPE_dim=64, varlen=False)。</p>
<pre tabindex="0"><code># cache_seqlens = tensor([4096, 4096, ..., 4096], dtype=torch.int32), size=batch_size, value=sequence_len
# s_q=1 (query_sequence_len=1且MTP=1), h_q(num_q_head)=128 (TP=1=128/128) h_kv(num_kv_head)=1
# 基于这些配置，计算mla kernel的metadata
tile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)
</code></pre><p>因为这里我们是在测试decoding步骤，所以有<code>query_sequence_len=1</code>，可以看到三个入参：</p>
<ol>
<li>kv cache的大小</li>
<li>类似GQA的Group数量，这个参数表示每个kv head对应多少个query head。</li>
<li>kv head的数量</li>
</ol>
<p><code>get_mla_metadata</code>会根据GPU中SM的数量和要处理的数据的大小，给每个SM分配任务。这个注意<code>get_mla_metadata_kernel</code>的参数为<code>&lt;&lt;&lt;1, 32, 0, stream&gt;&gt;&gt;</code>，因此所有计算会在1个warp中完成。</p>
<p>这里的关键就是具体怎么给每个(每组)SM分配工作的.</p>
                                
                            </div>
                            <div class="post-item-meta">
    2025-05-03
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                vLLM Paged Attention代码分析
                            </div>
                            <div class="post-item-summary">
                                
                                    <p>3月底整理了一个关于经典Paged Attention算法的ppt, 想起这个几年没写过的blog，把PPT改成一篇文章证明我还活着(-_-)。</p>
<img width="500"  src="/images/2025/20250420/paged_attention.png" class="center" />
<h2 id="vllm-的-paged-attention">vLLM 的 Paged Attention</h2>
<p>开始前先说明一下，vLLM里的Paged Attention Kernel是有好几个不同的版本的，大概是下面这样子：</p>
<p>vLLM早期版本：</p>
<ul>
<li>Prefilling -&gt; Flash Attention的flash_attn_varlen_func</li>
<li>Dedocding -&gt; 自己实现的Paged Attention
<ul>
<li>paged_attention_v1 : 用于比较短的sequence</li>
<li>paged_attention_v2 : 用于不想用v1的情况 :)</li>
</ul>
</li>
</ul>
<p>源码大概是这样的：</p>
<pre tabindex="0"><code>    # NOTE(woosuk): We use a simple heuristic to decide whether to use
    # PagedAttention V1 or V2. If the number of partitions is 1, we use
    # V1 to avoid the overhead of reduction. Also, if the number of
    # sequences or heads is large, we use V1 since there is enough work
    # to parallelize.
    # TODO(woosuk): Tune this heuristic.
    # For context len &gt; 8192, use V2 kernel to avoid shared memory
    # shortage.
    use_v1 = (max_seq_len &lt;= 8192 and (max_num_partitions == 1 or num_seqs * num_heads &gt; 512))
</code></pre><p>vLLM 最新版本就已经全部转向Flash Attention， 用cutlass实现了。</p>
                                
                            </div>
                            <div class="post-item-meta">
    2025-04-20
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2021/2021-03-08-google-small-towers%E4%B8%AD%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%9A%84%E6%8E%A2%E7%B4%A2/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                Google Small Towers中多目标优化的探索
                            </div>
                            <div class="post-item-summary">
                                
                                    <h2 id="背景">背景</h2>
<p>多目标优化中有一个很常见的跷跷板问题，就是说在训练时，多个目标会相互影响，导致震荡&mdash;你降我升，我升你降。有时间还会出现Nan的结果，需要很仔细的调参测试+清洗数据才能训练出一个理想的模型。</p>
<p>针对这种问题，自然就有了一些尝试，比如从帕累托最优的角度寻找优化方向（阿里PEA），修改模型结构使Shared部分存储更泛化的信息（腾讯PLE）。不过这两个写的人都挺多了，就写一下Google Small Towers的这篇文章吧。</p>
<h2 id="主要问题讨论">主要问题讨论</h2>
<p>文章首先讨论了两个问题：</p>
<h3 id="1-over-parameterization对多任务模型的适用性">1. Over-parameterization对多任务模型的适用性</h3>
<p>我们都知道over-parameterization对单任务模型是有价值的，那边对多任务模型是否成立？</p>
<p>这里以将多个目标的线性组合作为优化目标的例子，认为over-parameterization能够帮助处理各任务优化目标之间的冲突问题（既减少跷跷板问题的出现）。</p>
<h3 id="2-大型模型和小型模型的多目标学习表现对比">2. 大型模型和小型模型的多目标学习表现对比</h3>
<p>通过实验对比了大型模型和小型模型进行多目标学习中的不同表现。</p>
<p>实验中，不论是增加任务相关结构的复杂度，还是增加任务共享结构的复杂度，Pareto frontier都会呈现先变好在变差的趋势。</p>
<p>因此，文章认为over-parameterization并不利于多目标学习中的共享性，进而伤害了多目标学习中的泛化能力。因此，在多目标学习中，模型大小实质上是对模型有效性和泛化能力的一种平衡。</p>
<blockquote>
<p>To summarize our insights, for a multi-task learning model, small models benefit from good multi-task generalization but hurts Pareto efficiency; big models theoretically have better Pareto efficiency but could suffer from loss of generalization.</p></blockquote>
<h2 id="under-parameterized-self-auxiliaries模型结构">Under-parameterized Self-auxiliaries模型结构</h2>
<p>文章提出了under-parameterized self-auxiliaries的模型结构：</p>
<p>首先假设模型的共享方式是所有任务共享最下面的表示层（Hard Sharded，MMOE这种，PLE就不行）,既对任务t，有：</p>
<p>$$f_{t}(x; \theta_{sh}, \theta_{t})=f_{t}(h(x; \theta_{sh}); \theta_{t}), \forall t$$</p>
<p>其中 $\theta_t$ 是任务相关的参数， $\theta_sh$ 为共享参数， $h(x;\theta_sh)$ 既为共享的表示层输出。</p>
                                
                            </div>
                            <div class="post-item-meta">
    2021-03-08
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2021/2021-03-07-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%91%A8%E8%BE%B9%E8%AE%BE%E6%96%BD--%E7%89%B9%E5%BE%81%E5%95%86%E5%BA%97/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                推荐系统周边设施--特征商店
                            </div>
                            <div class="post-item-summary">
                                
                                    <p>好久没写博客了，今天写一点推荐系统周边设施的东西。</p>
<h2 id="特征管理">特征管理</h2>
<p>特征商店会存储特征元数据，比如特征的计算逻辑、血缘关系、数据类型。 一般来说，这些元数据用于管理特征的生命周期、计算任务和使用方式。</p>
<h2 id="离线训练数据生成">离线训练数据生成</h2>
<p>为了保证线上线下数据的一致性，推荐系统的训练数据通常有两个数据流Join得到：</p>
<ul>
<li>在Ranking中即实时打点：数据流以<code>traceId</code>为Key，排序时特征为Value。</li>
<li>客户端日志：记录了<code>traceId</code>和事件类型(曝光、点击、分享等）</li>
</ul>
<p>由于客户端日志必然晚于服务端日志，因此两个数据流Join时需要一定的窗口。</p>
<h2 id="训练数据扩展">训练数据扩展</h2>
<p>但是作为调参工程师，我们必然会遇到需要的特征没有记录在实时打点中，导致训练时缺少相关数据的情况，这个时候，就需要想办法来处理这个问题。</p>
<p>按照Uber的方法，我们可以把特征分为三类：</p>
<ol>
<li>离线特征</li>
<li>实时特征</li>
<li>RPC特征</li>
</ol>
<h3 id="离线特征">离线特征</h3>
<p>对于离线特征：我们可以使用Spark读取数据仓库中的历史数据，以天为单位进行生成历史数据，然后放在一个分区的Hive表中。</p>
<h3 id="实时特征">实时特征</h3>
<p>对于实时特征：基于kappa的思想，我们可以在Flink中编写实时特征计算逻辑，然后启动重跑一段时间以前的历史数据，并记录这个过程中特征的每一次变化（有点类似数据库中的WAL日志流），将其输出到Kafka中去，这样我们也就有一个特征在历史时间段中的值。(这里我们最好有一个服务化的Flink平台，来进行任务的添加、删除、修改等工作)</p>
<p>这里，特征的计算任务就可以通过特征元数据库进行管理。</p>
<p>接下来，我们就可以通过带时间戳的Join来完成训练数据和特征数据的拼接，并将特征回写到训练数据中去了。 需要注意的是，为了保证线上线下数据的一致性，我们需要引入一定的延时机制来模拟客户端日志的延迟。</p>
<h3 id="rpc特征">RPC特征</h3>
<p>最后对于来自外部系统的RPC特征：就没有什么好办法了，我们只能在线上添加这个特征的打点，然后跑上一段时间来得到有这个特征的训练数据了。</p>
<p>这里推荐一个比较新的开源项目可以完成类似的工作: <a href="https://github.com/feast-dev/feast">Feast</a></p>
<h2 id="在线特征推送">在线特征推送</h2>
<p>特征的线上存储可以使用KV数据库比如Redis，数据的来源和上面训练数据的扩展可以使用同一套代码，只需要在计算时根据元数据配置来决定是否推送上线。</p>
<p>另外，这里一般会做很多工程上的优化，比如：</p>
<ul>
<li>把多个特征作为一个特征组存在一个key里减少请求的次数</li>
<li>使用一些算法（比如XXHash32）对过长的特征名(比如<code>spu$realtime$orders_last_2w$spu_id</code>)进行压缩</li>
</ul>
                                
                            </div>
                            <div class="post-item-meta">
    2021-03-07
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2018/2018-11-08-npe%E9%97%AE%E9%A2%98%E4%B8%8E%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                NPE问题与一些语言中的解决方案
                            </div>
                            <div class="post-item-summary">
                                
                                    <p>NPE(NullPointerException)是一个很烦人的问题，这里简单列举了一些语言中对NPE的处理。</p>
<h2 id="1-通过语法标记进行检查">1. 通过语法标记进行检查</h2>
<h3 id="kotlin">Kotlin</h3>
<p>Kotlin要求可以为null的变量必需在定义时声明，同时在读取该类型变量属性时必须进行空值判断。例：String 和 String?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-kotlin" data-lang="kotlin"><span style="display:flex;"><span><span style="color:#66d9ef">var</span> a: String = <span style="color:#e6db74">&#34;abc&#34;</span>
</span></span><span style="display:flex;"><span>a = <span style="color:#66d9ef">null</span> <span style="color:#75715e">// compilation error, a can not be null
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">var</span> b: String? = <span style="color:#e6db74">&#34;abc&#34;</span>
</span></span><span style="display:flex;"><span>b = <span style="color:#66d9ef">null</span> <span style="color:#75715e">// ok
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">val</span> l = b.length <span style="color:#75715e">// compiler error: variable &#39;b&#39; can be null
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">val</span> l = <span style="color:#66d9ef">if</span> (b <span style="color:#f92672">!=</span> <span style="color:#66d9ef">null</span>) b.length <span style="color:#66d9ef">else</span> -<span style="color:#ae81ff">1</span> <span style="color:#75715e">// ok
</span></span></span></code></pre></div><h3 id="jetbrains-annotations-for-java">Jetbrains annotations for Java</h3>
<p>IntelliJ IDEA提供了一些工具，比如可以对@NotNull的参数进行检查，当出现null赋值时在IDE中会给出提示。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-java" data-lang="java"><span style="display:flex;"><span><span style="color:#f92672">import</span> org.jetbrains.annotations.NotNull;
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> java.util.ArrayList;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span> <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Test</span>{
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">foo</span>(<span style="color:#a6e22e">@NotNull</span> Object param){
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> param.<span style="color:#a6e22e">hashCode</span>();
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">test</span>(){
</span></span><span style="display:flex;"><span>        foo(<span style="color:#66d9ef">null</span>); <span style="color:#75715e">// warn in IntelliJ IDEA</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>（类似的，FindBugs也提供了@Nonnull注释，用于检查）</p>
                                
                            </div>
                            <div class="post-item-meta">
    2018-11-08
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2017/2017-01-19-%E6%AF%94%E8%BE%83%E4%B8%80%E4%B8%8Bspark2%E7%9A%84dataframe%E5%92%8Crdd/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                比较一下spark2的DataFrame和RDD
                            </div>
                            <div class="post-item-summary">
                                
                                    研究一下Spark2.x中成为主流的DataSet,DataFrame与原来的RDD之间的差别
                                
                            </div>
                            <div class="post-item-meta">
    2017-03-12
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
            <a href="/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/" class="a-block">
                <div class="post-item-wrapper">
                    <div class="post-item post-item-no-divider">
                        <div class="post-item-info-wrapper">
                            <div class="post-item-title">
                                谈谈Factorization Machine
                            </div>
                            <div class="post-item-summary">
                                
                                    聊聊因子分解机模型的基本形式和一些变化
                                
                            </div>
                            <div class="post-item-meta">
    2016-11-04
    &emsp;
    
    
    
    &emsp;
    
</div>

                        </div>
                        
                        
                    </div>
                </div>
        
        </a>
    </div>


                    </div>
            </div><div id="sideContainer" class="side-container">
    
    <a class="a-block nav-head active" href="https://sword865.github.io/">
    
        <div class="nav-title">
            悟剑阁
        </div>
        
    </a>

    <div class="nav-link-list">
        
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/posts">
                Archive
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/tags">
                Tags
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/about/">
                关于
            </a>
            
        
    </div>

    

    <div class="nav-footer">
        
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	Copyright (c) 2015. All rights reserved.
	

    </div>
    
</div><div id="extraContainer" class="extra-container">
    <div class="toc-wrapper">
        

        
    </div>
    <div class="pagination">
        <a id="globalBackToTop" class="pagination-action animated-visibility" href="#top"
            :class="{ invisible: scrollY == 0 }">
            <i class="material-icons pagination-action-icon">
                keyboard_arrow_up
            </i>
        </a>
        
        <a type="button" class="pagination-action" id="darkModeToggleButton">
            <span class="material-icons pagination-action-icon" id="darkModeToggleIcon">
                dark_mode
            </span>
        </a>
        
        
        

        <a class="pagination-action" 
            style="visibility:hidden">
            
            <i class="material-icons pagination-action-icon">
                chevron_left
            </i>
        </a>

        <div class="pagination-indicator">
            <span style="text-align: center">
                1<br>
                <div style="display: inline-block; transform: rotate(-28deg)">-</div><br>5
            </span>
        </div>


        <a class="pagination-action"  href="/page/2/">
            
            <i class="material-icons pagination-action-icon">
                chevron_right
            </i>
        </a>
        
        
    </div>
</div>


<div class="pagination">
    
    <a class="pagination-action" style="opacity:0">
    
        <i class="material-icons pagination-action-icon">
            chevron_left
        </i>
    </a>

        <div class="pagination-indicator">
            <span>1/5</span>
        </div>

        
        <a class="pagination-action" href="/page/2/"
           style="opacity:1">
        
                <i class="material-icons pagination-action-icon">
                    chevron_right
                </i>
            </a>
</div>



<div id="single-column-footer">
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	Copyright (c) 2015. All rights reserved.
	
</div>
            </div>
    
    <script src="/js/journal.js"></script></body>
</html>
