<!DOCTYPE html>
<html lang="en"><head>
<title>模型分片，KV Cache和推理加速的一些思考：计算与数据</title>




<meta charset="utf-8">
<meta name="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="">
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5, user-scalable=5" name="viewport">
<meta content="telephone=no" name="format-detection">
<meta name="description" content="">
<meta name="renderer" content="webkit">
<meta name="theme-color" content="#ffffff">















  






      <script src="/js/toc.js"></script>
    
    <link type="text/css" rel="stylesheet" href="/vendor/css/bootstrap.min.css">

<link rel="stylesheet" href="/scss/dark-mode.min.cb53f1bee2b8900cb4f082afbf00175d6618f281cf9a2fe8619e3b52d20b5721.css" integrity="sha256-y1PxvuK4kAy08IKvvwAXXWYY8oHPmi/oYZ47UtILVyE=" media="screen">


<link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Material+Icons">


















<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>


</head>
<body>
    	<div id="app"><div class="single-column-drawer-container" id="drawer"
     v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }">
    <div class="drawer-content">
        <div class="drawer-menu">
            
            
            
                
                
                
                    
                
                
                
                <a class="a-block drawer-menu-item active" href="/posts">
                    Archive
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/tags">
                    Tags
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/about/">
                    关于
                </a>
                
            
            
            <div class="toc">


	<div class="toc-content">
	
		
		
		
		<center>- CATALOG -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#1-%e9%9c%80%e6%b1%82%e7%9a%84%e6%bc%94%e5%8f%98%e4%bb%8e-chat-%e5%88%b0-agent" class="nav-1-需求的演变从-chat-到-agent">
									1. 需求的演变：从 Chat 到 Agent
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%ac%ac%e4%b8%80%e9%98%b6%e6%ae%b5%e5%8d%95%e8%bd%ae%e6%8c%87%e4%bb%a4" class="nav-第一阶段单轮指令">
									第一阶段：单轮指令
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e7%ac%ac%e4%ba%8c%e9%98%b6%e6%ae%b5%e5%a4%9a%e8%bd%ae%e5%af%b9%e8%af%9d" class="nav-第二阶段多轮对话">
									第二阶段：多轮对话
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e7%ac%ac%e4%b8%89%e9%98%b6%e6%ae%b5agentic-workflow" class="nav-第三阶段agentic-workflow">
									第三阶段：Agentic Workflow
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#2-%e6%a0%b8%e5%bf%83%e5%91%bd%e9%a2%98%e7%a7%bb%e5%8a%a8%e8%ae%a1%e7%ae%97-vs-%e7%a7%bb%e5%8a%a8%e5%ad%98%e5%82%a8" class="nav-2-核心命题移动计算-vs-移动存储">
									2. 核心命题：移动计算 vs 移动存储
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#3-shift-parallelism%e8%8a%82%e7%82%b9%e5%86%85%e7%9a%84%e7%a7%bb%e5%8a%a8%e8%ae%a1%e7%ae%97" class="nav-3-shift-parallelism节点内的移动计算">
									3. Shift Parallelism：节点内的移动计算
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%ad%98%e5%82%a8%e5%85%b1%e4%ba%abkv-cache-invariance" class="nav-存储共享kv-cache-invariance">
									存储共享：KV Cache Invariance
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#4-llm-d%e9%9b%86%e7%be%a4%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%b5%81" class="nav-4-llm-d集群间的数据流">
									4. llm-d：集群间的“数据流”
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%89%93%e7%a0%b4-data-locality-%e6%89%a7%e5%bf%b5" class="nav-打破-data-locality-执念">
									打破 Data Locality 执念
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e8%af%ad%e4%b9%89%e5%8c%96-kv-cache" class="nav-语义化-kv-cache">
									语义化 KV Cache
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#5-%e6%80%9d%e8%80%83%e4%b8%8e%e6%9c%aa%e6%9d%a5%e9%87%8d%e6%9e%84%e6%8e%a8%e7%90%86%e7%b3%bb%e7%bb%9f%e7%9a%84%e6%97%b6%e7%a9%ba%e8%a7%82" class="nav-5-思考与未来重构推理系统的时空观">
									5. 思考与未来：重构推理系统的时空观
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
            
        </div>
    </div>
</div>
<transition name="fade">
    <div id="drawer-mask" v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div>
</transition>
<nav id="navBar" class="navbar sticky-top navbar-light single-column-nav-container">
    <div id="navBackground" class="nav-background"></div>
    <div class="container container-narrow nav-content">
        <button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer">
            <i class="material-icons">
                menu
            </i>
        </button>
        <a id="navTitle" class="navbar-brand" href="https://sword865.github.io/">
            悟剑阁
        </a>
        
        <button type="button" class="nav-darkmode-toggle" id="darkModeToggleButton2">
            <i class="material-icons" id="darkModeToggleIcon2">
                dark_mode
            </i>
        </button>
        
    </div>
</nav>
<div class="single-column-header-container" id="pageHead"
     v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }">
    <a href="https://sword865.github.io/">
        <div class="single-column-header-title">悟剑阁</div>
        

    </a>
</div>

            <div id="content">
                <div id="streamContainer" class="stream-container">

    <div class="post-list-container post-list-container-shadow">
        <div class="post">
            
            
            

            <div class="post-head-wrapper-text-only"
                
            >
                <div class="post-title">
                    模型分片，KV Cache和推理加速的一些思考：计算与数据
                    
                    <div class="post-meta">
                        
                        <time itemprop="datePublished">
                            2026-01-29 21:55
                        </time>
                        

                        

                        
                            <i class="material-icons" style="">label</i>
                            
                                <a href="/tags/llm">LLM</a>
                                &nbsp;
                            
                                <a href="/tags/kv-cache">KV Cache</a>
                                &nbsp;
                            
                                <a href="/tags/inference">Inference</a>
                                &nbsp;
                            
                                <a href="/tags/vllm">vllm</a>
                                &nbsp;
                            
                                <a href="/tags/llm-d">llm-d</a>
                                &nbsp;
                            
                        
                        
                    </div>
                </div>
            </div>
            
            <div class="post-body-wrapper">
                
                <div class="post-body" v-pre>
                
                    <p>这段时间主要在做推理加速相关的工作，也做了一些实验，准备写点文章做一些记录。这篇文章就从“移动计算”和“移动存储”的视角开个头，聊聊<strong>并行策略的动态切换</strong>和<strong>KV Cache 流动管理</strong>的一些实践，梳理一下在 Agentic Workflow 日益普及的当下，我们能否通过对<strong>算力、存储与带宽</strong>的调度，在大规模集群上更好地提升推理效率。</p>
<p>在传统的大数据时代，“计算中心”还是“数据中心”一直是个有趣的问题。随着技术的发展，大家也逐渐总结出了 <strong>&ldquo;Move Compute to Data&rdquo;</strong> 的实践经验：因为 SSD 很贵、IO 很贵、带宽也很贵。相比之下，不如把代码调度过去，使用本地的 CPU 进行计算。因此，调度器的核心任务是保证 Data Locality，尽量把计算分发到数据所在的硬盘旁边。</p>
<p>但在 LLM 的时代，我们面对的是超大模型在 GPU 上的推理，移动计算已经变成了移动模型（权重）这个巨无霸。相比之下，似乎还是移动数据（KV Cache）更现实一些——但什么才是最合适的解法呢？</p>
<h2 id="1-需求的演变从-chat-到-agent">1. 需求的演变：从 Chat 到 Agent</h2>
<p>系统架构的演进，必然是随着业务不断演变的。</p>
<h3 id="第一阶段单轮指令">第一阶段：单轮指令</h3>
<ul>
<li><strong>特征</strong>：用户发送一句指令任务（如翻译、总结），模型执行并回复。请求之间几乎独立。</li>
<li><strong>瓶颈</strong>：纯粹的算力（Prefill）或显存带宽（Decoding）。</li>
<li><strong>调度</strong>：最简单的加权轮询。此时 <strong>KV Cache</strong> 的存在感很低，除了每台机器都有的 System Prompt，几乎没有状态复用的需求，我们可以随意把请求调度到任一台机器上执行。</li>
</ul>
<h3 id="第二阶段多轮对话">第二阶段：多轮对话</h3>
<ul>
<li><strong>特征</strong>：多轮对话可以通过 Prefix Caching 复用前面上文。Context 越来越长，每次对话对应一次交互。</li>
<li><strong>瓶颈</strong>：显存容量 &ndash;&gt; Prefill 时间</li>
<li><strong>调度</strong>：开始引入 <strong>Affinity（亲和性）</strong> 调度——为了命中 Cache，我们尽量把请求发给存储了该用户历史数据的节点。也就是 <strong>&ldquo;Move Compute to Data&rdquo;</strong>，因为此时 Prefill（重算数据）太贵，而搬运 KV Cache 也还没在大规模集群中普及。但是这也会导致热点问题。</li>
</ul>
<h3 id="第三阶段agentic-workflow">第三阶段：Agentic Workflow</h3>
<ul>
<li><strong>特征</strong>：系统提示词、工具定义、思维链、上下文可以在并行的分支任务中共享，多轮对话可以并行执行，但对应一次交互（用户从感知多次交互变成感知任务完成）。</li>
<li><strong>瓶颈</strong>：极其复杂的依赖关系，以及直接复用 KV Cache 带来的负载不均衡。</li>
<li><strong>调度（面临的问题）</strong>：如果 &ldquo;Move Compute to Data&rdquo; 会导致严重的热点问题——存有热门 Context 的节点会被打爆，而其他空闲节点却因为没有数据而帮不上忙。</li>
</ul>
<p>可以看到，随着需求的变化，我们不再只关注单次请求的 TTFT/TPOT，而是开始关注整个 Agent 任务的 <strong>任务完成时间</strong> 以及系统的 <strong>总吞吐</strong>量。</p>
<h2 id="2-核心命题移动计算-vs-移动存储">2. 核心命题：移动计算 vs 移动存储</h2>
<p>为了解决 Agent 时代的冲突，我们必须重新审视 <strong>调度策略的核心权衡</strong>。在大数据时代，移动数据（IO）是昂贵的；但在 LLM 时代，事情变得愈加复杂：</p>
<ol>
<li>移动计算（权重）也变得更加复杂（移动权重、计算图编译等）。</li>
<li>计算资源逐渐替代存储成为主要成本（某存储团队原话：我们终于不是最贵的了）。</li>
<li>NVLink/RDMA 让移动数据（KV Cache）变得越来越便宜。</li>
</ol>
<p>此外，对于 LLM 来说，不同的并行策略会带来不同的性能表现，并适配不同的请求类型（比如 batch 大小、提示词长度、预填充、解码等），因此这里我们可以定义两种“移动计算”方式：</p>
<ul>
<li>把计算移动到更合适、已经部署了模型的节点（如 PD 分离），无需移动权重。</li>
<li>把计算移动到新的、空白的节点（其实就是传统的 Auto Scaling）。</li>
</ul>
<p>在目前的实践中，我们发现，在前面第 1 种约束下，后面这一种移动计算的成本巨大。再加上我们希望最大化系统资源利用率，通常只有在任务无法被当前系统处理时，才会把它作为最终手段。所以这里先不展开，重点放在第一种移动计算上，用它来约束移动计算的成本。</p>
<p>这样一来，调度器就面临一个二选一的困境：</p>
<table>
  <thead>
      <tr>
          <th>策略</th>
          <th>A. 优先调度<strong>算力效率高</strong>的节点 (Compute-First)</th>
          <th>B. 优先调度<strong>存储效率高</strong>的节点 (Data-First)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>代价</strong></td>
          <td>需要<strong>移动数据</strong>（KV Cache 迁移，消耗带宽）</td>
          <td>可能需要<strong>等待计算</strong>（排队）或进行<strong>低效计算</strong></td>
      </tr>
      <tr>
          <td><strong>适用场景</strong></td>
          <td>带宽充足、延迟敏感、算力稀缺</td>
          <td>带宽紧张、吞吐优先、算力充足</td>
      </tr>
      <tr>
          <td><strong>经典案例</strong></td>
          <td>PD 分离、过热点保护</td>
          <td>多轮对话（KV Cache 复用）</td>
      </tr>
  </tbody>
</table>
<p>这个权衡并非一成不变，它取决于以下几个关键变量：</p>
<ol>
<li>
<p><strong>算力 vs 带宽的相对稀缺性</strong>：</p>
<ul>
<li>如果 GPU 算力极度紧张（如高峰期），让算力空转等待 Cache 命中是巨大浪费，此时应倾向于 <strong>A</strong>。</li>
<li>如果跨节点带宽是瓶颈（如跨机房），频繁移动 KV Cache 会引入不可接受的延迟，此时应倾向于 <strong>B</strong>。</li>
</ul>
</li>
<li>
<p><strong>Batching 策略</strong>：</p>
<ul>
<li><strong>大 Batch / 高吞吐场景</strong>：可以容忍一定的调度延迟来凑齐 Batch，倾向于 <strong>B</strong>。</li>
<li><strong>小 Batch / 低延迟场景</strong>：每一毫秒都很宝贵，必须立即找到可用算力，倾向于 <strong>A</strong>。</li>
</ul>
</li>
<li>
<p><strong>延迟 vs 吞吐的优化目标</strong>：</p>
</li>
</ol>
<h2 id="3-shift-parallelism节点内的移动计算">3. Shift Parallelism：节点内的移动计算</h2>
<p>Snowflake 提出的 <strong><a href="https://arxiv.org/abs/2509.16495">Shift Parallelism</a></strong> 是一个非常巧妙的设计，通过对计算进行预先编排（移动计算），把运行时移动数据的成本降到了 0。</p>
<p>传统的推理部署要么是 <strong>TP</strong>（Tensor Parallelism，低延时），要么是 <strong>SP/DP</strong>（Sequence/Data Parallelism，高吞吐）：</p>
<ul>
<li><strong>低负载时</strong>：我们想要 TP 来降低 Latency。</li>
<li><strong>高负载时</strong>：我们想要 SP 来提升 Throughput。</li>
</ul>
<p>Shift Parallelism 的核心观点是：<strong>与其移动数据（KV Cache），不如改变计算方式（并行策略）。</strong></p>
<h3 id="存储共享kv-cache-invariance">存储共享：KV Cache Invariance</h3>
<p>Shift Parallelism 通过对不同并行策略的研究，发现 Ulysses SP 策略下 KV Cache 的内存布局与经典 TP 模式下完全一致。这意味着在运行时从 TP 切换到 SP 时，<strong>KV Cache 无需移动或重排</strong>，可以通过巧妙的计算编排，避免对存储进行移动。</p>
<img width="800"  src="/images/2026/20260129/SP_TP_compare.png" class="center" />
<p>可以看出来，Shift Parallelism 选择了“存储共享”：通过预先对计算进行编排，把计算从一个进程移动到另一个进程来提升整体算力，降低计算能力的稀缺性。</p>
<p>不过，为了支持 TP 和 SP 的无缝切换，GPU 必须同时存储 TP 和 SP 所需要的不同权重。</p>
<p><strong>一些实验观察</strong>：
我也基于相关代码 <a href="https://github.com/snowflakedb/ArcticInference">ArcticInference</a> 做了一些实验和分析。在目前的实现中，TP 和 SP 实际上是存了两份权重，其中在每块 GPU 上，SP 所需要的权重又是 TP 的超集。</p>
<p>虽然理论上可以只保留 SP 一份权重，但对性能会有极大的损耗，这里主要问题在于 GEMM 计算时对<strong>权重矩阵连续性</strong>的要求：</p>
<ul>
<li>计算 Kernel（如 cuBLAS）通常要求输入 Tensor 在内存中是连续的，直接切分 SP 的权重矩阵无法满足这一要求。</li>
<li><em>虽然理论上可以通过编写定制的 <strong>CUDA Kernel</strong> 来处理这种复杂的非连续内存映射，但工程复杂度较高，而且分散的内存读取极易破坏合并访问，仍然会导致实际带宽利用率下降。</em></li>
</ul>
<p>因此，Shift Parallelism 使用 <strong>显存空间（冗余权重）</strong> 换取了 <strong>计算移动的灵活性（动态并行策略）</strong>，从而避免了高昂的运行时数据搬运。</p>
<h2 id="4-llm-d集群间的数据流">4. llm-d：集群间的“数据流”</h2>
<p>如果说 Shift Parallelism 延续的是“让计算适应数据”的思路，那么 <strong>llm-d</strong> 则提出了“让数据移向计算”：作为 <a href="https://docs.d.run/en/blogs/2026/llm-d#the-vision-a-context-aware-inference-ecosystem">Agentic Runtime</a>，主张 <strong>KV Cache 在集群间流动</strong>。</p>
<h3 id="打破-data-locality-执念">打破 Data Locality 执念</h3>
<p>前文提到，Agent 工作流的负载较为复杂，而且极不均衡。llm-d 作为集群的入口，不再只关心单次请求的流量，而是希望整体优化任务完成时间。其社区最新的提案中，提倡利用 P2P NVLink/RDMA 实现 KV Cache 的快速流转。这意味着调度器可以更自由地选择<strong>最空闲</strong>或<strong>逻辑最近</strong>的计算节点，而不是被历史遗留的 Cache 死死绑在某个过载的节点上。</p>
<p>这让人想起云原生数据库从“Shared-Nothing”走向“存算分离（Shared-Storage/Memory）”的演进路线。未来的推理集群，很可能是一个巨大的、通过高速总线互联的共享显存池。</p>
<h3 id="语义化-kv-cache">语义化 KV Cache</h3>
<p>为了让这种“流动”更高效，社区认为 llm-d 不能傻傻地移动所有数据，而是应该让集群调度器理解 Agent 工作流，理解 KV Cache：</p>
<ul>
<li><strong>System Prompt / Tool 定义</strong>：高频复用的“静态”数据 -&gt; **主动复制（推送）**到多个节点。</li>
<li><strong>Reasoning Branch</strong>：用完即弃的“瞬态”数据 -&gt; <strong>低优先级驱逐</strong>。</li>
</ul>
<p>这里又回到了权衡：对于高频数据，我们通过复制来减少传输；对于低频数据，我们通过传输来平衡算力。</p>
<p>这实际上对 llm-d 的调度提出了更高的要求：KV Cache 的复制不再是请求到来时的被动转移，系统需要具备类似于指令预取的调度预测能力，通过提前对 KV Cache 进行编排来保证缓存的可用性。</p>
<h2 id="5-思考与未来重构推理系统的时空观">5. 思考与未来：重构推理系统的时空观</h2>
<p>将两者的思路结合来看，未来的推理系统架构可能会展现出以下特征：</p>
<ol>
<li><strong>层次化的灵活性</strong>:
<ul>
<li><strong>在节点内部</strong>: 利用类似 Shift Parallelism 的技术，<strong>保持 KV Cache 不动</strong>，动态调整计算图（TP/SP），平衡延迟与吞吐。</li>
<li><strong>在集群层面</strong>: 利用语义化 KV Cache 和 P2P 网络，<strong>主动移动 KV Cache</strong>，平衡节点间的负载。</li>
</ul>
</li>
<li><strong>KV Cache 从“静态缓存”变为“流动的数据”</strong>：
它不再是单纯被动被命中的 Cache，而是被系统调度的核心资源。它可以作为静态数据驻留，也可以按照实际需求动态流转。</li>
<li><strong>智能调度与流量塑性</strong>：
为了更智能地进行调度，未来的调度策略不会只停留在被动转发流量，而是需要主动参与数据聚合和 KV Cache 的流动，让不同类型的负载在时间维度上更好地重叠与错峰，从而摊薄一些难以避免的开销。</li>
<li><strong>计算语境的代价与以空间换时间</strong>：
无论是 Shift Parallelism 中冗余的权重，还是 llm-d 中主动复制的上下文，其本质都在说明：<strong>权重和上下文本质上都是计算语境的一部分</strong>。
<ul>
<li>运行时重构语境（重新切分权重、重算上下文）极其昂贵（占用 SM、需要重新编译 CUDA 图）。</li>
<li>因此，<strong>&ldquo;以空间换时间&rdquo;</strong> 将成为常态。即使显存昂贵，但为了更好更便利的计算流动性，我们依然愿意支付这份“存储税”。</li>
</ul>
</li>
</ol>
<p>最后简单提一下百度最新的研究（如 <a href="https://arxiv.org/pdf/2512.16134">SPS</a>）。文章中提出的策略已经不再是简单的实时流量转发，而是通过感知负载的方式对流量进行重组：调度器可以把请求缓存在本地，等满足一定条件后再统一下发，主动将负载塑造成适合当前系统状态的形式。这相当于把原来在服务端做的动态攒批能力，上移到了网关调度层。</p>
<p>延续这些研究的思路，未来推理系统与调度服务之间的边界会变得越来越模糊，“网关 + 推理引擎”这两层调度器之间也会越来越紧密。调度的思路也会从被动处理请求，转向主动管理计算和数据：在上游通过攒批塑造流量形态，在中游通过并行策略切换匹配算力形态，在下游通过 KV Cache 的流动与复制打通存储形态。</p>
<p>看上去推理加速正在从单纯的“算子优化”走向“系统级编排”，这个演变的核心，仍然围绕着计算和存储的移动。在叠加多模态、稀疏注意力、线性注意力、Engram 等架构优化之后，我们以后看到的也许是一个巨大的推理集群，而不再是一个“网关 -&gt; 推理引擎”的两层服务。</p>
<p>总之我们在 26 年还有很多可以继续探索的空间。</p>

                    
                    <HR width="100%" id="EOF">
		    <p style="color:#777;">Last modified on 2026-01-29</p>
                    
                </div>
            </div>
            
            
            <nav class="post-pagination">

                
                <a class="newer-posts" href="/posts/2025/2026-02-10-linear-attention%E5%9F%BA%E7%A1%80-%E9%83%A8%E7%BD%B2%E7%AF%87/">
			Next<br>Linear Attention基础-部署篇
                </a>
                
                
                
                <a class="older-posts" href="/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/">
			Previous<br>Linear Attention基础-工程篇
                </a>
                
            </nav>
            <div class="post-comment-wrapper">
                












            </div>
        </div>
    </div>


                    </div>
            </div><div id="sideContainer" class="side-container">
    
    <a class="a-block nav-head false" href="https://sword865.github.io/">
    
        <div class="nav-title">
            悟剑阁
        </div>
        
    </a>

    <div class="nav-link-list">
        
        
            
            
            
                
            
            
            
            <a class="a-block nav-link-item active" href="/posts">
                Archive
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/tags">
                Tags
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/about/">
                关于
            </a>
            
        
    </div>

    

    <div class="nav-footer">
        
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	Copyright (c) 2015. All rights reserved.
	

    </div>
    
</div><div id="extraContainer" class="extra-container">
    <div class="toc-wrapper">
        

        
        <div class="toc">


	<div class="toc-content">
	
		
		
		
		<center>- CATALOG -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#1-%e9%9c%80%e6%b1%82%e7%9a%84%e6%bc%94%e5%8f%98%e4%bb%8e-chat-%e5%88%b0-agent" class="nav-1-需求的演变从-chat-到-agent">
									1. 需求的演变：从 Chat 到 Agent
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%ac%ac%e4%b8%80%e9%98%b6%e6%ae%b5%e5%8d%95%e8%bd%ae%e6%8c%87%e4%bb%a4" class="nav-第一阶段单轮指令">
									第一阶段：单轮指令
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e7%ac%ac%e4%ba%8c%e9%98%b6%e6%ae%b5%e5%a4%9a%e8%bd%ae%e5%af%b9%e8%af%9d" class="nav-第二阶段多轮对话">
									第二阶段：多轮对话
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e7%ac%ac%e4%b8%89%e9%98%b6%e6%ae%b5agentic-workflow" class="nav-第三阶段agentic-workflow">
									第三阶段：Agentic Workflow
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#2-%e6%a0%b8%e5%bf%83%e5%91%bd%e9%a2%98%e7%a7%bb%e5%8a%a8%e8%ae%a1%e7%ae%97-vs-%e7%a7%bb%e5%8a%a8%e5%ad%98%e5%82%a8" class="nav-2-核心命题移动计算-vs-移动存储">
									2. 核心命题：移动计算 vs 移动存储
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#3-shift-parallelism%e8%8a%82%e7%82%b9%e5%86%85%e7%9a%84%e7%a7%bb%e5%8a%a8%e8%ae%a1%e7%ae%97" class="nav-3-shift-parallelism节点内的移动计算">
									3. Shift Parallelism：节点内的移动计算
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e5%ad%98%e5%82%a8%e5%85%b1%e4%ba%abkv-cache-invariance" class="nav-存储共享kv-cache-invariance">
									存储共享：KV Cache Invariance
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#4-llm-d%e9%9b%86%e7%be%a4%e9%97%b4%e7%9a%84%e6%95%b0%e6%8d%ae%e6%b5%81" class="nav-4-llm-d集群间的数据流">
									4. llm-d：集群间的“数据流”
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%89%93%e7%a0%b4-data-locality-%e6%89%a7%e5%bf%b5" class="nav-打破-data-locality-执念">
									打破 Data Locality 执念
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e8%af%ad%e4%b9%89%e5%8c%96-kv-cache" class="nav-语义化-kv-cache">
									语义化 KV Cache
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#5-%e6%80%9d%e8%80%83%e4%b8%8e%e6%9c%aa%e6%9d%a5%e9%87%8d%e6%9e%84%e6%8e%a8%e7%90%86%e7%b3%bb%e7%bb%9f%e7%9a%84%e6%97%b6%e7%a9%ba%e8%a7%82" class="nav-5-思考与未来重构推理系统的时空观">
									5. 思考与未来：重构推理系统的时空观
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
        
    </div>
    <div class="pagination">
        <a id="globalBackToTop" class="pagination-action animated-visibility" href="#top"
            :class="{ invisible: scrollY == 0 }">
            <i class="material-icons pagination-action-icon">
                keyboard_arrow_up
            </i>
        </a>
        
        <a type="button" class="pagination-action" id="darkModeToggleButton">
            <span class="material-icons pagination-action-icon" id="darkModeToggleIcon">
                dark_mode
            </span>
        </a>
        
        
    </div>
</div>

<div id="single-column-footer">
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	Copyright (c) 2015. All rights reserved.
	
</div>
            </div>
    
    <script src="/js/journal.js"></script></body>
</html>
