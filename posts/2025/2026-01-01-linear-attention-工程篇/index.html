<!DOCTYPE html>
<html lang="en"><head>
<title>Linear Attention基础-工程篇</title>




<meta charset="utf-8">
<meta name="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="">
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5, user-scalable=5" name="viewport">
<meta content="telephone=no" name="format-detection">
<meta name="description" content="">
<meta name="renderer" content="webkit">
<meta name="theme-color" content="#ffffff">















  






      <script src="/js/toc.js"></script>
    
    <link type="text/css" rel="stylesheet" href="/vendor/css/bootstrap.min.css">

<link rel="stylesheet" href="/scss/dark-mode.min.cb53f1bee2b8900cb4f082afbf00175d6618f281cf9a2fe8619e3b52d20b5721.css" integrity="sha256-y1PxvuK4kAy08IKvvwAXXWYY8oHPmi/oYZ47UtILVyE=" media="screen">


<link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Material+Icons">


















<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>


</head>
<body>
    	<div id="app"><div class="single-column-drawer-container" id="drawer"
     v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }">
    <div class="drawer-content">
        <div class="drawer-menu">
            
            
            
                
                
                
                    
                
                
                
                <a class="a-block drawer-menu-item active" href="/posts">
                    Archive
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/tags">
                    Tags
                </a>
                
            
                
                
                
                
                
                <a class="a-block drawer-menu-item false" href="/about/">
                    关于
                </a>
                
            
            
            <div class="toc">


	<div class="toc-content">
	
		
		
		
		<center>- CATALOG -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#prefill%e4%b8%8edecoding" class="nav-prefill与decoding">
									Prefill与Decoding
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#linear-attention%e5%b8%b8%e8%a7%81%e7%ae%97%e6%b3%95" class="nav-linear-attention常见算法">
									Linear Attention常见算法
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#prefix-scan--cumsum-%e5%89%8d%e7%bc%80%e5%92%8c" class="nav-prefix-scan--cumsum-前缀和">
									Prefix Scan / Cumsum 前缀和
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%ae%97%e6%b3%95%e7%ae%80%e4%bb%8b" class="nav-算法简介">
									算法简介
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#linear-attention%e7%9a%84prefix-scan" class="nav-linear-attention的prefix-scan">
									Linear Attention的Prefix Scan
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc%e7%ba%bf%e6%80%a7%e9%80%92%e6%8e%a8%e7%9a%84%e7%bb%93%e5%90%88%e5%be%8b" class="nav-数学推导线性递推的结合律">
									数学推导：线性递推的结合律
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0%e5%8f%82%e8%80%83" class="nav-代码实现参考">
									代码实现参考
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#chunk-wise-parallel%e5%88%86%e5%9d%97%e5%b9%b6%e8%a1%8c" class="nav-chunk-wise-parallel分块并行">
									Chunk-wise Parallel：分块并行
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#triton-%e4%bc%aa%e4%bb%a3%e7%a0%81" class="nav-triton-伪代码">
									Triton 伪代码
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#fft%e5%92%8c%e5%8d%b7%e7%a7%af%e5%9e%8b%e7%ba%bf%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b" class="nav-fft和卷积型线性注意力">
									FFT和卷积型线性注意力
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e5%8d%b7%e7%a7%af%e5%8a%a0%e9%80%9f" class="nav-核心思想卷积加速">
									核心思想：卷积加速
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e4%bb%8e%e9%80%92%e6%8e%a8%e5%88%b0%e5%8d%b7%e7%a7%af" class="nav-从递推到卷积">
									从递推到卷积
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0" class="nav-代码实现">
									代码实现
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e8%bf%9b%e4%b8%80%e6%ad%a5%e7%9a%84%e8%ae%a1%e7%ae%97%e6%95%88%e7%8e%87%e4%b8%8e%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96" class="nav-进一步的计算效率与性能优化">
									进一步的计算效率与性能优化
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#delta-netwy%e8%a1%a8%e7%a4%ba%e4%b8%8eut%e5%8f%98%e6%8d%a2" class="nav-delta-netwy表示与ut变换">
									Delta Net：WY表示与UT变换
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#deltanet-%e5%8e%9f%e5%a7%8b%e5%bd%a2%e5%bc%8f%e7%9a%84%e9%97%ae%e9%a2%98" class="nav-deltanet-原始形式的问题">
									DeltaNet 原始形式的问题
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88wy-%e8%a1%a8%e7%a4%ba--ut-%e5%8f%98%e6%8d%a2" class="nav-解决方案wy-表示--ut-变换">
									解决方案：WY 表示 &#43; UT 变换
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95%e4%b8%8e%e5%88%86%e6%9e%90" class="nav-性能测试与分析">
									性能测试与分析
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#simple-gla%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95" class="nav-simple-gla性能测试">
									SIMPLE GLA性能测试
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%ae%97%e6%b3%95%e7%89%b9%e6%80%a7" class="nav-算法特性">
									算法特性
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95%e5%af%b9%e6%af%94" class="nav-性能测试对比">
									性能测试对比
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#delta-net%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95" class="nav-delta-net性能测试">
									Delta Net性能测试
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95%e5%af%b9%e6%af%94%e5%88%86%e6%9e%90" class="nav-性能测试对比分析">
									性能测试对比分析
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e6%80%bb%e7%bb%93" class="nav-总结">
									总结
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
            
        </div>
    </div>
</div>
<transition name="fade">
    <div id="drawer-mask" v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div>
</transition>
<nav id="navBar" class="navbar sticky-top navbar-light single-column-nav-container">
    <div id="navBackground" class="nav-background"></div>
    <div class="container container-narrow nav-content">
        <button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer">
            <i class="material-icons">
                menu
            </i>
        </button>
        <a id="navTitle" class="navbar-brand" href="https://sword865.github.io/">
            悟剑阁
        </a>
        
        <button type="button" class="nav-darkmode-toggle" id="darkModeToggleButton2">
            <i class="material-icons" id="darkModeToggleIcon2">
                dark_mode
            </i>
        </button>
        
    </div>
</nav>
<div class="single-column-header-container" id="pageHead"
     v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }">
    <a href="https://sword865.github.io/">
        <div class="single-column-header-title">悟剑阁</div>
        

    </a>
</div>

            <div id="content">
                <div id="streamContainer" class="stream-container">

    <div class="post-list-container post-list-container-shadow">
        <div class="post">
            
            
            

            <div class="post-head-wrapper-text-only"
                
            >
                <div class="post-title">
                    Linear Attention基础-工程篇
                    
                    <div class="post-meta">
                        
                        <time itemprop="datePublished">
                            2026-01-01 15:23
                        </time>
                        

                        

                        
                            <i class="material-icons" style="">label</i>
                            
                                <a href="/tags/attention">Attention</a>
                                &nbsp;
                            
                                <a href="/tags/llm">LLM</a>
                                &nbsp;
                            
                                <a href="/tags/linear-attention">Linear Attention</a>
                                &nbsp;
                            
                                <a href="/tags/triton">Triton</a>
                                &nbsp;
                            
                        
                        
                    </div>
                </div>
            </div>
            
            <div class="post-body-wrapper">
                
                <div class="post-body" v-pre>
                
                    <p>本文重点参考了文章<a href="https://srush.github.io/annotated-mamba/hard.html">Mamba: The Hard Way</a>和开源项目<a href="https://github.com/fla-org/flash-linear-attention">flash-linear-attention</a>。</p>
<h1 id="prefill与decoding">Prefill与Decoding</h1>
<p>我们都知道，在Attention的计算中，Prefill和Decoding是两个不同的场景，具体特性如下：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">特性</th>
          <th style="text-align: left">Prefill</th>
          <th style="text-align: left">Decoding</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">输入</td>
          <td style="text-align: left">长序列（长度 $L$）</td>
          <td style="text-align: left">1 个新 token + 历史状态</td>
      </tr>
      <tr>
          <td style="text-align: left">常见瓶颈</td>
          <td style="text-align: left">Compute bound（Tensor Core 利用）</td>
          <td style="text-align: left">Memory/Latency bound（状态读写 + 小矩阵计算）</td>
      </tr>
  </tbody>
</table>
<p>在回忆一下理论篇的介绍，特别是关于Mamba章节中的推导，常见的Linear Attention有两种表示格式：</p>
<p>矩阵格式（Attention视角）：</p>
<p>$$ y_i = \sum_{j=0}^i (CausalMask(Q_i K_j^T)) V_j $$</p>
<p>递推格式（SSM视角）：</p>
<p>$$ h_t = A_t h_{t-1} + B_t x_t $$
$$ y_t = C h_t $$</p>
<p>其中Decoding的算子可以比较直接的使用递归格式进行计算，因此我们本文重点还是看Prefill的实现。</p>
<h1 id="linear-attention常见算法">Linear Attention常见算法</h1>
<p>在Linear Attention的计算中，有一些常见的思路，本章结合<code>flash-linear-attention</code>的实现，对这些思路进行讲解。</p>
<h2 id="prefix-scan--cumsum-前缀和">Prefix Scan / Cumsum 前缀和</h2>
<h3 id="算法简介">算法简介</h3>
<p>先讲一下什么是Prefix Scan算法，简单来说Prefix是针对有结合性的算子提出的一种优化方式</p>
<p>假设有 $y_t = x_1 ⊗ x_2 ⊗ x_3 &hellip; ⊗ x_t$, 如果⊗支持结合律，那么y_t就可以用一个简单的Reduce进行求解</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>=== Parallel Prefix Scan (Up Sweep) ===
</span></span><span style="display:flex;"><span>序列: [x1][x2][x3][x4][x5][x6][x7][x8]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 1: 相邻元素两两合并 (4 次并行操作)
</span></span><span style="display:flex;"><span>        [x_1] [x1 ⊗ x2] [x_3] [x3 ⊗ x4] [x_5] [x5 ⊗ x6] [x_7] [x7 ⊗ x8]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 2: 继续合并 (2 次并行操作)
</span></span><span style="display:flex;"><span>        [x_1] [x_1 ⊗ x_2] [x_3] [x1 ⊗ x2 ⊗ x3 ⊗ x4] [x_5] [x5 ⊗ x6] [x_7] [x5 ⊗ x6 ⊗ x7 ⊗ x8]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 3: 最终合并 + Down-sweep (恢复所有前缀和)
</span></span><span style="display:flex;"><span>        [完整的前缀和序列(y_t)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>总计: O(log L) 轮，每轮 O(L) 次操作
</span></span></code></pre></div><p>我们把这个步骤称为Up Sweep，这个过程中我们不仅计算了最终的$y_t$，也产生了很多中间结果，我们可以把这个数组看成一个二叉树。
回忆一下，我们要算的可能不只有最终的 $y_L$，我们想计算所有的 $y_1$ 到 $y_L$。如果只用 Up Sweep，我们只能得到最后一个位置的完整前缀和。为了高效地得到所有位置的前缀和，我们需要引入 <strong>Down Sweep</strong> 阶段。</p>
<p>Down Sweep 的核心思想是：<strong>利用 Up Sweep 阶段在树节点中留下的中间结果，从根节点开始向下分发“左侧累加值”信息。</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>=== Parallel Prefix Scan (Down Sweep) ===
</span></span><span style="display:flex;"><span>Up Sweep 后的状态: [x1] [x1⊗x2] [x3] [x1⊗x2⊗x3⊗x4] [x5] [x5⊗x6] [x7] [x1⊗...⊗x8]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 1: 置零 (Set to Identity)
</span></span><span style="display:flex;"><span>        将最后一个元素（根节点）设为单位元（如 0）。
</span></span><span style="display:flex;"><span>        这是因为第一个元素的前缀和应该是 0。
</span></span><span style="display:flex;"><span>        [x1] [x1⊗x2] [x3] [x1⊗x2⊗x3⊗x4] [x5] [x5⊗x6] [x7] [0]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 2: 交换与分发 (Swap and Sum)
</span></span><span style="display:flex;"><span>        从上往下，每一层进行如下操作：
</span></span><span style="display:flex;"><span>        1. 暂存当前节点的左孩子值。
</span></span><span style="display:flex;"><span>        2. 将当前节点的值赋给左孩子。
</span></span><span style="display:flex;"><span>        3. 将“暂存的左孩子值 ⊗ 当前节点值”赋给右孩子。
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        (第一轮分发：将根节点的 0 传给左半区，将左半区的总和传给右半区)
</span></span><span style="display:flex;"><span>        [x1] [x1⊗x2] [x3] [0] [x5] [x5⊗x6] [x7] [x1⊗x2⊗x3⊗x4]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 3: 递归向下 (Recursive Down)
</span></span><span style="display:flex;"><span>        继续向下重复此过程，直到叶子节点。
</span></span><span style="display:flex;"><span>        最终数组会变成 Exclusive Scan 序列（即每个位置存储它之前所有元素的和）。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Step 4: 恢复 Inclusive Scan
</span></span><span style="display:flex;"><span>        将 Exclusive Scan 的结果与原始序列对应位置进行 ⊗ 操作，即可得到 y_1 到 y_L。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>总计: O(log L) 轮，每轮 O(L) 次操作，总复杂度依然是 O(L)。
</span></span></code></pre></div><p>这两个过程可以从两个个树型的计算来表示：</p>
<img width="800"  src="/images/2026/20260101/prefix-scan-tree.png" class="center" />
<p>顺带的，给出一个例子：
<img width="800"  src="/images/2026/20260101/prefix-scan-tree-example.png" class="center" /></p>
<h3 id="linear-attention的prefix-scan">Linear Attention的Prefix Scan</h3>
<p>了解了 Prefix Scan 算法，我们来看一下 Linear Attention 的递推公式：</p>
<p>$$ h_t = A_t h_{t-1} + B_t x_t $$
$$ y_t = C h_t $$</p>
<p>这个公式描述了一个一阶线性递推过程。在传统的 RNN 中，我们需要串行地计算 $h_1, h_2, \dots, h_L$，这在训练长序列时会成为严重的瓶颈。为了将其转化为 Prefix Scan，我们需要找到一个<strong>结合律算子</strong> $\otimes$。</p>
<h4 id="数学推导线性递推的结合律">数学推导：线性递推的结合律</h4>
<p>为什么线性递推可以转化为 Prefix Scan？关键在于将状态转移看作是<strong>线性函数的复合</strong>。</p>
<p>对于任意时刻 $t$，状态更新公式为：
$$ h_t = A_t h_{t-1} + b_t \quad (\text{其中 } b_t = B_t x_t) $$</p>
<p>我们可以将这个过程看作是一个线性变换 $f_t(h) = A_t h + b_t$。那么 $h_t$ 实际上是这些变换的连续嵌套：
$$ h_t = f_t(f_{t-1}(\dots f_1(h_0) \dots)) $$</p>
<p>如果我们定义一个算子 $\otimes$ 来表示两个线性变换的复合 $f_j \circ f_i$（假设 $j &gt; i$，即 $j$ 是较晚的时刻）：
$$ (f_j \circ f_i)(h) = A_j (A_i h + b_i) + b_j = (A_j A_i) h + (A_j b_i + b_j) $$</p>
<p>因此，我们可以定义元组 $(A, b)$ 上的算子 $\otimes$：
$$ (A_j, b_j) \otimes (A_i, b_i) = (A_j A_i, A_j b_i + b_j) $$</p>
<p><strong>这个算子的物理意义是</strong>：它计算了从状态 $h_{i-1}$ 到 $h_j$ 的“总转移矩阵”和“总偏置项”。
当我们对序列 $[(A_1, b_1), (A_2, b_2), \dots, (A_L, b_L)]$ 执行 Prefix Scan 时，第 $t$ 个位置的结果 $(A_{1:t}, b_{1:t})$ 就代表了从初始状态 $h_0$ 到 $h_t$ 的完整变换参数：
$$ h_t = A_{1:t} h_0 + b_{1:t} $$</p>
<p><strong>证明结合律</strong>：
假设有三个连续的状态转移 $(A_3, b_3), (A_2, b_2), (A_1, b_1)$：</p>
<ol>
<li><strong>左结合</strong>：$((A_3, b_3) \otimes (A_2, b_2)) \otimes (A_1, b_1) = (A_3 A_2, A_3 b_2 + b_3) \otimes (A_1, b_1) = (A_3 A_2 A_1, (A_3 A_2) b_1 + (A_3 b_2 + b_3))$</li>
<li><strong>右结合</strong>：$(A_3, b_3) \otimes ((A_2, b_2) \otimes (A_1, b_1)) = (A_3, b_3) \otimes (A_2 A_1, A_2 b_1 + b_2) = (A_3 A_2 A_1, A_3 (A_2 b_1 + b_2) + b_3)$</li>
</ol>
<p>展开后可以发现两者完全一致。这意味着我们可以使用 Parallel Prefix Scan 在 $O(\log L)$ 的时间内并行计算出所有的 $h_t$。</p>
<h4 id="代码实现参考">代码实现参考</h4>
<p>在 <code>flash-linear-attention</code> 或 <code>Mamba</code> 的实现中，通常会利用 <code>jax.lax.associative_scan</code> 或在 Triton 中手动实现类似的逻辑。以下是一个简化的逻辑示例：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">associative_scan_op</span>(q_earlier, q_later):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    q = (A, b)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    计算两个线性变换的复合: f_later(f_earlier(h))
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    a_i, b_i <span style="color:#f92672">=</span> q_earlier
</span></span><span style="display:flex;"><span>    a_j, b_j <span style="color:#f92672">=</span> q_later
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 新的 A 是两个矩阵的乘积 (注意顺序: 晚的在左)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 新的 b 是 晚的A * 早的b + 晚的b</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a_j <span style="color:#f92672">*</span> a_i, a_j <span style="color:#f92672">*</span> b_i <span style="color:#f92672">+</span> b_j
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. 准备输入元组序列</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># A_bars: [L, d_state]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># b_bars: [L, d_state], 即 B_t * x_t</span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> (A_bars, b_bars)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. 执行并行前缀扫描</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 返回每个位置 t 从 h_0 到 h_t 的总变换参数 (A_1:t, b_1:t)</span>
</span></span><span style="display:flex;"><span>combined_params <span style="color:#f92672">=</span> associative_scan(associative_scan_op, inputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. 计算隐藏状态 h_t (假设 h_0 = 0)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># h_t = A_1:t * h_0 + b_1:t = b_1:t</span>
</span></span><span style="display:flex;"><span>h_states <span style="color:#f92672">=</span> combined_params[<span style="color:#ae81ff">1</span>] 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. 计算输出 y = C * h</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> C <span style="color:#f92672">*</span> h_states
</span></span></code></pre></div><p>这种表示法的精妙之处在于，它将原本必须串行执行的 <code>for</code> 循环（$h_t$ 依赖 $h_{t-1}$）转化为了树状的并行规约过程。在 GPU 上，这意味着我们可以利用数千个核心同时处理序列的不同部分，极大地提升了 Prefill 阶段的吞吐量。</p>
<h2 id="chunk-wise-parallel分块并行">Chunk-wise Parallel：分块并行</h2>
<p>Chunk wise Parallel 是 <code>flash-linear-attention</code> 中最核心的 Prefill实现方式包括<code>Mamba the hard way</code>也是在讲解这一思想，虽然这一思想在具体实现时根据Kernel的划分可以有多种不同的实现方式，但是其核心思想始终都是 <strong>Two-pass</strong>：</p>
<p>具体来说，我们把整个要计算的序列（长度L）拆分成多个不同的Chunk（长度C），然后按照进行计算：</p>
<ol>
<li>Pass 1：计算每个 Chunk 的最后一个token的隐藏状态（顺序计算，一共$L/C$ 步，产生 $L/C$ 个状态）</li>
<li>Pass 2：在Chunk 内做局部计算 + 叠加来自前序 Chunk 的状态贡献（并行计算、MatMul 密集）</li>
</ol>
<p>该计算过程可以用下图表示：
<img width="800"  src="/images/2026/20260101/chunk-wise.png" class="center" /></p>
<ol>
<li>第一遍Pass顺序对每个Chunk进行计算，得到所有深绿色方块。</li>
<li>第二遍Pass同时在Chunk内先基于深绿色方块来计算所有浅绿色方块，然后计算最终要输出的蓝色方块。</li>
</ol>
<h3 id="triton-伪代码">Triton 伪代码</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Inter-chunk: 状态传递 (Pass 1)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 类似于 RNN，但只在 Chunk 边界进行</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_chunks):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 加载上一个 Chunk 的状态 (i=0 时为初始状态 S_0)</span>
</span></span><span style="display:flex;"><span>    b_s <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(s_ptr <span style="color:#f92672">+</span> (i <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> stride) <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> tl<span style="color:#f92672">.</span>zeros(<span style="color:#f92672">...</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 更新状态：S = S * Decay + K^T * V (当前 Chunk 的贡献)</span>
</span></span><span style="display:flex;"><span>    b_s <span style="color:#f92672">=</span> b_s <span style="color:#f92672">*</span> chunk_decay <span style="color:#f92672">+</span> tl<span style="color:#f92672">.</span>dot(tl<span style="color:#f92672">.</span>trans(b_k_block), b_v_block)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 保存状态供下一个 Chunk 使用</span>
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(s_ptr <span style="color:#f92672">+</span> i <span style="color:#f92672">*</span> stride, b_s)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Intra-chunk: 块内并行计算 (Pass 2)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 类似于标准 Attention，但只看 Chunk 内部</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 加载当前 Chunk 的 Q, K, V</span>
</span></span><span style="display:flex;"><span>b_q <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(q_ptr)  <span style="color:#75715e"># [BLOCK_SIZE, D_K]</span>
</span></span><span style="display:flex;"><span>b_k <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(k_ptr)  <span style="color:#75715e"># [BLOCK_SIZE, D_K]</span>
</span></span><span style="display:flex;"><span>b_v <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(v_ptr)  <span style="color:#75715e"># [BLOCK_SIZE, D_V]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算局部 Attention Score: Q @ K^T</span>
</span></span><span style="display:flex;"><span>b_s <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>dot(b_q, tl<span style="color:#f92672">.</span>trans(b_k)) <span style="color:#f92672">*</span> decay_mask  <span style="color:#75715e"># [BLOCK_SIZE, BLOCK_SIZE]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算输出</span>
</span></span><span style="display:flex;"><span>b_o <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>dot(b_s, b_v)  <span style="color:#75715e"># [BLOCK_SIZE, D_V]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 加上来自前一个 Chunk 状态的贡献</span>
</span></span><span style="display:flex;"><span>b_o <span style="color:#f92672">+=</span> tl<span style="color:#f92672">.</span>dot(b_q, prev_s) <span style="color:#f92672">*</span> decay
</span></span></code></pre></div><h2 id="fft和卷积型线性注意力">FFT和卷积型线性注意力</h2>
<h3 id="核心思想卷积加速">核心思想：卷积加速</h3>
<p>对于某些Linear Attention变体（如Hyena），其特殊之处在于：<strong>衰减权重只依赖于相对位置</strong>，而不是绝对位置。这意味着我们可以把计算转化为卷积操作，从而利用FFT加速。</p>
<h4 id="从递推到卷积">从递推到卷积</h4>
<p>考虑一个简化的输出公式：
$$o_t = \sum_{j=1}^t u_j \cdot \alpha_{t-j}$$</p>
<p>其中：</p>
<ul>
<li>$u_j$ 是时刻 $j$ 的输入表示</li>
<li>$\alpha_{t-j}$ 是<strong>只依赖于距离 $(t-j)$ 的衰减系数</strong></li>
</ul>
<p>可以看到这里$\alpha$ 的下标是 $t-j$（相对距离），和<strong>卷积</strong>的定义是可以对上的。</p>
<p>直接计算卷积的复杂度是 $O(L^2)$（对每个位置 $t$，都要累加前面所有的 $j$）。但根据<strong>卷积定理</strong>：</p>
<p>$$\text{时域卷积} = \text{频域点乘}$$
$$f * g = \mathcal{F}^{-1}(\mathcal{F}(f) \cdot \mathcal{F}(g))$$</p>
<p>FFT的复杂度是 $O(L \log L)$，频域点乘是 $O(L)$，总复杂度 $O(L \log L)$ 远小于 $O(L^2)$。</p>
<p>举一个具体的例子：</p>
<p>假设序列长度 $L=4$，输入 $u = [1, 2, 3, 4]$，衰减核 $\alpha = [1.0, 0.5, 0.25, 0.125]$（距离越远衰减越强）。</p>
<p><strong>直接计算</strong>（$O(L^2)$）：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>o_1 = u_1 * α_0 = 1 * 1.0 = 1.0
</span></span><span style="display:flex;"><span>o_2 = u_2 * α_0 + u_1 * α_1 = 2*1.0 + 1*0.5 = 2.5
</span></span><span style="display:flex;"><span>o_3 = u_3 * α_0 + u_2 * α_1 + u_1 * α_2 = 3*1.0 + 2*0.5 + 1*0.25 = 4.25
</span></span><span style="display:flex;"><span>o_4 = u_4 * α_0 + u_3 * α_1 + u_2 * α_2 + u_1 * α_3 = 4*1.0 + 3*0.5 + 2*0.25 + 1*0.125 = 6.125
</span></span></code></pre></div><p><strong>FFT加速</strong>（$O(L \log L)$）：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>1. FFT(u) 和 FFT(α)  → 转到频域（O(L*logL)）
</span></span><span style="display:flex;"><span>2. 频域点乘           → 逐元素相乘（O(L)）
</span></span><span style="display:flex;"><span>3. IFFT               → 转回时域（O(L*logL)）
</span></span></code></pre></div><h4 id="代码实现">代码实现</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fft_conv</span>(u, k, seqlen):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    使用 FFT 加速因果卷积计算
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    参数:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        u: 输入序列 [batch, seqlen, dim]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        k: 衰减核 [seqlen] (只依赖相对位置)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        seqlen: 序列长度
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    返回:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        输出序列 [batch, seqlen, dim]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 1. 补零到 2*L (避免循环卷积的边界问题)</span>
</span></span><span style="display:flex;"><span>    fft_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> seqlen
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. FFT 变换到频域 (O(L log L))</span>
</span></span><span style="display:flex;"><span>    k_f <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>fft<span style="color:#f92672">.</span>rfft(k, n<span style="color:#f92672">=</span>fft_size)  <span style="color:#75715e"># 衰减核</span>
</span></span><span style="display:flex;"><span>    u_f <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>fft<span style="color:#f92672">.</span>rfft(u, n<span style="color:#f92672">=</span>fft_size)  <span style="color:#75715e"># 输入信号</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 3. 频域点乘 (O(L)) -&gt; 这一步替代了原本 O(L^2) 的时域卷积。</span>
</span></span><span style="display:flex;"><span>    y_f <span style="color:#f92672">=</span> u_f <span style="color:#f92672">*</span> k_f
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 4. IFFT 逆变换回时域 (O(L log L))</span>
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>fft<span style="color:#f92672">.</span>irfft(y_f, n<span style="color:#f92672">=</span>fft_size)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5. 截取有效部分（去掉补零部分）</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> y[<span style="color:#f92672">...</span>, :seqlen]
</span></span></code></pre></div><p>虽然看上去FFT是一个很好的算法，但是因为大部分的Linear Attention变体的衰减核不只和位置有关(会受到输入x影响)，因此，FFT加速现在并不常见，只在特定架构中有效。</p>
<h1 id="进一步的计算效率与性能优化">进一步的计算效率与性能优化</h1>
<h3 id="delta-netwy表示与ut变换">Delta Net：WY表示与UT变换</h3>
<p>我们在前文中讲解了基础的Chunk Wise算法框架，但是在实践中这个框架并不能直接out-of-box的拿出来使用，这里以Delta Net为例讲解一下WY表示和UT变换（KDA中也使用了类似的方法）</p>
<p>先回顾一下 Chunk-wise 算法的核心思想（Two-pass）：</p>
<p><strong>Pass 1 (Inter-chunk)</strong>：顺序计算每个 Chunk 的边界状态</p>
<ul>
<li>对于传统 Linear Attention：$S[i+1] = S[i] + V[i]^T K[i]$（矩阵乘法，高效）</li>
</ul>
<p><strong>Pass 2 (Intra-chunk)</strong>：并行计算 Chunk 内部的输出</p>
<ul>
<li>需要把 Chunk 内的局部累积效果表示成矩阵乘法形式</li>
<li>例如：$O[i] = Q[i] S[i]^T + (\text{Chunk内部的局部贡献})$</li>
</ul>
<p>为了提高计算效率，这两个 Pass 都会希望计算必须能表示成<strong>高效的矩阵乘法</strong>（可以利用 GPU Tensor Core），而不是串行循环或者稠密的 $d \times d$ 矩阵加减法操作。</p>
<h4 id="deltanet-原始形式的问题">DeltaNet 原始形式的问题</h4>
<p>在传统的 Linear Attention 中，状态更新是简单的加法：$S_t = S_{t-1} + v_t k_t^T$。</p>
<ul>
<li>Chunk 内累积：$S[i+1] = S[i] + \sum_{t=1}^{C} v_t k_t^T = S[i] + V^T K$</li>
</ul>
<p>但是回忆一下理论篇的推导，在 <strong>DeltaNet</strong> 中， <strong>Delta Rule</strong>的更新规则会更加复杂：
$$ S_t = S_{t-1}(I - \beta_t k_t k_t^{\top}) + \beta_t v_t k_t^{\top} $$</p>
<p>现在让我们尝试在 Chunk 内做累积，就会发现很多计算效率上的问题：</p>
<p>经过论文<a href="https://arxiv.org/pdf/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a>中Section 2的公式推导，可以发现在一个 Chunk 内，从 $S[i]$ 到 $S[i+1]$ 的更新公式为：</p>
<p>$$ S[i+1] = S[i] \prod_{t=1}^C \bigl(I - \beta_t k_t k_t^{\top}\bigr) + \sum_{t=1}^C \left( \beta_t v_t k_t^{\top} \prod_{j=t+1}^C \bigl(I - \beta_j k_j k_j^{\top}\bigr) \right). $$</p>
<p>其中符号含义：</p>
<ul>
<li>$C$：Chunk size（每个 Chunk 的长度）</li>
<li>$S[i]$：第 $i$ 个 Chunk 的输入状态</li>
<li>$S[i+1]$：第 $i$ 个 Chunk 的输出状态（传递给下一个 Chunk）</li>
<li>$t$：当前 Chunk 内的位置索引（$t = 1, 2, \dots, C$）</li>
<li>$k_t, v_t$：第 $i$ 个 Chunk 内第 $t$ 个位置的 key 和 value</li>
<li>$\beta_t$：第 $i$ 个 Chunk 内第 $t$ 个位置的学习率/遗忘门</li>
</ul>
<p>还有公式种两个项的含义：</p>
<ul>
<li><strong>第一项</strong>：前一个 Chunk 的状态 $S[i]$ 经过当前 Chunk 内所有 Delta 更新的累积衰减</li>
<li><strong>第二项</strong>：当前 Chunk 内每个位置 $t$ 的贡献 $\beta_t v_t k_t^T$，并经过后续位置 $j &gt; t$ 的衰减</li>
</ul>
<p>基于这个公式，如果我们直接计算 $P[i] = \prod_{t=1}^C (I - \beta_t k_t k_t^T)$：</p>
<ul>
<li>展开两个矩阵的乘积：$(I - \beta_2 k_2 k_2^T)(I - \beta_1 k_1 k_1^T) = I - \beta_2 k_2 k_2^T - \beta_1 k_1 k_1^T + \beta_1\beta_2 k_2 k_2^T k_1 k_1^T$</li>
<li>继续乘下去，交叉项会越来越多，经过 $C$ 步，结果变成一个<strong>秩为 $O(C)$ 的稠密矩阵</strong></li>
</ul>
<p>对于这种方式计算 $P[i]$ 是非常低效的&ndash;&gt;因为我们<strong>无法高效计算和存储</strong>这个计算过程：需要 $O(d^2)$ 的空间，计算复杂度是 $O(Cd^3)$</p>
<h4 id="解决方案wy-表示--ut-变换">解决方案：WY 表示 + UT 变换</h4>
<p><strong>WY 表示：让 Chunk 累积可以用矩阵乘法表示</strong></p>
<p>可以看到，高秩矩阵值的状态对于chunk-wise的计算和存储都是一个很大的挑战，因此我们需要引入一些变换来解决这个问题。</p>
<p>针对DeltaNet，一个关键的发现是：虽然直接计算 $\prod (I - \beta_t k_t k_t^T)$ 会秩爆炸，但利用 Householder 变换的数学性质，这个连乘<strong>依然可以紧凑地表示</strong>为：
$$ \prod_{i=1}^C \bigl(I - \beta_i k_i k_i^{\top}\bigr) = I - W_C K_C^{\top}, $$
其中 $W_C \in \mathbb{R}^{C \times d}$，$K_C \in \mathbb{R}^{C \times d}$。</p>
<p>这样，Inter-chunk 更新就变成了：
$$ S[i+1] = S[i] \bigl(I - W[i] K[i]^{\top}\bigr) + U[i]^{\top} K[i] = S[i] - (S[i]W[i])K[i]^{\top} + U[i]^{\top} K[i] $$</p>
<p>这是一个标准的<strong>矩阵乘法</strong>，我们可以更好地利用Tensor Core, 计算复杂度 $O(Cd^3)$ 降到了 $O(Cd^2)$，同时状态空间也从 $O(d^2)$ 变成了 $O(dC)$。大大降低了计算和存储IO的负担。</p>
<p><strong>UT 变换：让 W 的计算也能并行化</strong></p>
<p>但是，$W$ 矩阵中的每一行 $w_t$ 的计算仍然还是递归的：
$$ w_t = \beta_t \Bigl(k_t - \sum_{i=1}^{t-1} w_i (k_i^{\top} k_t)\Bigr) $$</p>
<p>这又是<strong>串行计算</strong>，无法利用 Tensor Core。</p>
<p>UT 变换通过定义下三角矩阵 $A$（包含 $k_i^T k_j$ 的信息），将递归转化为：
$$ W = T \mathrm{diag}(\beta) K, \qquad \text{其中 } T = (I - A)^{-1} $$</p>
<p>由于 $A$ 是严格下三角，$T$ 可以通过前向替换高效求解，或者在 Chunk 较小时直接用矩阵乘法，于是：</p>
<ul>
<li><strong>计算变成了矩阵运算</strong>，可以调用GPU Tensor Core加速计算</li>
<li><strong>在 Chunk 内并行处理</strong>，不在需要一步步串行计算</li>
</ul>
<p><strong>WY 表示 + UT 变换</strong>将 DeltaNet 的 Chunk 内计算转化为了 GPU 友好的矩阵乘法，这让DeltaNet的 Chunk-wise 并行算法真正可行。</p>
<h1 id="性能测试与分析">性能测试与分析</h1>
<p>最后，我在半张H200(H200 MIG3-70G)的GPU上对上面的算法做了一些性能测试：</p>
<h2 id="simple-gla性能测试">SIMPLE GLA性能测试</h2>
<h3 id="算法特性">算法特性</h3>
<p>Simple GLA 相比完整的 GLA，采用了 <strong>head-wise gating</strong> 而非 elementwise gating。这一简化减少了参数量，也降低了数值不稳定的概率。</p>
<p>其状态更新公式为：
$$S_{t+1} = g_{t+1} \odot S_{t} + K_{t+1} V_{t+1}^{\top}$$
其中 $g$ 是一个标量，这使得衰减操作可以高效地融合到矩阵计算中。</p>
<h3 id="性能测试对比">性能测试对比</h3>
<p>测试配置：Batch=4, Heads=8, d_head=128, 对不同的序列长度L进行测试：</p>
<p>测试一共使用了3种算法：</p>
<ol>
<li>Chunk-wise算法： 使用2个Kernel的Chunk-wise算法</li>
<li>Fused Chunk-wise算法： 使用1个Kernel的Chunk-wise算法</li>
<li>Parallel Scan：基于Sweep Up/Down的算法</li>
</ol>
<table>
  <thead>
      <tr>
          <th>L</th>
          <th>Chunk-wise</th>
          <th>Fused Chunk-wise</th>
          <th>Parallel Scan</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>32</td>
          <td>0.099184</td>
          <td>0.045680</td>
          <td>0.013440</td>
      </tr>
      <tr>
          <td>64</td>
          <td>0.093040</td>
          <td>0.044832</td>
          <td>0.018016</td>
      </tr>
      <tr>
          <td>128</td>
          <td>0.113344</td>
          <td>0.070464</td>
          <td>0.020704</td>
      </tr>
      <tr>
          <td>256</td>
          <td>0.111024</td>
          <td>0.112288</td>
          <td>0.034528</td>
      </tr>
      <tr>
          <td>512</td>
          <td>0.172096</td>
          <td>0.281088</td>
          <td>0.064032</td>
      </tr>
      <tr>
          <td>1024</td>
          <td>0.498624</td>
          <td>0.546480</td>
          <td>0.150272</td>
      </tr>
      <tr>
          <td>2048</td>
          <td>0.985968</td>
          <td>1.078208</td>
          <td>0.422528</td>
      </tr>
      <tr>
          <td>4096</td>
          <td>1.955248</td>
          <td>2.132128</td>
          <td>1.350048</td>
      </tr>
      <tr>
          <td>8192</td>
          <td>3.882000</td>
          <td>4.229376</td>
          <td>4.766928</td>
      </tr>
      <tr>
          <td>16384</td>
          <td>7.750400</td>
          <td>8.427360</td>
          <td>17.839392</td>
      </tr>
  </tbody>
</table>
<p>可以看到，在序列较短时（例如 $L\le 4096$），Parallel Scan 的优势更明显；这主要来自两点：</p>
<ol>
<li>并行 scan 的关键路径是 $O(\log L)$ 轮的 up-sweep/down-sweep，短序列时轮数少；</li>
<li>这类实现往往可以把“状态合并”写成很轻量的向量/小矩阵算子，kernel 本身更偏 latency-bound，短 $L$ 时更容易占优。</li>
</ol>
<p>但是随着序列变长（从表中可见在 $L\approx 8192$ 附近出现拐点，$L=16384$ 时差距被明显拉开），Parallel Scan 会逐渐变慢，原因通常是：</p>
<ul>
<li>需要的 sweep 轮数随 $\log L$ 增长，并且每一轮都伴随全局同步/跨 block 的数据交换，整体更偏 memory-bound；</li>
<li>scan 的算术强度相对较低，序列越长越容易被 HBM 带宽与同步开销限制。</li>
</ul>
<p>相比之下，Chunk-wise 的 Two-pass 结构在长序列时更“吃得满” Tensor Core：Pass 1 只在 chunk 边界更新状态、Pass 2 在 chunk 内做更大粒度的矩阵运算（更高的 arithmetic intensity），因此随 $L$ 增长时吞吐更稳定。</p>
<p>另外，Fused Chunk-wise 虽然只需要启动一次 Kernel，但它往往需要在一个 kernel 内同时承担 Pass 1 + Pass 2 的职责：为了避免中间状态落到显存，必须把更多的中间量/状态（例如每个 head 的累计状态 $S$ 或 chunk 边界状态）尽可能保存在寄存器中。
这会带来寄存器压力上升与 occupancy 下降，极端情况下还会触发 register spill（溢出到 local memory），从而在长序列时反而不如两 Kernel 的 Chunk-wise 实现。</p>
<h2 id="delta-net性能测试">Delta Net性能测试</h2>
<p>这节我们以 Delta Net 为例，先给出核心的性能对比结果，然后解释为什么某些并行化策略（特别是基于Parallel scan 的方法）在实践中会失败。</p>
<p>对于不同的序列长度L，运行的结果表现如下：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: right">L</th>
          <th style="text-align: right">Chunk-wise Parallel</th>
          <th style="text-align: right">Parallel Scan</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: right">128</td>
          <td style="text-align: right">0.178384</td>
          <td style="text-align: right">0.159504</td>
      </tr>
      <tr>
          <td style="text-align: right">256</td>
          <td style="text-align: right">0.172256</td>
          <td style="text-align: right">0.159840</td>
      </tr>
      <tr>
          <td style="text-align: right">512</td>
          <td style="text-align: right">0.185056</td>
          <td style="text-align: right">0.210512</td>
      </tr>
      <tr>
          <td style="text-align: right">1024</td>
          <td style="text-align: right">0.198560</td>
          <td style="text-align: right">0.478272</td>
      </tr>
      <tr>
          <td style="text-align: right">2048</td>
          <td style="text-align: right">0.350336</td>
          <td style="text-align: right">1.239392</td>
      </tr>
      <tr>
          <td style="text-align: right">4096</td>
          <td style="text-align: right">0.674592</td>
          <td style="text-align: right">3.765408</td>
      </tr>
      <tr>
          <td style="text-align: right">8192</td>
          <td style="text-align: right">1.318208</td>
          <td style="text-align: right">12.916416</td>
      </tr>
      <tr>
          <td style="text-align: right">16384</td>
          <td style="text-align: right">2.600304</td>
          <td style="text-align: right">47.368912</td>
      </tr>
  </tbody>
</table>
<h3 id="性能测试对比分析">性能测试对比分析</h3>
<p>可以看到在Delta Net种，<code>Parallel Scan</code>算法的优势明显变小，在L=512时候就被Chunk-wise的算法反超了。</p>
<p>为了看清 Parallel Scan 需要合并的是什么，我们把 DeltaNet 的单步更新再写一遍：</p>
<p>$$ S_t = S_{t-1}(I - \beta_t k_t k_t^{\top}) + \beta_t v_t k_t^{\top}. $$</p>
<p>然后，记</p>
<p>$$M_t = I - \beta_t k_t k_t^{\top} \in\mathbb{R}^{d\times d}$$</p>
<p>$$B_t = \beta_t v_t k_t^{\top} \in\mathbb{R}^{d\times d}$$</p>
<p>于是每一步都是对矩阵S的仿射变换：</p>
<p>$$S_t = S_{t-1} M_t + B_t$$</p>
<p>要把这个过程用Parallel Scan加速，我们可以继续使用前文种定义的算子 $\otimes$ ：</p>
<p>$$(A_j,b_j)\otimes (A_i,b_i) = (A_j A_i,; A_j b_i + b_j),$$</p>
<p>它表示先做时刻 $i$ 的变换再做时刻 $j$ 的变换。将该算子应用到序列 $(M_t,B_t)$ 后：</p>
<p>$$S_1 = S_0 M_1 + B_1$$
$$S_2 = S_1 M_2 + B_2 = S_0 (M_1 M_2) + (B_1 M_2 + B_2)$$</p>
<p>可以看到，在合并一段区间时需要把对应的 $M$ 矩阵相乘（产生 $M_1M_2$），同时把先前的 $B$ 按新的 $M$ 变换后累加（产生 $B_1M_2+B_2$），这是两个比较重的矩阵操作，也是Delta Net的Parallel Scan算法效率不高的根本原因：</p>
<ol>
<li>
<p><strong>合并里必须显式算/存 $M$</strong>：
对普通线性注意力（加法累积）而言，scan 合并的是固定形状的小矩阵/向量，算子很轻。
但 DeltaNet 合并的是矩阵 $M_t$ 是 $d\times d$，而且要在合并时做 $M_1M_2$等运算，复杂度要高得多。</p>
</li>
<li>
<p><strong>难以通过低秩假设优化计算</strong>：
单步 $M_t = I - \beta_t k_t k_t^T$ 是 rank-1 更新（把一个已有矩阵加上或减去一个rank为1的矩阵）；但两步相乘：
$$ (I - \beta_2 k_2 k_2^T)(I - \beta_1 k_1 k_1^T)
= I - \beta_1 k_1 k_1^T - \beta_2 k_2 k_2^T + \beta_1\beta_2 k_2 (k_2^T k_1) k_1^T $$
最后的交叉项会不断出现。把一段长度为 $m$ 的连乘写成“单位阵减低秩”的形式时，其有效秩一般会随 $m$ 增长（直观上接近 $O(m)$）。
这意味着：如果 scan 想只携带低秩因子（比如 $I - W K^T$），那么在 up-sweep 的更高层节点里，$W,K$ 的行数会越来越大——<strong>中间态不再是常数大小</strong>，从而无法像普通 scan 那样用固定 shape 的张量高效实现。</p>
</li>
</ol>
<p>Parallel Scan 虽然只有 $O(\log L)$ 轮，但<strong>每一轮</strong>都要对整段序列做一次“合并 + 全局读写/同步”，当合并算子本身已经是大矩阵操作时，就会表现出你表里那种随长度急剧变慢的趋势。</p>
<h1 id="总结">总结</h1>
<p>可以看到，在Linear Attention中，虽然有着多重不同的工程实现方式，但是具体到特定的算子，还是需要根据算子的特性和计算的参数(如序列长度)来选择最合适的算法。</p>

                    
                    <HR width="100%" id="EOF">
		    <p style="color:#777;">Last modified on 2026-01-01</p>
                    
                </div>
            </div>
            
            
            <nav class="post-pagination">

                
                <a class="newer-posts" href="/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/">
			Next<br>模型分片，KV Cache和推理加速的一些思考：计算与数据
                </a>
                
                
                
                <a class="older-posts" href="/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/">
			Previous<br>Linear Attention基础-理论篇
                </a>
                
            </nav>
            <div class="post-comment-wrapper">
                












            </div>
        </div>
    </div>


                    </div>
            </div><div id="sideContainer" class="side-container">
    
    <a class="a-block nav-head false" href="https://sword865.github.io/">
    
        <div class="nav-title">
            悟剑阁
        </div>
        
    </a>

    <div class="nav-link-list">
        
        
            
            
            
                
            
            
            
            <a class="a-block nav-link-item active" href="/posts">
                Archive
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/tags">
                Tags
            </a>
            
        
            
            
            
            
            
            <a class="a-block nav-link-item false" href="/about/">
                关于
            </a>
            
        
    </div>

    

    <div class="nav-footer">
        
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	Copyright (c) 2015. All rights reserved.
	

    </div>
    
</div><div id="extraContainer" class="extra-container">
    <div class="toc-wrapper">
        

        
        <div class="toc">


	<div class="toc-content">
	
		
		
		
		<center>- CATALOG -</center>
		
		
		<ul>
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#prefill%e4%b8%8edecoding" class="nav-prefill与decoding">
									Prefill与Decoding
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#linear-attention%e5%b8%b8%e8%a7%81%e7%ae%97%e6%b3%95" class="nav-linear-attention常见算法">
									Linear Attention常见算法
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#prefix-scan--cumsum-%e5%89%8d%e7%bc%80%e5%92%8c" class="nav-prefix-scan--cumsum-前缀和">
									Prefix Scan / Cumsum 前缀和
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%ae%97%e6%b3%95%e7%ae%80%e4%bb%8b" class="nav-算法简介">
									算法简介
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#linear-attention%e7%9a%84prefix-scan" class="nav-linear-attention的prefix-scan">
									Linear Attention的Prefix Scan
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc%e7%ba%bf%e6%80%a7%e9%80%92%e6%8e%a8%e7%9a%84%e7%bb%93%e5%90%88%e5%be%8b" class="nav-数学推导线性递推的结合律">
									数学推导：线性递推的结合律
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0%e5%8f%82%e8%80%83" class="nav-代码实现参考">
									代码实现参考
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#chunk-wise-parallel%e5%88%86%e5%9d%97%e5%b9%b6%e8%a1%8c" class="nav-chunk-wise-parallel分块并行">
									Chunk-wise Parallel：分块并行
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#triton-%e4%bc%aa%e4%bb%a3%e7%a0%81" class="nav-triton-伪代码">
									Triton 伪代码
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#fft%e5%92%8c%e5%8d%b7%e7%a7%af%e5%9e%8b%e7%ba%bf%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b" class="nav-fft和卷积型线性注意力">
									FFT和卷积型线性注意力
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e5%8d%b7%e7%a7%af%e5%8a%a0%e9%80%9f" class="nav-核心思想卷积加速">
									核心思想：卷积加速
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e4%bb%8e%e9%80%92%e6%8e%a8%e5%88%b0%e5%8d%b7%e7%a7%af" class="nav-从递推到卷积">
									从递推到卷积
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0" class="nav-代码实现">
									代码实现
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e8%bf%9b%e4%b8%80%e6%ad%a5%e7%9a%84%e8%ae%a1%e7%ae%97%e6%95%88%e7%8e%87%e4%b8%8e%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96" class="nav-进一步的计算效率与性能优化">
									进一步的计算效率与性能优化
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#delta-netwy%e8%a1%a8%e7%a4%ba%e4%b8%8eut%e5%8f%98%e6%8d%a2" class="nav-delta-netwy表示与ut变换">
									Delta Net：WY表示与UT变换
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#deltanet-%e5%8e%9f%e5%a7%8b%e5%bd%a2%e5%bc%8f%e7%9a%84%e9%97%ae%e9%a2%98" class="nav-deltanet-原始形式的问题">
									DeltaNet 原始形式的问题
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88wy-%e8%a1%a8%e7%a4%ba--ut-%e5%8f%98%e6%8d%a2" class="nav-解决方案wy-表示--ut-变换">
									解决方案：WY 表示 &#43; UT 变换
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95%e4%b8%8e%e5%88%86%e6%9e%90" class="nav-性能测试与分析">
									性能测试与分析
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#simple-gla%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95" class="nav-simple-gla性能测试">
									SIMPLE GLA性能测试
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e7%ae%97%e6%b3%95%e7%89%b9%e6%80%a7" class="nav-算法特性">
									算法特性
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
						
							<li>
								<a href="#%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95%e5%af%b9%e6%af%94" class="nav-性能测试对比">
									性能测试对比
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
						
						
						
							<li>
								<a href="#delta-net%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95" class="nav-delta-net性能测试">
									Delta Net性能测试
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
						
							
								
									<ul>
								
							
						
						
							<li>
								<a href="#%e6%80%a7%e8%83%bd%e6%b5%8b%e8%af%95%e5%af%b9%e6%af%94%e5%88%86%e6%9e%90" class="nav-性能测试对比分析">
									性能测试对比分析
								</a>
							</li>
						
						
					
				
			
				
				
					
						
						
						
							
								</ul>
							
								</ul>
							
						
						
						
							<li>
								<a href="#%e6%80%bb%e7%bb%93" class="nav-总结">
									总结
								</a>
							</li>
						
						
					
				
			
		</ul>
	</div>

</div>
        
    </div>
    <div class="pagination">
        <a id="globalBackToTop" class="pagination-action animated-visibility" href="#top"
            :class="{ invisible: scrollY == 0 }">
            <i class="material-icons pagination-action-icon">
                keyboard_arrow_up
            </i>
        </a>
        
        <a type="button" class="pagination-action" id="darkModeToggleButton">
            <span class="material-icons pagination-action-icon" id="darkModeToggleIcon">
                dark_mode
            </span>
        </a>
        
        
    </div>
</div>

<div id="single-column-footer">
Hugo Theme <a href="https://github.com/amazingrise/hugo-theme-diary">Diary</a> by <a href="https://risehere.net/">Rise</a>
<br>
Ported from <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a>'s <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> <br>
<br>

&copy;
	
	Copyright (c) 2015. All rights reserved.
	
</div>
            </div>
    
    <script src="/js/journal.js"></script></body>
</html>
