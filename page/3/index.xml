<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>悟剑阁</title>
    <link>https://sword865.github.io/</link>
    <description>Recent content on 悟剑阁</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Sat, 03 May 2025 15:51:35 +0800</lastBuildDate>
    
        <atom:link href="https://sword865.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>算法总结9—优化</title>
        <link>https://sword865.github.io/posts/archives/2009-09-30-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%939-%E4%BC%98%E5%8C%96/</link>
        <pubDate>Wed, 30 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-30-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%939-%E4%BC%98%E5%8C%96/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-30-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%939-%E4%BC%98%E5%8C%96/ -&lt;p&gt;不同于之前的分类和聚类算法，优化的目的是尝试找到一个使成本函数输出最小化的值。这里主要包括两个算法：模拟退火算法和遗传算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;成本函数&lt;/strong&gt;:&lt;br&gt;
接受一个经推测的题解，并返回一个数值结果，该值越大代表成本越高（题解表现越差），该值越小就表示题解越好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模拟退火算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;优化算法的目标可以看为寻找x使函数f(x)最小。&lt;/p&gt;
&lt;p&gt;但是严格的最小值往往是很难达到的，我们不得不把眼光投入到寻找一个尽可能好的次优解去。&lt;/p&gt;
&lt;p&gt;最简单的方法被称为随机法，即成千上万次的对x进行猜测，然后把这些x中使f(x)最小的一个作为答案。虽然这样很简单，但是效果很差。于是出现了爬山法。&lt;/p&gt;
&lt;p&gt;爬山法从一个随机解出发，然后不断向该解附近的使f(x)的值更小的x移动，直到当前x附近的解都比x差为止。为了使效果更好，我们可以从多个随机解出发重复着一个过程，将最好的一个作为答案。很容易就能认识到，这样找到的解是一个极值点，是一个局部最小值。&lt;/p&gt;
&lt;p&gt;爬山法虽然好，但是在其寻找最优解的过程中，前进的方向是固定的（使f(x)更小的方向），但是有时向其他方法前进也是必要的，因为f(x)可能先增大在变小成为最优的。&lt;/p&gt;
&lt;p&gt;于是就有了模拟退火法。&lt;/p&gt;
&lt;p&gt;该算法这源于固体的退火过程，即先将温度加到很高(大量原子被激发)，再缓慢降温(即退火)，则能使达到能量最低点。如果急速降温(即为淬火)则不能达到最低点。&lt;/p&gt;
&lt;p&gt;模拟退火法同样是从一个随机解出发。但是它在寻找最优解时并不一定是向更好的x移动，也有一定的概率向更差的x移动，这个概率开始较大，但是会随时间而渐渐变小，直到稳定。一般该概率可以定义为：p=e ^ (-  (highcost – lowcost ) / temperature )，其中temperature是随时间增大而变小的温度，开始温度很高时p -&amp;gt; 1，后来会渐渐变小使p-&amp;gt;0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;遗传算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;遗传算法的思想来自生物的遗传和变异，算法以种群为单位（一个种群为一组既多个解），其算法的运行过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;随机生成一组初始解（初始种群）。&lt;/li&gt;
&lt;li&gt;计算种群中各个解的成本，然后进行排序。&lt;/li&gt;
&lt;li&gt;我们将种群中靠前（成本低）的解保留下来，删除其他解，这一过程称为精英选拔。&lt;/li&gt;
&lt;li&gt;对已有解进行微小的改变，将改变后的结果作为新的元素加入种群，这一过程称为变异。&lt;/li&gt;
&lt;li&gt;选择一些优秀的解两两组合，然后将他们按某种方式进行结合（如求平均），将得到的结果作为新的元素加入种群，这一过程称为交叉（配对）。&lt;/li&gt;
&lt;li&gt;不断重复2—5步，直到达到指定迭代次数或成本函数连续数代都没有更好的改善。&lt;/li&gt;
&lt;li&gt;得到一组解，该组解为算法输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;该系列结束，恩，也许以后学了更多，有了更好的了解后会回来改一改，谁知道呢？&lt;/p&gt;
- https://sword865.github.io/posts/archives/2009-09-30-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%939-%E4%BC%98%E5%8C%96/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>算法总结8—非负矩阵因式分解</title>
        <link>https://sword865.github.io/posts/archives/2009-09-25-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%938-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%9B%A0%E5%BC%8F%E5%88%86%E8%A7%A3/</link>
        <pubDate>Fri, 25 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-25-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%938-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%9B%A0%E5%BC%8F%E5%88%86%E8%A7%A3/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-25-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%938-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%9B%A0%E5%BC%8F%E5%88%86%E8%A7%A3/ -&lt;p&gt;&lt;strong&gt;数学基础:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;线性代数的矩阵乘法运算。&lt;/p&gt;
&lt;p&gt;   非负矩阵分解是一种特征提取的算法，它尝试从数据集中寻找新的数据行，将这些新找到的数据行加以组合，就可以重新构造出数据集。&lt;/p&gt;
&lt;p&gt;   算法要求输入多个样本数据，每个样本数据都是一个m维数值向量，首先把我们的数据集用矩阵的形式写出来，每一列是一个数据，而每一行是这些数据对应维度的数值。于是我们就有了一个大小为m*n的输入矩阵。而算法的目标就是将这个矩阵分解为另外两个非负矩阵的积。&lt;/p&gt;
&lt;div&gt;$$M_{m,n}=A_{m,r}B_{r,n}$$&lt;/div&gt;
&lt;p&gt;   我们将分解矩阵后新得出的一个维度称为特征，那么在前一个m*r的矩阵中，第i行第j列的值就代表属性i对第j种特征的贡献值，而后一个矩阵的第i行第j列则代表第i种特征对第j个样本的贡献值。这样我们就找出了输入样本的r种特征。&lt;/p&gt;
&lt;p&gt;   r的大小应该依照需要进行选择，比如如果是希望找到某些共性特征，则就要选择较小的r。当我们确定了一个较为合适的r值后，就要想办法确定后面两个矩阵具体的值了。&lt;/p&gt;
&lt;p&gt;   书中给出的算法大致如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定义一个函数计算用来两个矩阵的差异程度（每个对应元素相减后平方的和）&lt;/li&gt;
&lt;li&gt;随机生成2个矩阵(m&lt;em&gt;r维和r&lt;/em&gt;n维)记为A（权重矩阵）,B（特征矩阵）&lt;/li&gt;
&lt;li&gt;计算A&lt;em&gt;B与输入的m&lt;/em&gt;n的数据矩阵的差异，足够小则停止，否则继续&lt;/li&gt;
&lt;li&gt;按一定规则调整A，B的值后转3.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于调整的方法，可以用模拟退火（下一篇文章中会提到）等多种算法，书里使用的是乘法更新法则，该法则我没有认真去看….感兴趣的可以去看论文….英文的…&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://hebb.mit.edu/people/seung/papers/nmfconverge.pdf&#34;&gt;http://hebb.mit.edu/people/seung/papers/nmfconverge.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;算法如下：&lt;/p&gt;
&lt;p&gt;hn 转置后的权重矩阵和数据矩阵相乘的结果&lt;/p&gt;
&lt;p&gt;hd 转置后的权重矩阵和原权重矩阵相乘再乘特征矩阵的结果&lt;/p&gt;
&lt;p&gt;wn数据矩阵与转置后的特征矩阵相乘的结果&lt;/p&gt;
&lt;p&gt;wd权重矩阵与特征矩阵相乘，再与转置后的特诊矩阵相乘得到的矩阵&lt;/p&gt;
&lt;p&gt;为了更新特征矩阵和权重矩阵，我们先把上面所有矩阵变为数组．然后把特征矩阵中每一个值与hn中对应值相乘，并除以hd中对应的值．类似的，我们再将权重矩阵中每一个值与wn中的对应值相乘，并除以wd中对应的值．&lt;/p&gt;
&lt;p&gt;最近的算法都很好理解的样子…不过写起来还是挺麻烦的….还有最后一篇优化了，内容挺多，包括模拟退火和遗传算法….恩&lt;/p&gt;
&lt;div&gt;
  &lt;embed id=&#34;lingoes_plugin_object&#34; width=&#34;0&#34; height=&#34;0&#34; type=&#34;application/lingoes-npruntime-capture-word-plugin&#34; hidden=&#34;true&#34; /&gt;
&lt;/div&gt;
- https://sword865.github.io/posts/archives/2009-09-25-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%938-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%9B%A0%E5%BC%8F%E5%88%86%E8%A7%A3/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>算法总结7—多维缩放</title>
        <link>https://sword865.github.io/posts/archives/2009-09-20-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%937-%E5%A4%9A%E7%BB%B4%E7%BC%A9%E6%94%BE/</link>
        <pubDate>Sun, 20 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-20-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%937-%E5%A4%9A%E7%BB%B4%E7%BC%A9%E6%94%BE/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-20-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%937-%E5%A4%9A%E7%BB%B4%E7%BC%A9%E6%94%BE/ -&lt;p&gt;一直没有时间写…..唉&lt;/p&gt;
&lt;p&gt;这个东西好像是属于数据可视化？反正就是把多维的数据降到低维空间但是仍然尽可能的保持原来数据之间的距离关系(就是在原来维度下离的远的点仍然离得远，接近的点仍然接近) 。最常见的应该就是降到2维以方便打印和屏幕输出。&lt;/p&gt;
&lt;p&gt;算法的输入是所有数据在高维情况下两两之间的距离（记i与j的距离为Dij）。现在以降到2维为例说明这个算法。&lt;/p&gt;
&lt;p&gt;首先我们把所有数据点随机绘制在一张二维图像上，然后计算它们两两之间的距离dij，然后我们计算出它与高维距离Dij的误差，根据这些误差，我们将每对数据点按比例移近或移远，然后重新计算所有dij，不断重复到我们没法减少误差为止。&lt;/p&gt;
&lt;p&gt;还是来具体说明一下吧，假设有n个点&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输入每一对点之间的距离Dij。&lt;/li&gt;
&lt;li&gt;随机在2维平面生成n个点，点i坐标记为x[i]、y[i]，计算它们两之间的距离，记为dij.&lt;/li&gt;
&lt;li&gt;对所有i 和j计算：eij=(dij-Dij) / Dij，每个点用一个二维的值grad[k]来表示它要移动的距离的比例因子(初始为0,0)。在计算出每个eij后，计算 ((x[i]-x[j]) / dij)* eij，然后把它加到grad[i][x]上，同样把((y[i]-y[j]) / dij)* eij加到grad[i][y]上。&lt;/li&gt;
&lt;li&gt;把所有eij的绝对值相加，为总误差，与前一次的总误差比较(初始化为无穷大)，大于前一次的话就停止。否则把它作为上一次总误差，继续。&lt;/li&gt;
&lt;li&gt;对每个点，新的坐标为x[i] -= rate * grad[i][x]  y[i] -= rate*grad[i][y]，其中rate是开始时自己定义的一个常数参数，该参数影响了点的移动速度。重新计算各个dij，回到3。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;伪码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    for m = 1 to 1000{
      for i=1 to n
        for  j = 1 to n
          dij=sqrt((x[i] – x[j])^2+(y[i]-y[j])^2)
      for i=1 to n
        gradi=0
      totale=0
      for i= 1 to n
        for j= 1 to n{
           if(j==i) continue
             eij=(dij-Dij) / Dij
             grad[i][0]+= ((x[i] - x[j]) / dij)* eij
             grad[i][1]+=((y[i] - y[j]) / dij)* eij
             totale+=abs(eij)
           }
      if (laste &amp;amp;lt; totale) break;
      laste=totale
      for i=1 to n{
        x[i] -= rate * grad[i][x]
        y[i] - = rate* grad[i][y]
      }
    }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; &lt;/p&gt;
&lt;p&gt;恩，就是这样….最近这几个算法都不是很难恩………..&lt;/p&gt;
- https://sword865.github.io/posts/archives/2009-09-20-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%937-%E5%A4%9A%E7%BB%B4%E7%BC%A9%E6%94%BE/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>算法总结5&amp;6—-k-最近邻与聚类</title>
        <link>https://sword865.github.io/posts/archives/2009-09-14-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%9356-k-%E6%9C%80%E8%BF%91%E9%82%BB%E4%B8%8E%E8%81%9A%E7%B1%BB/</link>
        <pubDate>Mon, 14 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-14-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%9356-k-%E6%9C%80%E8%BF%91%E9%82%BB%E4%B8%8E%E8%81%9A%E7%B1%BB/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-14-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%9356-k-%E6%9C%80%E8%BF%91%E9%82%BB%E4%B8%8E%E8%81%9A%E7%B1%BB/ -&lt;p&gt;因为这两个算法比较简单，又有些相似，所以这里放在一起。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;K-最近邻：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;k-最近邻也是一种用来进行预测的算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;工作原理：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;接受一个用以进行数值预测的新数据项，然后将它与一组已经赋过值的数据项进行比较。算法会从中找出与待预测数据最为接近的k项，并这k项其求均值以得到最终的结果。&lt;/p&gt;
&lt;p&gt;总计来说这是一个很简单的算法，只要我们做好距离的定义并选择一个适合的k值，我们就可以很容易的实现它。&lt;/p&gt;
&lt;p&gt;由于我们计算2组数据的距离的通常方法是将他们中对应的每一项目的差值的绝对值(或平方)相加，所以就会出现不同数据范围不同导致的误差。比如每组数据有2个分量，一个取值为0—10,另一个是0—-999999，那么第二的值就会几乎完全决定我们最后的结果。所以我们要对每一组数据进行缩放。&lt;/p&gt;
&lt;p&gt;对数据的缩放取决于具体的应用，我们可以通过交叉验证尝试多组缩放因子然后比较它们的优劣。交叉验证的做法是先数据的一部分去除，然后用剩余数据去推测这组数据，我们就可以根据预测的结果对缩放因子进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;能利用复杂函数进行数值预测，又简单易懂，并且我们可以很容易的在算法中实现查看用哪些近邻进行预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每次进行预测，它都会使用所有样本，这会导致效率的低下。&lt;/p&gt;
&lt;p&gt;寻找缩放因子是一项很乏味的工作.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;聚类：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;聚类算法可以用于任何具有一个或多个数值属性的数据集合，通过这些数值属性，我们将其所有数据映射到一个n维空间中，并定义该空间中的距离，然后我们可以通过各个数据间的距离对其实现聚类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分级聚类:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;分级聚类的算法是不断找出所有数据中距离最小的两个数据A、B，然后将它们合并成一个新的节点，该节点在n维空间中的坐标是原来两数据点的均值，通过不断进行这一操作，我们最终可以得到一个树形的层级结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;K-均值聚类:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;不同于分级聚类，K-均值聚类的目的是将数据拆成K个不同的群组，其具体算法如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在n维空间中随机生成K个中心点&lt;/li&gt;
&lt;li&gt;将每个数据项分配给与其距离最近的中心点。&lt;/li&gt;
&lt;li&gt;将中心点位置移动到所有分配给它的数据项的中心。如果中心点位置没有改变，则结束算法，否则回到第二步。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体选择哪种聚类算法取决于要处理的问题，当要将数据拆分到不同的群组时，k均值聚类往往是很有价值的，而如果我们更想了解哪些群组间更为接近，分级聚类更好。当然，我们也可以同时使用２种算法得到更加详细的信息。&lt;/p&gt;
&lt;div&gt;
  &lt;embed id=&#34;lingoes_plugin_object&#34; width=&#34;0&#34; height=&#34;0&#34; type=&#34;application/lingoes-npruntime-capture-word-plugin&#34; hidden=&#34;true&#34; /&gt;
&lt;/div&gt;
- https://sword865.github.io/posts/archives/2009-09-14-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%9356-k-%E6%9C%80%E8%BF%91%E9%82%BB%E4%B8%8E%E8%81%9A%E7%B1%BB/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>统计,逻辑与智能</title>
        <link>https://sword865.github.io/posts/archives/2009-09-10-%E7%BB%9F%E8%AE%A1%E9%80%BB%E8%BE%91%E4%B8%8E%E6%99%BA%E8%83%BD/</link>
        <pubDate>Thu, 10 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-10-%E7%BB%9F%E8%AE%A1%E9%80%BB%E8%BE%91%E4%B8%8E%E6%99%BA%E8%83%BD/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-10-%E7%BB%9F%E8%AE%A1%E9%80%BB%E8%BE%91%E4%B8%8E%E6%99%BA%E8%83%BD/ -&lt;p&gt;      今天上了开学的第一节统计学，开了很久的小差，想了不少东西。&lt;/p&gt;
&lt;p&gt;      以前虽然自学过概率论与数理统计，但是也只是了解了一些公式与原理，一直对于统计学的一些应用不甚理解(或者说不能接受)，尤其是基于统计的机器学习，一直不能接受它作为一种实现的人工智能的手段。因为我心中的人工智能是绝对理性，严谨，逻辑的。虽然我可以接受统计学的理论，却不能把它作为一种严谨的逻辑。&lt;/p&gt;
&lt;p&gt;　 但是，今天突然想到了感性，是的，人是理性的，但是人的思维中也充满了感性的，当然，这是早已熟知的事实。&lt;/p&gt;
&lt;p&gt;　 先给感性下一个定义吧。&lt;/p&gt;
&lt;p&gt;　 感性：作用于人的感觉器官而产生的感觉，知觉和表象等直观认识，相对与‘理性’”&lt;/p&gt;
&lt;p&gt;　 是的，感性一种直观的认识，那么这种认识从哪里来呢？过去的经验。人们的感性是在经验的基础上建立的，是一种仅仅由经验得出而没有任何逻辑背景的判断。&lt;/p&gt;
&lt;p&gt;　 统计学不也是这样么？将大量的样本作为过去的经验，仅仅由这些经验而不带任何逻辑推断的去快速做出一种“感性”的判断。只是这种感性比人的感性更加严谨，不会受到类似“小概率事件经常发生”这种错觉的影响，但也可以算是一种理性的感性了。&lt;/p&gt;
&lt;p&gt;      对应的，我又想起了逻辑学，如果统计是根据经验快速简单的做出判断的话，那么逻辑学就是通过严谨的逻辑推理去寻找正确的答案，这个过程会很繁琐，但是它使绝对严谨理性的，比我们的大脑更加严谨，理性——那何不把它看成一种是理性的理性呢？&lt;/p&gt;
&lt;p&gt;     但是仅仅有统计与逻辑，我们无法建立一个系统，因此也许还需要一个驱动吧？在完成一个任务、解决一个问题时，这个驱动不断的让感性提供可能解，然后让理性验证它——突然我发现，这不就是“启发式搜索”所作的事情么？&lt;/p&gt;
&lt;p&gt;     以前翻过一些人工智能的书，总是觉的虽然那些方法可以达到目的，但是却没有触及到智能的本质，因此总是有些失望的，可是现在，我释然了。什么是智能的本质？好像是在《与众不同的心理学》这本书上，我看到过类似问题(也许问得是别的什么，不过差不多)。书里说，这是不可验证的，如果我们甚至不能解释，验证它，我们为什么可以凭借自己的主观推断去确定一个机器是否拥有智能？我们凭什么可以认为，这些机器，当他们把现在这些技术发挥到一定程度后就不可以拥有智能？也许我们自己的自我认知也只是一种数学的算法对自身产生的作用呢？（是不是有谁说过，这个宇宙，连同我们的存在，都只是一种错觉？记不清了…..不过看来这句话还是很有意思的。）&lt;/p&gt;
&lt;p&gt;　 想到了这些之后，我对“人工智能最难的是处理常识”第一次有了很深的认同，以前总是不能充分认识常识的作用，但是如果直觉，经验在智能中占了如此重要的一部分，那么我们就必须去处理常识――其中的困难自然不用多说了。&lt;/p&gt;
&lt;p&gt;     最后，把我上课时写在书上的话记录下来吧：&lt;/p&gt;
&lt;p&gt;     统计学—以理性研究感性，我们的直觉从过去的经验去推导未来，这种推断不能解释结果的原因。（因为它在历史上倾向于如此，所以它很可能如此。）统计学将这种感性理性化，并出除了一些直觉上的错误（如：小概率事件经常发生），但其根本上还是一种感性的判断，因此解释这种感性推断背后的原因，事物呈现这种状态的原因，就是人的工作了。所以统计学也可以用来在没有线索时，作为一种“事后诸葛亮”式的推断的第一步（即先找出最可能答案，在设法解释它，不过这种方法具有不可证伪性，所以不是科学严谨的――毕竟是直觉么）。同时，统计的机器学习可以就可用来模拟人的直觉学习了（而且是一种没有错误的直觉）。&lt;/p&gt;
- https://sword865.github.io/posts/archives/2009-09-10-%E7%BB%9F%E8%AE%A1%E9%80%BB%E8%BE%91%E4%B8%8E%E6%99%BA%E8%83%BD/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>算法总结4—支持向量机</title>
        <link>https://sword865.github.io/posts/archives/2009-09-08-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%934-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</link>
        <pubDate>Tue, 08 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-08-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%934-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-08-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%934-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/ -&lt;p&gt;支持向量机……复杂的东西，书里讲得也不怎么详细，起码具体算法没有说……所以又去查了些资料……&lt;/p&gt;
&lt;p&gt;支持向量机是用来对数据进行分类的。&lt;/p&gt;
&lt;p&gt;首先从最简单的情况开始吧：&lt;/p&gt;
&lt;p&gt;如果有一条直线，我们把它看成一条数轴，上面有一些样本点，其中坐标大于某个值的点都属于一类，坐标小于某个值的点都属于一类，那么我们就可以用这个值来做分分界点，它点把直线上的点分为了两类。因为样本点是有限可数的。所以这个分类点的取法不唯一。选好后，随便给我们一个点，我们就可以根据这个随机给出的点是在分界点的左侧还是右侧来判断这个点的类别。&lt;/p&gt;
&lt;p&gt;同样，一个平面上有很多样本点，这些点也分为2类，如果我们在平面上可以找到这样一条直线满足这两类样本点分别分布在直线的两侧，那么我们就可以用这个平面作分界面，来对之后随机给出的点进行分类。&lt;/p&gt;
&lt;p&gt;仍然用同样的方法，我们可以用一个平面给分布在一个3维立体空间中的点分类。&lt;/p&gt;
&lt;p&gt;总结起来就是说：在n维空间中有很多样本点，如果我们能找到一个n-1维的超平面，这个平面恰好把空间中的样本点分在它的两侧，那么我们就可以用这个n-1维的超平面来对之后随机给出点分类。&lt;/p&gt;
&lt;p&gt;这种方法有两个问题：&lt;/p&gt;
&lt;p&gt;1）  因为那个n-1维的超平面选法往往是不唯一的，我们要选哪一个?&lt;/p&gt;
&lt;p&gt;2）  更多情况下，我们找不到这样一个n-1维超平面，你可以想象更多情况下，我们要分类的数据是“混合”在一起的，很难简单的用一个点，一条线，或者一个更高纬度的线性分类器把它分开。&lt;/p&gt;
&lt;p&gt;接下来我们就要来解决这个问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最优超平面的确定:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果选择合适的分界超平面呢？直观的来说，我们因该选择一个距离两组数据“最远”的超平面。首先每个点都和这个超平面有一个距离（该距离可以通过把n维空间放入一个n维坐标系后用代数的方法计算出来，具体计算过程此处就不说了。不过1维２维３维的情况你应该能自己算出来吧～～～，理解就好）我们选择的超平面要让所有这些距离中最小的一个值最大。&lt;/p&gt;
&lt;p&gt;我们在n维空间空建立一个n维坐标系&lt;/p&gt;
&lt;p&gt;在这个n维坐标系中，每个n-1维超平面都可一个用一个方程表示出来，这里设为。&lt;/p&gt;
&lt;div&gt;$$H(x)=a_{0}+\sum_{i=1}^{n}(a_{i}*x_{i})$$&lt;/div&gt;
&lt;p&gt;我们用一个变量Y表示一个点相对超平面的关系，在一侧为1.另一侧为-1.&lt;/p&gt;
&lt;p&gt;可以证明:（证明过程略）&lt;/p&gt;
&lt;p&gt;该平面在满足下面的约束时：&lt;/p&gt;
&lt;div&gt;$Y_{i}H_(x_{i})\geq 1$&lt;/div&gt;
&lt;p&gt;极小化函数&lt;/p&gt;
&lt;div&gt;$\frac{1}{2}\sum_{j=1}^{n}(a_{j}^{2})$&lt;/div&gt;
&lt;p&gt;这是一个二次规划问题，我们对它求解就可以得到最优平面。&lt;/p&gt;
&lt;p&gt;有时我们找不到这样一个超平面，这时，我们可以把超平面的约束条件放的宽松一点，也就是在超平面附近允许两种分类的点的重叠，可以同过把改为（e&amp;gt;0）来实现这一目的。&lt;/p&gt;
&lt;p&gt;（具体证明与求解参考《统计学完全教程》 科学出版社 P290的支持向量机一节）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二个问题的解决—核方法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;很多时候，我们是找不到一个简单的超平面对样本进行划分的，这个时候，我们可以通过坐标变换，把样本点映射到一个可以线形划分的空间中。&lt;/p&gt;
&lt;p&gt;这个映射可以是同维度的，即映射前后样本空间的纬度相同，比如：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://pic002.cnblogs.com/images/2012/52809/2012063021394557.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;就可以通过一个简单的&lt;strong&gt;求平方&lt;/strong&gt;运算，把数据从线性不可划分变为线性可分—我们可以很容易的找到一条直线把后者的样本点分成两个部分。&lt;/p&gt;
&lt;p&gt;但是很多时候，问题没有这么简单，我们就需要用另外一种映射，即把样本点映射到更高纬度的空间去。&lt;/p&gt;
&lt;p&gt;比如上面的左图还可以做这么一种变换：&lt;/p&gt;
&lt;p&gt;$$z_1=x_1x_1, z_2=\sqrt{2}x_1x_2, z_3=x_2x_2$$&lt;/p&gt;
&lt;p&gt;这样我们就可以在新的样本空间中很简单的找到一个平面把这些点分开了．仔细分析，你可以发现，这个平面其实是左图中的一个椭圆的经过上述变换后得到的．&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;较高维空间的线性分类器对应于原空间的一个非线性分类器．&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这就是&lt;strong&gt;核方法&lt;/strong&gt;的核心。&lt;/p&gt;
&lt;p&gt;通过找到一个合适的映射，我们就可以前面的问题(2)了&lt;/p&gt;
&lt;p&gt;这种映射称为核函数，核函数的选择是很有技巧的，它也有一些常见的模型，很多时候我们只要选择合适的模型并计算适当的参数就可以了。具体方法这里不说了，有兴趣的可以参见《&lt;a href=&#34;http://download.csdn.net/source/1353188&#34;&gt;RBF核函数的支持向量机参数选择&lt;/a&gt;》一文。&lt;/p&gt;
&lt;p&gt;找到核函数后，我们就完全解决上述问题了。&lt;/p&gt;
&lt;p&gt;（其实这里还有一些简化计算的技巧，这些技巧与其它更具体的东西还是可以去看《统计学完全教程》 科学出版社，真是一本非常强大的书。）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以很快的判断一个样本的种类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺点:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;由于对每个数据集的最佳核变换及相应参数都不一样，所以对每个数据集都要重新学习确定函数与参数。&lt;/p&gt;
&lt;p&gt;一般而言，支持向量机更适合包含大量数据的问题，而其他方法如决策树，更适合小规模的数据集。&lt;/p&gt;
&lt;p&gt;支持向量机也是一种黑盒技术，由于存在像高维空间的判断，我们很难解释分类的具体标准与原因。&lt;/p&gt;
&lt;div&gt;
  &lt;embed id=&#34;lingoes_plugin_object&#34; width=&#34;0&#34; height=&#34;0&#34; type=&#34;application/lingoes-npruntime-capture-word-plugin&#34; hidden=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p style=&#34;margin:0;padding:0;height:1px;overflow:hidden;&#34;&gt;
  &lt;a href=&#34;http://www.wumii.com/widget/relatedItems&#34; style=&#34;border:0;&#34;&gt;&lt;img src=&#34;http://static.wumii.cn/images/pixel.png&#34; alt=&#34;无觅相关文章插件，快速提升流量&#34; style=&#34;border:0;padding:0;margin:0;&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
- https://sword865.github.io/posts/archives/2009-09-08-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%934-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>算法总结3—神经网络</title>
        <link>https://sword865.github.io/posts/archives/2009-09-07-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%933-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Mon, 07 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-07-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%933-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-07-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%933-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ -&lt;p&gt;&lt;strong&gt;生物神经网络：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;    &lt;/strong&gt; 在生物的神经网络中的基本单位是神经元，神经元与神经元之间是由突触的相互联系来传递信息的，在静止息状态时，神经元的膜的内外电压保持一种稳定状态（膜内电压低于膜外电压），当神经元受到刺激后，在被刺激的部分周围，这种平衡状态会被打破，电压改变，与没有受到刺激的部分形成电流传递信息，电流的强弱取决于受刺激部位电压的改变量。&lt;/p&gt;
&lt;p&gt;     前一个神经元的轴突末梢作用于下一个神经元的胞体、树突或轴突等处组成突触。不同的轴突末梢可以释放不同的化学物质对下一个神经元产生不同的影响。也就是说会使下一个神经元的受刺激部分产生不同的电压，也就导致了不同程度的电流，最终也就传递了完全不同的信息。&lt;/p&gt;
&lt;p&gt;     一个神经元可以通过轴突作用于成千上万的神经元，也可以通过树突从成千上万的神经元接受信息。当多个神经元同时对一个神经元产生作用时，结果这些神经元的作用强度共同决定。&lt;/p&gt;
&lt;p&gt;     神经系统按功能可大致分为传入神经（感觉神经）、中间神经（脑：延脑、脑桥、小脑、中脑、间脑、大脑脊髓）与传出神经（运动神经）三类。&lt;/p&gt;
&lt;p&gt;     感受神经的作用是接受外界信息（输入），中间神经则起到了信息传递与计算分析的作用，最用，传出神经会负责对外界信息作出相应的反应（输出）。&lt;/p&gt;
&lt;p&gt;     模仿这一过程，我们就可以建立人工神经网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;人工神经网络：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;     人工神经网络的基本单位是人工神经元（以下简称神经元）。一个神经元可以有多个输入，每个输入有一个相应权值。&lt;/p&gt;
&lt;p&gt;图示如下：&lt;/p&gt;
&lt;img class=&#34;alignnone  wp-image-125&#34; src=&#34;http://upload.wikimedia.org/wikipedia/commons/9/97/Ncell.png&#34; alt=&#34;nn&#34; /&gt;
&lt;pre&gt;&lt;code&gt;a1~an为神经元的输入值
w1~wn为神经元各个的输入所拥有的权值
b为偏移量
sum对各个输入与其权值的积求和(含偏移量)。
f为传递函数，接受sum的输出，通过一个函数变换，输出t
t为神经元输出
数学表示 t=f(WA&#39;+b)
W为权向量
A为输入向量，A&#39;为A向量的转置
b为偏移量
f为传递函数
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在人工神经网络中，神经元之间相互连接，在连接点将前者的输出作为后者的输出，形成错综复杂的网状结构，进行信息的传递与计算。&lt;/p&gt;
&lt;p&gt;我们这里要介绍的是其中比较简单的一种模型，称为“多层感知机（MLP）”网络。&lt;/p&gt;
&lt;p&gt;为了简化模型，我们假设偏移量b=0.&lt;/p&gt;
&lt;p&gt;多层感知机网络由3部分组成：&lt;/p&gt;
&lt;p&gt;输入层：功能类似感受神经，每个节点接受外界的直接输入。这里的模型中，每个节点接受单一输入，权值为1。&lt;/p&gt;
&lt;p&gt;输出层：功能类似运动神经，该层输出就是神经网络的输出。&lt;/p&gt;
&lt;p&gt;隐藏层：是输入层和输出层之间的多层神经网络，可以有1或多层。&lt;/p&gt;
&lt;p&gt;因此，MLP网络中至少有3个层次。&lt;/p&gt;
&lt;img class=&#34;alignnone  wp-image-125&#34; src=&#34;https://sword865.github.io/images/archives/2012063021381464.jpg&#34; alt=&#34;mlp&#34; /&gt;
&lt;p&gt;这些层次中，每层的每个神经元的输出都会作为下一层的每个神经元的输入，因此当我们对输入层进行输入后，该信息会一层层传递下去，最终从输出层输出。&lt;/p&gt;
&lt;p&gt;神经网络建立后，我们需要设法确定每个神经元的各个输入的权重w，并选择合适的函数f对输入进行变换，只有完成以上工作后，我们才能使用神经网络完成相应的工作。&lt;/p&gt;
&lt;p&gt;我们一般会选择过关于源点对称的S形函数作为函数f，该种函数特点是:输入接近0时，函数对输入的变化有敏感的反应，这一敏感度将随输入绝对值的增大而下降，最终趋于0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;权重的获取：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;选择合适的函数后，我们就要去确定各权重w了，权重的选择取决于我们想要神经网络完成的任务，我们首先会给每个输入一个初始化的默认值，该值可任意选取。&lt;/p&gt;
&lt;p&gt;完成初始化后，我们就要开始训练神经网络了，即给神经网络大量的已知的正确的输入及其对应的输出，神经网络会将自己得到到的输出与正确输出向比较，然后根据某一算法调整自身的权重，使自身输出更接近正确答案。&lt;/p&gt;
&lt;p&gt;我们这里要介绍的调整算法称为&lt;strong&gt;反向传播法&lt;/strong&gt;，因为该算法是沿网络反向调整权值的。&lt;/p&gt;
&lt;p&gt;这一算法中，我们会分析输出与正确答案，并将将输出向正确答案推进，为了了解如何推进，我们需要一个函数来计算函数f的斜率，设该函数为g。根据该函数，我们可以计算sum因改变的值。&lt;/p&gt;
&lt;p&gt;整个算法如下：&lt;/p&gt;
&lt;p&gt;从后向前对输出层和所有隐含层：&lt;/p&gt;
&lt;p&gt;1）  计算节点当前输出与期望结果的差值d。(期望结果t – 实际输出 y)&lt;/p&gt;
&lt;p&gt;对输出层: t在输入训练数据时一同输入。&lt;/p&gt;
&lt;p&gt;对隐含层: t = sum ( 前一层的每个节点的差值di * 这两个节点间连线的权值 )&lt;/p&gt;
&lt;p&gt;2）  利用函数g确定函数f在节点输出值y处的改变速率v。v=g(y)&lt;/p&gt;
&lt;p&gt;3）  改变每个输入链接的权值，其改变量与链接的当前输入强度与学习速率rate（自己定义的属于(0,1)的常量）成正比。&lt;/p&gt;
&lt;p&gt;（每个wi的改变量为（v&lt;em&gt;d&lt;/em&gt;rate*输入ai））&lt;/p&gt;
&lt;p&gt;这样一层层的从后向前反推，最终完成对一个训练样本的学习。&lt;/p&gt;
&lt;p&gt;当对所有样本完成训练后，我们就可以使用这个神经网络了。&lt;/p&gt;
&lt;p&gt;比如，我们想用神经网络模拟一个数学函数，我们先向网络提供大量的正确的输入输出进行训练，然后就可以用神经网络作模拟这个函数进行计算了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;神经网络可以处理复杂的非线性函数，发现不同输入的隐含关系。&lt;/p&gt;
&lt;p&gt;神经网络也允许增量式训练，并且不需要大量的空间保存数据模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;神经网络只能接受数值输入输出，因此我们在处理非数值数据时，必须以一种方法把其每个取值映射到一个数值上。&lt;/p&gt;
&lt;p&gt;神经网络是一种黑盒方法，这使得我们无法得知其处理输入和输出的方法与原因，尤其是较大的神经网络有数百的节点和上千的连接，更是如此。&lt;/p&gt;
&lt;p&gt;在训练神经网络时，在选择训练数据的比例及与问题相适应的网络规模方面，没有明确的 规则。如果训练数据比率太大，可能导致网络对噪音数据产生过度归纳，而训练比率太低，网络就不够精准。&lt;/p&gt;
- https://sword865.github.io/posts/archives/2009-09-07-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%933-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>算法总结2—决策树分类器</title>
        <link>https://sword865.github.io/posts/archives/2009-09-06-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%932-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E5%99%A8/</link>
        <pubDate>Sun, 06 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-06-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%932-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E5%99%A8/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-06-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%932-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E5%99%A8/ -&lt;p&gt;&lt;strong&gt;数学基础：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;树：树是一种数据结构，它是由n（n&amp;gt;=1）个有限结点组成一个具有层次关系的集合。把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。它具有以下的特点：&lt;/p&gt;
&lt;p&gt;每个结点有零个或多个子结点；&lt;/p&gt;
&lt;p&gt;每一个子结点只有一个父结点；&lt;/p&gt;
&lt;p&gt;没有前驱的结点为根结点；&lt;/p&gt;
&lt;p&gt;除了根结点外，每个子结点可以分为m个不相交的子树；&lt;/p&gt;
&lt;p&gt;没有子节点的节点称为叶节点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;决策树分类器原理：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;决策树是一颗树，要分类的样本从树根进入，在树的每个节点通过对样本的某种属性的判断选择不同的路径逐步下降到底,得出其所属类别。&lt;/p&gt;
&lt;p&gt;例图:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://pic002.cnblogs.com/images/2012/52809/2012063021362216.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;为了建立一棵决策树,我们首先应向程序输入大量训练数据(包含所属类别的数据)，程序将根据训练数据按某一算法自动生成决策树。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;决策树生成算法:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;   为了构造决策树，算法首先创建一个根节点，然后通过分析训练数据，逐步选出适合的变量对数据进行拆分(即逐步构造上图中的非叶子节点。)&lt;/p&gt;
&lt;p&gt;   为了选择适合的变量对数据进行拆分，我们需要一个方法来评估一种拆分方案的好坏，&lt;strong&gt;其评估方法包括：&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;基尼不纯度：&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;定义：基尼不存度是指来自集合的某种结果随机应用于集合中某一数据的预期误差。（如果集合中所有结果属于同一类，则误差为0）&lt;/p&gt;
&lt;p&gt;使用：利用这一思想，我们可以将集合中每种类别的数据出现的次数除以数据总数计算相应概率，再将这些概率的乘积相加（所有概率两两相乘后在相加），这样就会得到某一数据被随机分配到错误结果的总概率。&lt;/p&gt;
&lt;p&gt;伪代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;imp=0
for k1 in kinds
    p1=count(k1) / total
    for k2 in counts
        if (k1==k2)continue
        p2=count(k2) / total
        imp+=p1*p2
ans=imp
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;left&#34;&gt;
  &amp;nbsp;&amp;nbsp; (p1*p2是一个p1类别的数据被当作p2的概率)
&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;**熵：**在信息论中，熵代表的是集合的无序程度—–基本上就相当于我们在此处所说的集合的混杂程度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;熵的值是遍历所有结果后得到的pi*log2(pi)的和的绝对值&lt;/p&gt;
&lt;p&gt;伪代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ent=0.0
for k in kinds
    p=count(k) / total
    ent=ent – p*log2(p)     // 因为0&amp;lt;p&amp;lt;=1，所以必有log2(p)&amp;lt;=0
ans=ent
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;有了上述评估方法后，我们就可以不断尝试各种拆分方法，然后选出最好的拆分方法构造树中的节点了。我们将计算拆分前的熵（基尼不存度）值，与拆分后的熵（基尼不存度）的值的加权平均，将其差值作为&lt;strong&gt;信息增益&lt;/strong&gt;。最终对能得到最大信息增益的属性进行拆分。然后再分别对拆分后得集合选择属性进行拆分，直到最大信息增益为非正时停止拆分，这时决策树就构建完毕了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优化：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;为了防止决策树变的过度拟合（过度针对训练数据），我们可以在信息增益小于某个值后就停止拆分。但是我们可能遇到这样的数据―――某次拆分信息增益很小，但下一次就会很大。为了防止这一状况，我们可以在用先前的方法构造整棵树后，在尝试消除多余的节点。这个过程就是&lt;strong&gt;剪枝&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;剪枝的过程就是对具有相同父节点的节点进行检查，判断将其合并后，信息增益是否会小于某个指定发值。若是，则合并这些节点。合并后节点包括所有可能的结果值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在处理数值型数据时，熵和基尼不存度并不是一个好的选择，因为有些数值相差很近，有些相差很远，不能简单用是否为同一类别进行判断。所以我们可以用方差代替它们。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;决策树对缺失数据的处理：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当我们要判断类别的样本缺少某些决策树作判断时必须的数据时，我们可以选择同时走两个分支，不过我们不是平均统计各分支的结果值，而是进行加权统计。为了达到这一目标，决策树中每个节点都有一个值为１的权重，即观测数据对于数据向是否属于某个特定分类的概率具有１００％的影响，而如果走多个分支，我们将给每个分支一个权重，其值等于所有位于该分支的其他数据所占的比重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;决策树最大的优势是它可以轻易对一个受训模型给予解释。（解释分类原理）&lt;/p&gt;
&lt;p&gt;决策树可以同时接受分类型和数值型数据。&lt;/p&gt;
&lt;p&gt;比起贝叶斯分类器（参考**[&amp;lt;集体智慧编程&amp;gt;算法总结1—贝叶斯分类器][2]**）决策树可以更好的处理变量间的相互影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;决策树无法单独对某一数据进行训练。&lt;/p&gt;
&lt;p&gt;面对有大量可能结果的数据集时，决策树不够有效。当可能的分类结果较多时，决策树就会过度复杂，预测效果也会较差。&lt;/p&gt;
&lt;p&gt;另外，决策树不适合处理大量的数值型输入输出，因为它只能创建一些简单的&amp;gt; ，&amp;lt;，＝等节点，但数值之间可能存在更多更复杂的关系。&lt;/p&gt;
&lt;p&gt;呼….这个写的不怎么累了….很多都是书上的原话哈，不像昨天那个几乎都是自己写的……&lt;/p&gt;
&lt;p style=&#34;margin:0;padding:0;height:1px;overflow:hidden;&#34;&gt;
  &lt;a href=&#34;http://www.wumii.com/widget/relatedItems&#34; style=&#34;border:0;&#34;&gt;&lt;img src=&#34;http://static.wumii.cn/images/pixel.png&#34; alt=&#34;无觅相关文章插件，快速提升流量&#34; style=&#34;border:0;padding:0;margin:0;&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
- https://sword865.github.io/posts/archives/2009-09-06-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%932-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E5%99%A8/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>算法总结1—贝叶斯分类器</title>
        <link>https://sword865.github.io/posts/archives/2009-09-05-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%931-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</link>
        <pubDate>Sat, 05 Sep 2009 00:00:00 +0000</pubDate>
        
        <guid>https://sword865.github.io/posts/archives/2009-09-05-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%931-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</guid>
        <description>悟剑阁 https://sword865.github.io/posts/archives/2009-09-05-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%931-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/ -&lt;p&gt; 这几天以很快的速度翻完了&amp;lt;集体智慧编程&amp;gt;,因为只是对里面的算法感兴趣,对那些web2.0的应用没什么感觉,所以很多地方都是一扫而过,现在按最后一章的顺序来对所有相关的算法作一个详细的复习….&lt;/p&gt;
&lt;p&gt;这个是第一篇……贝叶斯分类器&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数学基础：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;条件概率&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;定义：设A, B是两个事件，且P(A)&amp;gt;0 称P(B∣A)=P(AB)/P(A)为在条件 A下发生的条件事件B发生的条件概率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;乘法公式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设P(A)&amp;gt;0，则有P(AB)=P(B∣A)P(A)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全概率公式和贝叶斯公式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;定义: 设S为试验E的样本空间，B1, B2, …Bn为E的一组事件，若BiBj=Ф, i≠j, i, j=1, 2, …,n; B1∪B2∪…∪Bn=S则称B1, B2, …, Bn为样本空间的一个划分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设试验E的样本空间为，A为E的事件，B1, B2, …,Bn为的一个划分，且P(Bi)&amp;gt;0(i=1, 2, …n)，则P(A)=P(A∣B1)P(B1)+P(A∣B2)+ …+P(A∣Bn)P(Bn)称为全概率公式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设试验E的样本空间为S，A为E的事件，B1, B2, …,Bn为的一个划分，则P(Bi∣A)=P(A∣Bi)P(Bi)/∑P(B｜Aj)P(Aj)=P(B｜Ai)P(Ai)/P(B)称为贝叶斯公式。&lt;/p&gt;
&lt;p&gt;说明：i，j均为下标，求和均是1到n&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯分类器原理：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过某些特征对不同的内容进行分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征的定义&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;任何可以用来判断内容中具备或缺失的东西。如要对文档进行分类时，所谓的内容就是文档，特征就是文档中的单词(当然你也可以选择其他合理的东西)&lt;/p&gt;
&lt;p&gt;当向贝叶斯分类器输入一个要进行分类的样本后，分类器会先对该样本进行分析，确定其特征，然后将根据这些特征时，计算样本属于各分类的概率。&lt;/p&gt;
&lt;p&gt;朴素贝叶斯分类器的具体工作步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;学习：向分类器输入一系列的训练数据，注意这些数据是包括其所属类别的，分类器将对训练数据进行分析，计算出&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1.各个特征在各个分类中出现的概率(=某分类中具有该特征的数据数目/该分类数目)如先计算出各个单词在各种分类的文档出现的概率。&lt;/p&gt;
&lt;p&gt;将该概率作为某分类下某特征出现的条件概率P(feature|category)&lt;/p&gt;
&lt;p&gt;2.任选一个样本属于某分类的概率(=某分类文章数 / 文章总数)&lt;/p&gt;
&lt;p&gt;记该概率为p(category)&lt;/p&gt;
&lt;p&gt;在朴素的贝叶斯分类器中，我们假设将要组合的各个概率相互独立(当然，很多时候并非如此。我们有时会发现，当样本拥有某一特征时，则它就更可能拥有另一项特征。)&lt;/p&gt;
&lt;p&gt;2)分类计算：在向分类器提供大量学习数据后，我们就可以用它对新的样本进行分类了。&lt;/p&gt;
&lt;p&gt;首先对样本进行分析，找出其具有的各种特征，利用这些特征，我们来计算各个分类中出现该样本的概率p(sample | category)。为了完成这一计算，我们只要简单将该分类下在该文档中出现过的特征出现的条件概率相乘即可。即∏P(feature | category) 这里的feature是该样本拥有的所有特征。&lt;/p&gt;
&lt;p&gt;但是，我们实际要计算的是P (category | sample),即给定样本属于某分类的条件概率。&lt;/p&gt;
&lt;p&gt;这里，就用到了贝叶斯定理：P(A | B)=P(B | A)P(A) / P(B)&lt;/p&gt;
&lt;p&gt;这里就是：P(category | sample)= P(sample | category)P(category) / P(sample)&lt;/p&gt;
&lt;p&gt;其中，P(sample | category)P(category)都已经在学习中计算得到，而P(sample)是样本出现 的概率，我们可以计算它，但是这是没有意义的，因为我们会计算出各个分类的条件概率，然后比较它们的大小确定样本所属的分类，而对各个条件概率而言p(sample)是完全一样的。所以，我们就省去了对它的计算。&lt;/p&gt;
&lt;p&gt;这样，我们就可以确定一个样本的具体分类了。&lt;/p&gt;
&lt;p&gt;当然，以上算法是有很多改进的，比如对各分类定义一个最小阀值，一旦没有达到任一分类的阀值，则归属为未知。&lt;/p&gt;
&lt;p&gt;另外，针对朴素的贝叶斯分类器的缺点，还有很多其他的计算方法，如费舍尔方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;费舍尔方法&lt;/strong&gt;为文档中的每个特征都求得了分类的概率，然后又将这些概率组合起来，并判断其是否有可能构成一个随机集合。该方法还会返回每个分类的概率，这些概率彼此间可以进行比较。尽管这种方法更为复杂，但是因为它在为分类选择临界值（cutoff）时允许更大的灵活性，所以还是值得一学的。&lt;/p&gt;
&lt;p&gt;不同于之前计算的P(feature | category)，费舍尔方法将计算P(category | feature)，即拥有某一特征的样本属于某分类的概率。首先用P(具有指定特征的属于某分类的样本数 |具有指定特征的样本数)作为这一概率，然后将特征出现在所有所有分类中的概率fresum=∑P(category | feature),最后用P(category | feature) / fresum作为新的P(category | feature)。(这样修正后的值将比修正前更加有效)&lt;/p&gt;
&lt;p&gt;然后，我们就可以通过将各个特征的概率组合起来得到待分类样本所属的类别了。我们可以通过简单的相乘完成这一目的，虽然这里也作了相互独立的假设，但也比之前号上很多了。费舍尔方法的计算方法是将所有概率相乘后取自然对数，在乘以-2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;朴素的贝叶斯分类器最大的优势是他在接受大量数据训练和查询时的高速度。尤其当训练量递增时更是如此(我们可以分多次的对其进行学习的训练，而一些其他的方法如决策树和支持向量机要一次传送整个训练数据集)&lt;/p&gt;
&lt;p&gt;另一个优点是，其对分类器的学习情况有着比较简单的解释，我们可以简单的通过查询学习时计算的一些概率值来了解其分类原理。&lt;/p&gt;
&lt;p&gt;朴素的贝叶斯分类最大的缺陷是它无法处理特征符合所产生的变化(即前面提到过的实际上难以满足的相互独立)&lt;/p&gt;
&lt;p&gt;终于写完了…好累啊…还有8个算法……&lt;/p&gt;
- https://sword865.github.io/posts/archives/2009-09-05-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%931-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/ - Copyright (c) 2015. All rights reserved.</description>
        </item>
    
    
  </channel>
</rss> 