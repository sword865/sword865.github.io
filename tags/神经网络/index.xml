<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>神经网络 on 悟剑阁</title>
    <link>https://sword865.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
    <description>Recent content in 神经网络 on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Fri, 26 Apr 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://sword865.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[转] Deep Learning（深度学习）学习笔记整理系列</title>
      <link>https://sword865.github.io/posts/archives/2013-04-26-%E8%BD%AC-deep-learning%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E7%B3%BB%E5%88%97/</link>
      <pubDate>Fri, 26 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/posts/archives/2013-04-26-%E8%BD%AC-deep-learning%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E7%B3%BB%E5%88%97/</guid>
      <description>&lt;p&gt;转一套Deep Learning的文章&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/zouxy09/article/details/8775360&#34;&gt;http://blog.csdn.net/zouxy09/article/details/8775360&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;顺便附上翻译的UFLDL&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B&#34;&gt;http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>算法总结3—神经网络</title>
      <link>https://sword865.github.io/posts/archives/2009-09-07-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%933-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Mon, 07 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/posts/archives/2009-09-07-%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%933-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>&lt;p&gt;&lt;strong&gt;生物神经网络：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;    &lt;/strong&gt; 在生物的神经网络中的基本单位是神经元，神经元与神经元之间是由突触的相互联系来传递信息的，在静止息状态时，神经元的膜的内外电压保持一种稳定状态（膜内电压低于膜外电压），当神经元受到刺激后，在被刺激的部分周围，这种平衡状态会被打破，电压改变，与没有受到刺激的部分形成电流传递信息，电流的强弱取决于受刺激部位电压的改变量。&lt;/p&gt;&#xA;&lt;p&gt;     前一个神经元的轴突末梢作用于下一个神经元的胞体、树突或轴突等处组成突触。不同的轴突末梢可以释放不同的化学物质对下一个神经元产生不同的影响。也就是说会使下一个神经元的受刺激部分产生不同的电压，也就导致了不同程度的电流，最终也就传递了完全不同的信息。&lt;/p&gt;&#xA;&lt;p&gt;     一个神经元可以通过轴突作用于成千上万的神经元，也可以通过树突从成千上万的神经元接受信息。当多个神经元同时对一个神经元产生作用时，结果这些神经元的作用强度共同决定。&lt;/p&gt;&#xA;&lt;p&gt;     神经系统按功能可大致分为传入神经（感觉神经）、中间神经（脑：延脑、脑桥、小脑、中脑、间脑、大脑脊髓）与传出神经（运动神经）三类。&lt;/p&gt;&#xA;&lt;p&gt;     感受神经的作用是接受外界信息（输入），中间神经则起到了信息传递与计算分析的作用，最用，传出神经会负责对外界信息作出相应的反应（输出）。&lt;/p&gt;&#xA;&lt;p&gt;     模仿这一过程，我们就可以建立人工神经网络。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;人工神经网络：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;     人工神经网络的基本单位是人工神经元（以下简称神经元）。一个神经元可以有多个输入，每个输入有一个相应权值。&lt;/p&gt;&#xA;&lt;p&gt;图示如下：&lt;/p&gt;&#xA;&lt;img class=&#34;alignnone  wp-image-125&#34; src=&#34;http://upload.wikimedia.org/wikipedia/commons/9/97/Ncell.png&#34; alt=&#34;nn&#34; /&gt;&#xD;&#xA;&lt;pre&gt;&lt;code&gt;a1~an为神经元的输入值&#xD;&#xA;w1~wn为神经元各个的输入所拥有的权值&#xD;&#xA;b为偏移量&#xD;&#xA;sum对各个输入与其权值的积求和(含偏移量)。&#xD;&#xA;f为传递函数，接受sum的输出，通过一个函数变换，输出t&#xD;&#xA;t为神经元输出&#xD;&#xA;数学表示 t=f(WA&#39;+b)&#xD;&#xA;W为权向量&#xD;&#xA;A为输入向量，A&#39;为A向量的转置&#xD;&#xA;b为偏移量&#xD;&#xA;f为传递函数&#xD;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;在人工神经网络中，神经元之间相互连接，在连接点将前者的输出作为后者的输出，形成错综复杂的网状结构，进行信息的传递与计算。&lt;/p&gt;&#xA;&lt;p&gt;我们这里要介绍的是其中比较简单的一种模型，称为“多层感知机（MLP）”网络。&lt;/p&gt;&#xA;&lt;p&gt;为了简化模型，我们假设偏移量b=0.&lt;/p&gt;&#xA;&lt;p&gt;多层感知机网络由3部分组成：&lt;/p&gt;&#xA;&lt;p&gt;输入层：功能类似感受神经，每个节点接受外界的直接输入。这里的模型中，每个节点接受单一输入，权值为1。&lt;/p&gt;&#xA;&lt;p&gt;输出层：功能类似运动神经，该层输出就是神经网络的输出。&lt;/p&gt;&#xA;&lt;p&gt;隐藏层：是输入层和输出层之间的多层神经网络，可以有1或多层。&lt;/p&gt;&#xA;&lt;p&gt;因此，MLP网络中至少有3个层次。&lt;/p&gt;&#xA;&lt;img class=&#34;alignnone  wp-image-125&#34; src=&#34;https://sword865.github.io/images/archives/2012063021381464.jpg&#34; alt=&#34;mlp&#34; /&gt;&#xD;&#xA;&lt;p&gt;这些层次中，每层的每个神经元的输出都会作为下一层的每个神经元的输入，因此当我们对输入层进行输入后，该信息会一层层传递下去，最终从输出层输出。&lt;/p&gt;&#xA;&lt;p&gt;神经网络建立后，我们需要设法确定每个神经元的各个输入的权重w，并选择合适的函数f对输入进行变换，只有完成以上工作后，我们才能使用神经网络完成相应的工作。&lt;/p&gt;&#xA;&lt;p&gt;我们一般会选择过关于源点对称的S形函数作为函数f，该种函数特点是:输入接近0时，函数对输入的变化有敏感的反应，这一敏感度将随输入绝对值的增大而下降，最终趋于0。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;权重的获取：&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;选择合适的函数后，我们就要去确定各权重w了，权重的选择取决于我们想要神经网络完成的任务，我们首先会给每个输入一个初始化的默认值，该值可任意选取。&lt;/p&gt;&#xA;&lt;p&gt;完成初始化后，我们就要开始训练神经网络了，即给神经网络大量的已知的正确的输入及其对应的输出，神经网络会将自己得到到的输出与正确输出向比较，然后根据某一算法调整自身的权重，使自身输出更接近正确答案。&lt;/p&gt;&#xA;&lt;p&gt;我们这里要介绍的调整算法称为&lt;strong&gt;反向传播法&lt;/strong&gt;，因为该算法是沿网络反向调整权值的。&lt;/p&gt;&#xA;&lt;p&gt;这一算法中，我们会分析输出与正确答案，并将将输出向正确答案推进，为了了解如何推进，我们需要一个函数来计算函数f的斜率，设该函数为g。根据该函数，我们可以计算sum因改变的值。&lt;/p&gt;&#xA;&lt;p&gt;整个算法如下：&lt;/p&gt;&#xA;&lt;p&gt;从后向前对输出层和所有隐含层：&lt;/p&gt;&#xA;&lt;p&gt;1）  计算节点当前输出与期望结果的差值d。(期望结果t – 实际输出 y)&lt;/p&gt;&#xA;&lt;p&gt;对输出层: t在输入训练数据时一同输入。&lt;/p&gt;&#xA;&lt;p&gt;对隐含层: t = sum ( 前一层的每个节点的差值di * 这两个节点间连线的权值 )&lt;/p&gt;&#xA;&lt;p&gt;2）  利用函数g确定函数f在节点输出值y处的改变速率v。v=g(y)&lt;/p&gt;&#xA;&lt;p&gt;3）  改变每个输入链接的权值，其改变量与链接的当前输入强度与学习速率rate（自己定义的属于(0,1)的常量）成正比。&lt;/p&gt;&#xA;&lt;p&gt;（每个wi的改变量为（v&lt;em&gt;d&lt;/em&gt;rate*输入ai））&lt;/p&gt;&#xA;&lt;p&gt;这样一层层的从后向前反推，最终完成对一个训练样本的学习。&lt;/p&gt;&#xA;&lt;p&gt;当对所有样本完成训练后，我们就可以使用这个神经网络了。&lt;/p&gt;&#xA;&lt;p&gt;比如，我们想用神经网络模拟一个数学函数，我们先向网络提供大量的正确的输入输出进行训练，然后就可以用神经网络作模拟这个函数进行计算了。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
