<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CUDA on 悟剑阁</title>
    <link>https://sword865.github.io/tags/cuda/</link>
    <description>Recent content in CUDA on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Sat, 03 May 2025 15:51:35 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/tags/cuda/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flash MLA Kernel分析</title>
      <link>https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 03 May 2025 15:51:35 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-05-03-flashmla-kernel%E5%88%86%E6%9E%90/</guid>
      <description>&lt;p&gt;准备对DeepSeek的开源项目整理一些文档，也顺便强化一下记忆，先从FlashMLA开始。&lt;/p&gt;&#xA;&lt;p&gt;FlashMLA是DeepSeek开源的MLA算子实现，这个实现主要给inference decoding用的，Training和prefill应该是另外一个算子。&lt;/p&gt;&#xA;&lt;p&gt;先拿下面的图表示一下MLA算子是在计算一个什么东西，这篇文章就不讲具体的推导了，反正这个算子大概就是下面的2个GEMM算子的融合。需要注意的是：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;这里矩阵K和矩阵V的共享一部分参数。&lt;/li&gt;&#xA;&lt;li&gt;图里只画显示了一个Query Head和一对KV Head的计算。在实际计算中还要num_kv_head和batch_size两个维度。&lt;/li&gt;&#xA;&lt;li&gt;两个GEMM中间其实还有一个sotfmax，不过这里可以通过online softmax算法把这块逻辑独立处理分块处理，所以不影响主流程。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;img width=&#34;600&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/mla.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;Kernel的调用主要分两部分&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;调用&lt;code&gt;get_mla_metadata&lt;/code&gt;来计算一些metadata，用来优化kernel的执行&lt;/li&gt;&#xA;&lt;li&gt;调用&lt;code&gt;flash_mla_with_kvcache&lt;/code&gt;进行计算&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;在进入调用前，先大概说一下FlashMLA计算的拆分逻辑。这块和FlashDecoding很像，并没有要求一个thread-block必须处理一个完整的sequence，而是通过一个负载均衡算法，把所有的sequence放到一起，然后拆分成一个个的sequence-block，然后每个thread-block就去处理分配给它的那些block的计算，最后再把这些thread-block的结果用合并，得到正确的输出。&lt;/p&gt;&#xA;&lt;p&gt;大概是下面这个图的样子：&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2025/20250503/computation-pattern.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;所以为了完成计算，第一步就是决定每个block需要处理哪些sub-sequence，也就是&lt;code&gt;get_mla_metadata&lt;/code&gt;要完成的事情。&lt;/p&gt;&#xA;&lt;h1 id=&#34;get_mla_metadata&#34;&gt;get_mla_metadata&lt;/h1&gt;&#xA;&lt;p&gt;先看&lt;code&gt;get_mla_metadata&lt;/code&gt;具体提供了哪些元数据，我们从repo提供的测试代码入手，考虑最简单的情况(batch_size=128, query_sequence_len=1, mean_key_sequence_len=4096, MTP=1, num_kv_head=1, num_q_head=16, TP=1, hidden_NoRoPE_dim=512, hidden_RoPE_dim=64, varlen=False)。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# cache_seqlens = tensor([4096, 4096, ..., 4096], dtype=torch.int32), size=batch_size, value=sequence_len&#xD;&#xA;# s_q=1 (query_sequence_len=1且MTP=1), h_q(num_q_head)=128 (TP=1=128/128) h_kv(num_kv_head)=1&#xD;&#xA;# 基于这些配置，计算mla kernel的metadata&#xD;&#xA;tile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为这里我们是在测试decoding步骤，所以有&lt;code&gt;query_sequence_len=1&lt;/code&gt;，可以看到三个入参：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;kv cache的大小&lt;/li&gt;&#xA;&lt;li&gt;类似GQA的Group数量，这个参数表示每个kv head对应多少个query head。&lt;/li&gt;&#xA;&lt;li&gt;kv head的数量&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;code&gt;get_mla_metadata&lt;/code&gt;会根据GPU中SM的数量和要处理的数据的大小，给每个SM分配任务。这个注意&lt;code&gt;get_mla_metadata_kernel&lt;/code&gt;的参数为&lt;code&gt;&amp;lt;&amp;lt;&amp;lt;1, 32, 0, stream&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;，因此所有计算会在1个warp中完成。&lt;/p&gt;&#xA;&lt;p&gt;这里的关键就是具体怎么给每个(每组)SM分配工作的.&lt;/p&gt;</description>
    </item>
    <item>
      <title>vLLM Paged Attention代码分析</title>
      <link>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 20 Apr 2025 15:51:35 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>&lt;p&gt;3月底整理了一个关于经典Paged Attention算法的ppt, 想起这个几年没写过的blog，把PPT改成一篇文章证明我还活着(-_-)。&lt;/p&gt;&#xA;&lt;img width=&#34;500&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/paged_attention.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;h2 id=&#34;vllm-的-paged-attention&#34;&gt;vLLM 的 Paged Attention&lt;/h2&gt;&#xA;&lt;p&gt;开始前先说明一下，vLLM里的Paged Attention Kernel是有好几个不同的版本的，大概是下面这样子：&lt;/p&gt;&#xA;&lt;p&gt;vLLM早期版本：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prefilling -&amp;gt; Flash Attention的flash_attn_varlen_func&lt;/li&gt;&#xA;&lt;li&gt;Dedocding -&amp;gt; 自己实现的Paged Attention&#xA;&lt;ul&gt;&#xA;&lt;li&gt;paged_attention_v1 : 用于比较短的sequence&lt;/li&gt;&#xA;&lt;li&gt;paged_attention_v2 : 用于不想用v1的情况 :)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;源码大概是这样的：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    # NOTE(woosuk): We use a simple heuristic to decide whether to use&#xD;&#xA;    # PagedAttention V1 or V2. If the number of partitions is 1, we use&#xD;&#xA;    # V1 to avoid the overhead of reduction. Also, if the number of&#xD;&#xA;    # sequences or heads is large, we use V1 since there is enough work&#xD;&#xA;    # to parallelize.&#xD;&#xA;    # TODO(woosuk): Tune this heuristic.&#xD;&#xA;    # For context len &amp;gt; 8192, use V2 kernel to avoid shared memory&#xD;&#xA;    # shortage.&#xD;&#xA;    use_v1 = (max_seq_len &amp;lt;= 8192 and (max_num_partitions == 1 or num_seqs * num_heads &amp;gt; 512))&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;vLLM 最新版本就已经全部转向Flash Attention， 用cutlass实现了。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
