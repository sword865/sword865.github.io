<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Logistic on 悟剑阁</title>
    <link>https://sword865.github.io/tags/logistic/</link>
    <description>Recent content in Logistic on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Sat, 03 Aug 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://sword865.github.io/tags/logistic/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Google在KDD2013上关于CTR的一篇论文</title>
      <link>https://sword865.github.io/archives/6/</link>
      <pubDate>Sat, 03 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/archives/6/</guid>
      <description>&lt;p&gt;最近在做CTR，刚好Google在KDD发了一篇文章，讲了他们的一些尝试，总结一下：&lt;/p&gt;&#xA;&lt;p&gt;先是一些公式的符号说明：&lt;/p&gt;&#xA;&lt;p&gt;[&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011358-949b9f116ca94563ab5a0efa047697db.png&#34; alt=&#34;image&#34; width=&#34;447&#34; height=&#34;98&#34; border=&#34;0&#34; /&gt;][1]&lt;/p&gt;&#xA;&lt;p&gt;一、优化算法&lt;/p&gt;&#xA;&lt;p&gt;CTR中经常用Logistic regression进行训练，一个常用的Loss Function为&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011358-9e64ee86e7f14dbd8a3da9f8a2bb7c13.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011358-03407ff016d04735b28103abc573d428.png&#34; alt=&#34;image&#34; width=&#34;329&#34; height=&#34;34&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Online gradient descent(OGD)是一个常用的优化方法，但是在加上L1正则化后，这种方法不能产生有效的稀疏模型。相比之下 Regularized Dual Averaging (RDA)拥有更好的稀疏性，但是精度不如OGD好。&lt;/p&gt;&#xA;&lt;p&gt;FTRL-Proximal 方法可以同时得到稀疏性与精确性，不同于OGD的迭代步骤：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011358-2e856d4483c64ef0a668ee8c8df2a548.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011359-6f55d3e1f55f4389979bec14102120b5.png&#34; alt=&#34;image&#34; width=&#34;187&#34; height=&#34;39&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;其中$\eta_t$是一个非增的学习率&lt;/p&gt;&#xA;&lt;p&gt;FTRL-Proximal通过下式迭代：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011359-900ea01f64a547bd99cefe8e375a3a9d.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011359-37b635900b57402a9efc1d0fbf12e7d2.png&#34; alt=&#34;image&#34; width=&#34;482&#34; height=&#34;60&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;参数$\sigma$是学习率，一般我们有 ${\sum_{s=1}^t\sigma_s=\frac{1}{\eta_t}}$。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
