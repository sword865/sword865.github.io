<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Attention on 悟剑阁</title>
    <link>https://sword865.github.io/tags/attention/</link>
    <description>Recent content in Attention on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Tue, 10 Feb 2026 23:26:44 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/tags/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Attention基础-部署篇</title>
      <link>https://sword865.github.io/posts/2025/2026-02-10-linear-attention%E5%9F%BA%E7%A1%80-%E9%83%A8%E7%BD%B2%E7%AF%87/</link>
      <pubDate>Tue, 10 Feb 2026 23:26:44 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2026-02-10-linear-attention%E5%9F%BA%E7%A1%80-%E9%83%A8%E7%BD%B2%E7%AF%87/</guid>
      <description>&lt;p&gt;系列的前两篇文章写了线性注意力基本的理论和工程(Kernel)，但是相关的内容只是针对开发环境的一些讨论，实际上在大规模部署时，我们为了更好地利用资源，往往需要做很多优化，这篇文章就从这些落地优化的角度聊一下线性注意力，以及为什么我们会认为现在线性注意力的infra只是在初级阶段。&lt;/p&gt;&#xA;&lt;h1 id=&#34;量化&#34;&gt;量化&lt;/h1&gt;&#xA;&lt;h2 id=&#34;llmint8-和-massive-activations&#34;&gt;LLM.int8() 和 Massive Activations&lt;/h2&gt;&#xA;&lt;p&gt;早在ChatGPT爆火前，就有一些研究讨论了Full-Attention架构在量化时遇到的问题，在&lt;a href=&#34;https://arxiv.org/pdf/2208.07339&#34;&gt;LLM.int8()&lt;/a&gt;这篇论文中，就提到在Full-Attention的激活值中有部分离群点会影响量化效果，因此建议把离群点和其他值分开处理：&lt;/p&gt;&#xA;&lt;img width=&#34;800&#34;  src=&#34;https://sword865.github.io/images/2026/20260210/llm_int8.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;p&gt;这一现象被称为Massive Activations，目前业界对这个现象进行了大量的研究，其中两个主要因素就是Softmax 和 RoPE。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Softmax 归一化&lt;/strong&gt;：Softmax有着强制要求“总和为1”的特性，但是在实际的LLM运算中，我们往往有更复杂的需求（比如有时我们输入模型的问题比较简单，一个60层的网络可能在前40层就已经完成了计算，对于后面层我们需要的是一个什么都不做的操作，那么在残差网络的帮助下我们其实需要一个0输出），为了满足这种需求就会出现类似Attention Sink的现象，导致注意力集中于单一无关标记，同时产生&lt;strong&gt;massive activation&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;RoPE位置编码&lt;/strong&gt;：在文章&lt;a href=&#34;https://arxiv.org/pdf/2502.01563&#34;&gt;Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding&lt;/a&gt;就提到：RoPE在Query/Key的通道维度对上应用了带有频率性角度参数的旋转变换，该变换依赖相对位置，同时在某些维度的投影上显著放大或缩小分量，从而导致部分维度“成簇集中”，出现了异常的spike值。 实际上，该文章也在实验中发现凡是采用 RoPE 的模型（如 LLaMA、Qwen、Gemma、Falcon 等）都会展示这种 massive values stripes，而不使用 RoPE 的模型（如 GPT-2、OPT、Jamba）则没有这一现象。&lt;/p&gt;&#xA;&lt;h2 id=&#34;linear-attention量化&#34;&gt;Linear Attention量化&lt;/h2&gt;&#xA;&lt;p&gt;虽然在Linear Attention中没有了Softmax和RoPE的干扰，大大缓解了上面提到的问题。比如&lt;a href=&#34;https://arxiv.org/pdf/2601.22966v1&#34;&gt;A Unified View of Attention and Residual Sinks&lt;/a&gt;研究发现：将Full(Softmax) Attention替换为Linear-Attention后，隐藏状态的峰值激活从6000降到510，问题的复杂度似乎减少了很多。&lt;/p&gt;&#xA;&lt;p&gt;但是，其实问题并没有被彻底解决，在不同的Linear Attention架构中仍然有着不同的问题：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;状态累积&lt;/strong&gt;：Linear Attention的递归特性导致数值误差或特定模式在时间步上不断累积。例如Mamba的&lt;strong&gt;PScan&lt;/strong&gt;，在长序列下容易产生“长度诱导”的数值退化。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;门控放大效应&lt;/strong&gt;：GLA/Mamba 等架构大量使用的门控机制，其权重往往呈现“稀疏-极端”的双边分布，在不同的channel间拉大值的范围差异。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;因此，针对Full-Attention设计的量化方案有些（如LLM.int8()）可以用于Linear-Attention，但是也有些（如 SmoothQuant）难以直接迁移到Linear Attention领域，我们会需要针对不同架构设计架构感知的专用量化策略。&lt;/p&gt;&#xA;&lt;p&gt;不过，相比下面两个领域，Linear Attention在模型量化领域的研究还是最多，相对最成熟的一个。&lt;/p&gt;&#xA;&lt;h1 id=&#34;prefix-cache&#34;&gt;Prefix Cache&lt;/h1&gt;&#xA;&lt;p&gt;如果说量化并非LLM部署时的必选项，那么Prefix Cache（前缀缓存）一定是大规模部署时绕不过的一个策略。它允许复用之前的计算结果（KV Cache），对于多轮对话或Shared System Prompt场景，能极大地提升首字延迟TTFT和系统吞吐。&lt;/p&gt;&#xA;&lt;p&gt;然而，Linear Attention并不能直接无缝的集成这一功能：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Attention基础-工程篇</title>
      <link>https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/</link>
      <pubDate>Thu, 01 Jan 2026 15:23:41 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2026-01-01-linear-attention-%E5%B7%A5%E7%A8%8B%E7%AF%87/</guid>
      <description>&lt;p&gt;本文重点参考了文章&lt;a href=&#34;https://srush.github.io/annotated-mamba/hard.html&#34;&gt;Mamba: The Hard Way&lt;/a&gt;和开源项目&lt;a href=&#34;https://github.com/fla-org/flash-linear-attention&#34;&gt;flash-linear-attention&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;h1 id=&#34;prefill与decoding&#34;&gt;Prefill与Decoding&lt;/h1&gt;&#xA;&lt;p&gt;我们都知道，在Attention的计算中，Prefill和Decoding是两个不同的场景，具体特性如下：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;特性&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Prefill&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Decoding&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;输入&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;长序列（长度 $L$）&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 个新 token + 历史状态&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;常见瓶颈&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Compute bound（Tensor Core 利用）&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Memory/Latency bound（状态读写 + 小矩阵计算）&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;在回忆一下理论篇的介绍，特别是关于Mamba章节中的推导，常见的Linear Attention有两种表示格式：&lt;/p&gt;&#xA;&lt;p&gt;矩阵格式（Attention视角）：&lt;/p&gt;&#xA;&lt;p&gt;$$ y_i = \sum_{j=0}^i (CausalMask(Q_i K_j^T)) V_j $$&lt;/p&gt;&#xA;&lt;p&gt;递推格式（SSM视角）：&lt;/p&gt;&#xA;&lt;p&gt;$$ h_t = A_t h_{t-1} + B_t x_t $$&#xA;$$ y_t = C h_t $$&lt;/p&gt;&#xA;&lt;p&gt;其中Decoding的算子可以比较直接的使用递归格式进行计算，因此我们本文重点还是看Prefill的实现。&lt;/p&gt;&#xA;&lt;h1 id=&#34;linear-attention常见算法&#34;&gt;Linear Attention常见算法&lt;/h1&gt;&#xA;&lt;p&gt;在Linear Attention的计算中，有一些常见的思路，本章结合&lt;code&gt;flash-linear-attention&lt;/code&gt;的实现，对这些思路进行讲解。&lt;/p&gt;&#xA;&lt;h2 id=&#34;prefix-scan--cumsum-前缀和&#34;&gt;Prefix Scan / Cumsum 前缀和&lt;/h2&gt;&#xA;&lt;h3 id=&#34;算法简介&#34;&gt;算法简介&lt;/h3&gt;&#xA;&lt;p&gt;先讲一下什么是Prefix Scan算法，简单来说Prefix是针对有结合性的算子提出的一种优化方式&lt;/p&gt;&#xA;&lt;p&gt;假设有 $y_t = x_1 ⊗ x_2 ⊗ x_3 &amp;hellip; ⊗ x_t$, 如果⊗支持结合律，那么y_t就可以用一个简单的Reduce进行求解&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Attention基础-理论篇</title>
      <link>https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/</link>
      <pubDate>Tue, 16 Dec 2025 22:09:11 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-12-16-linear-attention%E5%9F%BA%E7%A1%80-%E7%90%86%E8%AE%BA%E7%AF%87/</guid>
      <description>&lt;p&gt;准备写一些关于线性注意力的文章，对相关理论和工程(Kernel)做一些梳理，这一篇是关于基础理论的。&lt;/p&gt;&#xA;&lt;h1 id=&#34;softmax-attention到线性注意力&#34;&gt;Softmax Attention到线性注意力&lt;/h1&gt;&#xA;&lt;h2 id=&#34;softmax-attention与on2复杂度&#34;&gt;Softmax Attention与O(N^2)复杂度&lt;/h2&gt;&#xA;&lt;p&gt;标准的Transformer使用的是Softmax Attention。给定查询（Query）、键（Key）、值（Value）矩阵 $Q, K, V \in \mathbb{R}^{N \times d}$，其中 $N$ 是序列长度，$d$ 是特征维度（通常 $d \ll N$）。Attention的计算公式为：&lt;/p&gt;&#xA;&lt;p&gt;$$ Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V $$&lt;/p&gt;&#xA;&lt;p&gt;让我们仔细分析一下这个计算过程的维度变化：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;计算相似度矩阵&lt;/strong&gt;：$QK^T$。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$Q$ 是 $N \times d$，$K^T$ 是 $d \times N$。&lt;/li&gt;&#xA;&lt;li&gt;相乘得到 $N \times N$ 的矩阵。这一步的计算复杂度是 $O(N^2 d)$。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;应用Softmax&lt;/strong&gt;：对每一行进行归一化，维度不变，仍为 $N \times N$。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;加权求和&lt;/strong&gt;：乘以 $V$。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$N \times N$ 的矩阵乘以 $N \times d$ 的矩阵 $V$。&lt;/li&gt;&#xA;&lt;li&gt;结果是 $N \times d$。这一步的计算复杂度也是 $O(N^2 d)$。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;瓶颈所在&lt;/strong&gt;：&#xA;由于Softmax是非线性的，我们必须先完整地计算出 $N \times N$ 的Attention Matrix。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
