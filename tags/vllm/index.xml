<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vllm on 悟剑阁</title>
    <link>https://sword865.github.io/tags/vllm/</link>
    <description>Recent content in Vllm on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Thu, 29 Jan 2026 21:55:26 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/tags/vllm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>模型分片，KV Cache和推理加速的一些思考：计算与数据</title>
      <link>https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</link>
      <pubDate>Thu, 29 Jan 2026 21:55:26 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2026-01-29-%E6%A8%A1%E5%9E%8B%E5%88%86%E7%89%87kv-cache%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</guid>
      <description>&lt;p&gt;这段时间主要在做推理加速相关的工作，也做了一些实验，准备写点文章做一些记录。这篇文章就从“移动计算”和“移动存储”的视角开个头，聊聊&lt;strong&gt;并行策略的动态切换&lt;/strong&gt;和&lt;strong&gt;KV Cache 流动管理&lt;/strong&gt;的一些实践，梳理一下在 Agentic Workflow 日益普及的当下，我们能否通过对&lt;strong&gt;算力、存储与带宽&lt;/strong&gt;的调度，在大规模集群上更好地提升推理效率。&lt;/p&gt;&#xA;&lt;p&gt;在传统的大数据时代，“计算中心”还是“数据中心”一直是个有趣的问题。随着技术的发展，大家也逐渐总结出了 &lt;strong&gt;&amp;ldquo;Move Compute to Data&amp;rdquo;&lt;/strong&gt; 的实践经验：因为 SSD 很贵、IO 很贵、带宽也很贵。相比之下，不如把代码调度过去，使用本地的 CPU 进行计算。因此，调度器的核心任务是保证 Data Locality，尽量把计算分发到数据所在的硬盘旁边。&lt;/p&gt;&#xA;&lt;p&gt;但在 LLM 的时代，我们面对的是超大模型在 GPU 上的推理，移动计算已经变成了移动模型（权重）这个巨无霸。相比之下，似乎还是移动数据（KV Cache）更现实一些——但什么才是最合适的解法呢？&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-需求的演变从-chat-到-agent&#34;&gt;1. 需求的演变：从 Chat 到 Agent&lt;/h2&gt;&#xA;&lt;p&gt;系统架构的演进，必然是随着业务不断演变的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;第一阶段单轮指令&#34;&gt;第一阶段：单轮指令&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：用户发送一句指令任务（如翻译、总结），模型执行并回复。请求之间几乎独立。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：纯粹的算力（Prefill）或显存带宽（Decoding）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;：最简单的加权轮询。此时 &lt;strong&gt;KV Cache&lt;/strong&gt; 的存在感很低，除了每台机器都有的 System Prompt，几乎没有状态复用的需求，我们可以随意把请求调度到任一台机器上执行。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;第二阶段多轮对话&#34;&gt;第二阶段：多轮对话&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：多轮对话可以通过 Prefix Caching 复用前面上文。Context 越来越长，每次对话对应一次交互。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：显存容量 &amp;ndash;&amp;gt; Prefill 时间&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;：开始引入 &lt;strong&gt;Affinity（亲和性）&lt;/strong&gt; 调度——为了命中 Cache，我们尽量把请求发给存储了该用户历史数据的节点。也就是 &lt;strong&gt;&amp;ldquo;Move Compute to Data&amp;rdquo;&lt;/strong&gt;，因为此时 Prefill（重算数据）太贵，而搬运 KV Cache 也还没在大规模集群中普及。但是这也会导致热点问题。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;第三阶段agentic-workflow&#34;&gt;第三阶段：Agentic Workflow&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;特征&lt;/strong&gt;：系统提示词、工具定义、思维链、上下文可以在并行的分支任务中共享，多轮对话可以并行执行，但对应一次交互（用户从感知多次交互变成感知任务完成）。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;瓶颈&lt;/strong&gt;：极其复杂的依赖关系，以及直接复用 KV Cache 带来的负载不均衡。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;调度（面临的问题）&lt;/strong&gt;：如果 &amp;ldquo;Move Compute to Data&amp;rdquo; 会导致严重的热点问题——存有热门 Context 的节点会被打爆，而其他空闲节点却因为没有数据而帮不上忙。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;可以看到，随着需求的变化，我们不再只关注单次请求的 TTFT/TPOT，而是开始关注整个 Agent 任务的 &lt;strong&gt;任务完成时间&lt;/strong&gt; 以及系统的 &lt;strong&gt;总吞吐&lt;/strong&gt;量。&lt;/p&gt;</description>
    </item>
    <item>
      <title>vLLM Paged Attention代码分析</title>
      <link>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 20 Apr 2025 15:51:35 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2025/2025-04-20-vllm-paged-attention%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>&lt;p&gt;3月底整理了一个关于经典Paged Attention算法的ppt, 想起这个几年没写过的blog，把PPT改成一篇文章证明我还活着(-_-)。&lt;/p&gt;&#xA;&lt;img width=&#34;500&#34;  src=&#34;https://sword865.github.io/images/2025/20250420/paged_attention.png&#34; class=&#34;center&#34; /&gt;&#xD;&#xA;&lt;h2 id=&#34;vllm-的-paged-attention&#34;&gt;vLLM 的 Paged Attention&lt;/h2&gt;&#xA;&lt;p&gt;开始前先说明一下，vLLM里的Paged Attention Kernel是有好几个不同的版本的，大概是下面这样子：&lt;/p&gt;&#xA;&lt;p&gt;vLLM早期版本：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prefilling -&amp;gt; Flash Attention的flash_attn_varlen_func&lt;/li&gt;&#xA;&lt;li&gt;Dedocding -&amp;gt; 自己实现的Paged Attention&#xA;&lt;ul&gt;&#xA;&lt;li&gt;paged_attention_v1 : 用于比较短的sequence&lt;/li&gt;&#xA;&lt;li&gt;paged_attention_v2 : 用于不想用v1的情况 :)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;源码大概是这样的：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    # NOTE(woosuk): We use a simple heuristic to decide whether to use&#xD;&#xA;    # PagedAttention V1 or V2. If the number of partitions is 1, we use&#xD;&#xA;    # V1 to avoid the overhead of reduction. Also, if the number of&#xD;&#xA;    # sequences or heads is large, we use V1 since there is enough work&#xD;&#xA;    # to parallelize.&#xD;&#xA;    # TODO(woosuk): Tune this heuristic.&#xD;&#xA;    # For context len &amp;gt; 8192, use V2 kernel to avoid shared memory&#xD;&#xA;    # shortage.&#xD;&#xA;    use_v1 = (max_seq_len &amp;lt;= 8192 and (max_num_partitions == 1 or num_seqs * num_heads &amp;gt; 512))&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;vLLM 最新版本就已经全部转向Flash Attention， 用cutlass实现了。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
