<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CTR on 悟剑阁</title>
    <link>https://sword865.github.io/tags/ctr/</link>
    <description>Recent content in CTR on 悟剑阁</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Fri, 04 Nov 2016 22:47:21 +0800</lastBuildDate>
    <atom:link href="https://sword865.github.io/tags/ctr/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>谈谈Factorization Machine</title>
      <link>https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/</link>
      <pubDate>Fri, 04 Nov 2016 22:47:21 +0800</pubDate>
      <guid>https://sword865.github.io/posts/2016/2016-11-04-%E8%B0%88%E8%B0%88factorization-machine/</guid>
      <description>&lt;p&gt;因子分解机(Factorization Machine, 简称FM)是一种不错的CTR预估模型，也是我们现在在使用的广告点击率预估模型，比起著名的Logistic Regression, FM能够把握一些组合的高阶特征，因此拥有更强的表现力。&lt;/p&gt;&#xA;&lt;p&gt;在做点击率预估时，我们的特征往往来自于用户(user)、广告(item)和上下文环境(context)，在线性模型中，这些特征不进行组合的话，就会发生一个很尴尬的情况，因为：&lt;/p&gt;&#xA;&lt;div&gt;$$score = f(w_{user} * x_{user} + w_{item} * x_{item} + w_{contex} * x_{contex})$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;所以，对所有用户&amp;ndash;我们将得到相同的排序。&lt;/p&gt;&#xA;&lt;p&gt;因此我们需要引入一些组合特征作为输入模型，然而仅二阶特征组合的可能性就是原始特征的平方之多，但是由于很多特征其实是相互独立的，他们的组合并没有什么价值。FM就是一种能够自动把握一些高阶特征的点击率预估模型，可以自动帮助使用者选择合适的高阶输入。&lt;/p&gt;&#xA;&lt;p&gt;我们先写出带有所有二阶组合特征的目标函数，其中矩阵W中有n^2个参数，求解非常复杂：&lt;/p&gt;&#xA;&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;我们都知道在矩阵分解的协同过滤中中，我们认为每个用户可以表示成一个K维的特征向量$u_k$,每个物品也能表示成一个高维向量$i_k$，这样用户与物品的相关性就可以用两个向量的点击表示，所有用户与所有物品的相关性就可以用两个矩阵相乘的形式表示出来了。&lt;/p&gt;&#xA;&lt;p&gt;FM的模型参考了矩阵分解的思想，认为每个特征 $x_i$都可以用一个K维的特征向量$v_i$表示，那么所有特征之前的相关性就也就可以用两个矩阵的相乘进行表示了，模型表示如下：&lt;/p&gt;&#xA;&lt;p&gt;其中矩阵W中代表了特征的特征向量的內积，既：$w_{ij}=v_i v_j$，所以，公式可以改写为:&lt;/p&gt;&#xA;&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} v_{i} v_{j} x_i x_j$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;也就是：&lt;/p&gt;&#xA;&lt;div&gt;$$y(x) = w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \sum_{h=i}^{k} v_{i,h} v_{j,h} x_i x_j$$&lt;/div&gt;&#xD;&#xA;&lt;p&gt;可以看出，W中的参数现在只有nk个了，因为一般有k&amp;laquo;n，所以FM大大降低的目标函数的复杂度。推导可得梯度公式：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google在KDD2013上关于CTR的一篇论文</title>
      <link>https://sword865.github.io/posts/archives/2013-08-03-google%E5%9C%A8kdd2013%E4%B8%8A%E5%85%B3%E4%BA%8Ectr%E7%9A%84%E4%B8%80%E7%AF%87%E8%AE%BA%E6%96%87/</link>
      <pubDate>Sat, 03 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://sword865.github.io/posts/archives/2013-08-03-google%E5%9C%A8kdd2013%E4%B8%8A%E5%85%B3%E4%BA%8Ectr%E7%9A%84%E4%B8%80%E7%AF%87%E8%AE%BA%E6%96%87/</guid>
      <description>&lt;p&gt;最近在做CTR，刚好Google在KDD发了一篇文章，讲了他们的一些尝试，总结一下：&lt;/p&gt;&#xA;&lt;p&gt;先是一些公式的符号说明：&lt;/p&gt;&#xA;&lt;p&gt;[&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011358-949b9f116ca94563ab5a0efa047697db.png&#34; alt=&#34;image&#34; width=&#34;447&#34; height=&#34;98&#34; border=&#34;0&#34; /&gt;][1]&lt;/p&gt;&#xA;&lt;p&gt;一、优化算法&lt;/p&gt;&#xA;&lt;p&gt;CTR中经常用Logistic regression进行训练，一个常用的Loss Function为&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011358-9e64ee86e7f14dbd8a3da9f8a2bb7c13.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011358-03407ff016d04735b28103abc573d428.png&#34; alt=&#34;image&#34; width=&#34;329&#34; height=&#34;34&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Online gradient descent(OGD)是一个常用的优化方法，但是在加上L1正则化后，这种方法不能产生有效的稀疏模型。相比之下 Regularized Dual Averaging (RDA)拥有更好的稀疏性，但是精度不如OGD好。&lt;/p&gt;&#xA;&lt;p&gt;FTRL-Proximal 方法可以同时得到稀疏性与精确性，不同于OGD的迭代步骤：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011358-2e856d4483c64ef0a668ee8c8df2a548.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011359-6f55d3e1f55f4389979bec14102120b5.png&#34; alt=&#34;image&#34; width=&#34;187&#34; height=&#34;39&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;其中$\eta_t$是一个非增的学习率&lt;/p&gt;&#xA;&lt;p&gt;FTRL-Proximal通过下式迭代：&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://sword865.github.io/images/archives/04011359-900ea01f64a547bd99cefe8e375a3a9d.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;https://sword865.github.io/images/archives/04011359-37b635900b57402a9efc1d0fbf12e7d2.png&#34; alt=&#34;image&#34; width=&#34;482&#34; height=&#34;60&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;参数$\sigma$是学习率，一般我们有 ${\sum_{s=1}^t\sigma_s=\frac{1}{\eta_t}}$。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
